include './base'
include './mask'
include './spaced'

def ind_types = tup{i8, i16, i32}
def dat_types = tup{...ind_types, u64}

# Indices and Replicate using plus- or max-scan
def scan_core{upd, set, scan, rp:*T, wp:W, s:(usz)} = {
  def getw{j} = if (isptr{W}) cast_i{usz,load{wp,j}} else wp
  b:usz = 1<<10
  k:usz = 0; j:usz = 0; ij:=getw{j}
  while (1) {
    e := tern{b<s-k, k+b, s}
    @for (rp over i from k to e) rp = 0
    if (set) store{rp, k, cast_i{T,j}}
    while (ij<e) { ++j; upd{rp, j, ij}; ij+=getw{j} }
    scan{rp+k, e-k}
    if (e==s) return{}
    k = e
  }
}
def indrep_by_sum{T, rp:*T, wp, s:(usz), js, inc} = {
  def scan{ptr, len} = @for (ptr over len) js=ptr+=js
  def scan{ptr:*E, len if width{T}<=32} = {
    def scanfn = merge{'si_scan_pluswrap_u',fmtnat{width{T}}}
    p := *ty_u{E}~~ptr
    emit{void, scanfn, p, p, len, js}; js=load{ptr,len-1}
  }
  def upd{rp, j, ij} = store{rp, ij, load{rp,ij}+inc{j}}
  scan_core{upd, 0, scan, rp, wp, s}
}

fn ind_by_scan_i32{W}(xv:*void, rp:*i32, s:usz) : void = {
  xp := *W~~xv
  if (hasarch{'X86_64'} and not hasarch{'SSE4.1'}) { # no min instruction
    js:i32 = 0
    indrep_by_sum{i32, rp, xp, s, js, {j}=>1}
  } else {
    scan_core{
      {rp,j,ij} => store{rp,ij,cast_i{i32,j}}, 1,
      {ptr,len} => emit{void, 'si_scan_max_i32', ptr,ptr,len},
      rp, xp, s
    }
  }
}

def rep_by_scan{T, wp, xv:(*void), rv:(*void), s} = {
  xp := *T~~xv; js := *xp; px := js
  def inc{j} = {sx:=px; px=load{xp,j}; px-sx}
  indrep_by_sum{T, *T~~rv, wp, s, js, inc}
}
fn rep_by_scan{W, T}(wp:*void, xv:*void, rv:*void, s:usz) : void = {
  rep_by_scan{T, *W~~wp, xv, rv, s}
}

exportT{'si_indices_scan_i32', each{ind_by_scan_i32, ind_types}}
exportT{'si_replicate_scan', flat_table{rep_by_scan, ind_types, dat_types}}


# Constant replicate
if_inline (not (hasarch{'AVX2'} or hasarch{'AARCH64'})) {

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  rep_by_scan{T, cast_i{usz,wv}, x, r, cast_i{usz, wv*n}}
}

} else {

def incl{a,b} = slice{iota{b+1},a}

# 1+Ëâˆ¨`âŒ¾âŒ½0=div|âŒœrange
def makefact{divisor, range} = {
  def t = table{{a,b}=>0==b%a, divisor, range}
  fold{+, 1, reverse{scan{|, reverse{t}}}}
}
def basic_rep = incl{2, 7}
def fact_size = 128
def fact_inds = slice{iota{fact_size},8}
def fact_tab = makefact{basic_rep, fact_inds}
factors:*u8 = fact_tab



def sdtype = [arch_defvw/8]i8 # shuf data type
def get_shufs{step, wv, len} = {
  def i = iota{len*step}
  split{step, (i - i%wv)/wv}
}
def get_shuf_data{wv, len} = get_shufs{vcount{sdtype}, wv, len} # [len] byte-selector vectors for wv/sdtype (expanded to wider types by read_shuf_vecs)
def get_shuf_data{wv} = get_shuf_data{wv, wv}

# all shuffle vectors for ð•¨â‰¤7
def special_2 = ~hasarch{'AARCH64'} # handle 2 specially on x86-64
def rcsh_vals = slice{basic_rep, special_2}
rcsh_offs:*u8 = shiftright{0, scan{+,rcsh_vals}}
rcsh_data:*i8 = join{join{each{get_shuf_data, rcsh_vals}}}

# first 4 shuffle vectors for 11â‰¤ð•¨â‰¤61; only uses the low half of the input
def rcsh4_dom = replicate{bind{>=,64}, replicate{fact_tab==1, fact_inds}}
rcsh4_dat:*i8 = join{join{each{get_shuf_data{., 4}, rcsh4_dom}}}
rcsh4_lkup:*i8 = shiftright{0, scan{+, fold{|, table{==, rcsh4_dom, iota{64}}}}}

def read_shuf_vecs{l, ellw:(u64), shp:*V} = { # tuple of byte selectors in 1<<ellw
  def double{x:X if hasarch{'AVX2'}} = {
    s:=shuf{[4]u64, x, 4b3120}; s+=s
    r:=each{bind{~~,[32]i8},mzip128{s, s + X**1}}
    r
  }
  def double{x:X if hasarch{'AARCH64'}} = {
    s:= x+x
    zip{s, s + X**1}
  }
  def doubles{n,tup} = slice{join{each{double,tup}}, 0, n}
  
  def sh = each{{v}=>{r:=v}, l**V**0}
  def tlen{e} = cdiv{l, e}  # Length for e bytes, rounded up
  def set{i} = { select{sh,i} = each{load{shp,.}, i} }
  def ext{e} = {
    def m = tlen{2*e}; def n = tlen{e}  # m<n
    if (ellw <= lb{e}) set{slice{iota{n},m}}
    else slice{sh,0,n} = doubles{n,slice{sh,0,m}}
  }
  set{iota{tlen{8}}}; ext{4}; ext{2}; ext{1}
  sh
}

def rep_const_shuffle{wv, onreps, xv:*V=[step]T, rv:*V, n:(u64)} = { # onreps{inputVector, {nextOutputVector} => ...}
  nv := n / step
  j:u64 = 0
  def write{v} = { store{rv, j, v}; ++j }
  @for (xv over nv) onreps{xv, write}
  if (nv*step < n) {
    nr := n * wv
    e := nr / step
    s := V**0
    def end = makelabel{}
    onreps{load{xv,nv}, {v} => {
      s = v
      if (j == e) goto{end}
      write{s}
    }}
    setlabel{end}
    q := nr & (step-1)
    if (q!=0) homMaskStoreF{rv+e, maskOf{V, q}, s}
  }
}

if_inline (hasarch{'AVX2'}) {
  def rep_iter_from_sh{sh}{x, gen} = {
    def l = length{sh}
    def h = l>>1
    def fs{v, s} = gen{sel{[16]i8, v, s}}
    a := shuf{[4]u64, x, 4b1010}; each{fs{a,.}, slice{sh,0,h}}
    if (l%2) fs{x, select{sh, h}}
    b := shuf{[4]u64, x, 4b3232}; each{fs{b,.}, slice{sh,-h}}
  }
  
  def get_rep_iter{V, wv==2}{x, gen} = {
    def s = shuf{[4]u64, x, 4b3120}
    each{{q}=>gen{V~~q}, mzip128{s, s}}
  }
  def get_rep_iter{V==[4]u64, wv} = {
    def step = 4
    def sh = each{base{4,.}, get_shufs{step, wv, wv}}
    {x, gen} => each{{s}=>gen{shuf{V, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv, xv:*V, rv:*V, n:(u64)} = rep_const_shuffle{wv, get_rep_iter{V, wv}, xv, rv, n}
  
} else if_inline (hasarch{'AARCH64'}) {
  
  def rep_iter_from_sh{sh}{x, gen} = {
    each{{s} => gen{sel{[16]u8, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv==2, xv0:*V=[_]T, rv0:*V, n:(u64)} = {
    def E = ty_u{T}
    rv:= *E~~rv0
    @for (x in *E~~xv0 over i to n) { # autovectorized well enough, probably
      store{rv, i*2, x}
      store{rv, i*2+1, x}
    }
  }
}

fn rep_const_shuffle_partial4(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  def h = 4
  def V = sdtype
  def sh = read_shuf_vecs{h, ellw, *V~~rcsh4_dat + h*load{rcsh4_lkup,wv}}
  def [step]_ = V          # Bytes written
  def wvb = wv << ellw
  def hs = (h*step) / wvb  # Actual step size in argument elements
  def shufbase{i if hasarch{'AVX2'}} = shuf{[4]u64, load{*V~~(x+i)}, 4b1010}
  def shufbase{i if hasarch{'AARCH64'}} = load{*V~~(x+i)}
  def shufrun{a, s} = sel{[16]i8, a, s} # happens to be the same across AVX2 & NEON
  
  i:u64 = 0
  re := r + n*wvb - h*step
  while (r <= re) {
    a := shufbase{i}
    @unroll (j to h) store{*V~~r, j, shufrun{a, select{sh,j}}}
    i += hs << ellw
    r += hs*wvb
  }
  re+= (h-1)*step
  a:= shufbase{i}
  s:= V**0
  
  def end = makelabel{}
  @unroll (j to h) {
    s = shufrun{a, select{sh,j}}
    if (r > re) goto{end}
    store{*V~~r, 0, s}
    r+= step
  }
  setlabel{end}
  
  q := (re+step) - r
  if (q!=0) homMaskStoreF{*V~~r, maskOf{V, q}, s}
}



fn rcsh_sub{wv, V}(ellw:u64, x:*i8, r:*i8, n:u64, sh:*V) : void = {
  def st = read_shuf_vecs{wv, ellw, sh}
  rep_const_shuffle{wv, rep_iter_from_sh{st}, *V~~x, *V~~r, n}
}

fn rep_const_shuffle_any(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  if (wv > select{rcsh_vals,-1}) {
    return{rep_const_shuffle_partial4(wv, ellw, x, r, n)}
  }
  n <<= ellw
  ri := wv - select{rcsh_vals,0}
  sh := *sdtype~~rcsh_data + load{rcsh_offs,ri}
  def try{k} = { if (wv==k) rcsh_sub{k, sdtype}(ellw, x, r, n, sh) }
  each{try, rcsh_vals}
}

def rep_const_broadcast{T, kv, loop, wv:(u64), x:*T, r:*T, n:(u64)} = {
  assert{kv > 0}
  def V = [arch_defvw/width{T}]T
  @for (x over n) {
    v := V**x
    @loop (j to kv) store{*V~~r, j, v}
    r += wv
    store{*V~~r, -1, v}
  }
}
fn rep_const_broadcast{T, kv    }(wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, unroll, wv, x, r, n}
fn rep_const_broadcast{T}(kv:u64, wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, for   , wv, x, r, n}

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  assert{wv>=2}
  if (wv>=8 and wv<=fact_size) {
    fa := promote{u64, load{factors,wv-8}}
    if (fa > 1) {
      fi := wv / fa
      def t = *void~~(*T~~r + (promote{u64,wv}-fi)*n)
      rep_const{T}(fi,x,t,n)
      rep_const{T}(fa,t,r,fi*n)
      return{}
    }
  }
  def wT = width{T}
  def vn = arch_defvw/wT
  def V = [vn]T
  def max_shuffle = 2*vn
  if (wv <= max_shuffle) {
    def specialize{k} = {
      if (wv==k) return{rep_const_shuffle{k, *V~~x, *V~~r, n}}
    }
    if (special_2) specialize{2}
    rep_const_shuffle_any(wv, lb{wT/8}, *i8~~x, *i8~~r, n)
  } else {
    kv := wv / vn
    @unroll (k from (max_shuffle/vn) to 4) {
      if (kv == k) return{rep_const_broadcast{T, k}(wv, *T~~x, *T~~r, n)}
    }
    rep_const_broadcast{T}(kv, wv, *T~~x, *T~~r, n)
  }
}

}

exportT{'si_constrep', each{rep_const, dat_types}}



# Constant replicate on boolean
fn rep_const_bool{}(wv:usz, x:*u64, r:*u64, rlen:usz) : u1 = {
  if (wv > 64) return{0}
  nw := cdiv{rlen, 64}
  if (wv&1 == 0) {
    p := ctz{wv | 8} # Power of two for second replicate
    wf := wv>>p
    if (wf == 1) {
      rep_const_bool_div8{wv, x, r, nw}
    } else {
      tlen := rlen>>p
      wq := usz~~1<<p
      if ((not hasarch{'SSSE3'} and (p == 1 or (p == 2 and wv>=52))) or (p == 1 and wv>=24)) {
        # Expanding odd second is faster
        tlen = rlen / wf
        t:=wf; wf=wq; wq=t
      }
      t := r + cdiv{rlen, 64} - cdiv{tlen, 64}
      rep_const_bool{}(wf, x, t, tlen)
      rep_const_bool{}(wq, t, r, rlen)
    }
  } else {
    rep_const_bool_odd{wv, x, r, nw}
  }
  1
}

def rep_const_bool_div8{wv, x, r, nw} = { # wv in 2,4,8
  def run{k} = {
    # 2 -> 64w0x33, 12 -> 64w0x000f, etc.
    def getm{sh} = base{2, iota{64}&sh == 0}
    def osh{v, s} = v | v<<s
    def expand = match (k) {
      {2} => fold{
        {v, sh} => osh{v, sh} & getm{sh},
        ., 1 << reverse{iota{5}}
      }
      {4} => fold{
        {v, sh} => osh{osh{v, sh}, 2*sh} & getm{sh},
        ., tup{12, 3}
      }
      {8} => {
        def mult = base{1<<7, 8**1}
        {x} => (x | ((x&~1) * mult)) & 64w0x01
      }
    }
    @for (xt in *ty_u{64/k}~~x, r over nw) {
      def v = expand{promote{u64, xt}}
      r = v<<k - v
    }
  }
  def cases{k} = if (wv==k) run{k} else if (k<8) cases{2*k}
  cases{2}
}

def get_boolvec_writer{V, r, nw} = {
  def vwords = width{V}/64
  rv := *V~~r
  re := rv + nw / vwords
  last_res:V = V**0
  def end = makelabel{}
  def output{v:(V)} = {
    last_res = v
    if (rv==re) goto{end}
    store{rv, 0, v}; ++rv
  }
  def flush{} = {
    setlabel{end}
    q := nw & (vwords-1)
    if (q != 0) homMaskStoreF{rv, V~~maskOf{re_el{u64,V}, q}, last_res}
  }
  tup{output, flush}
}

def rep_const_bool_div8{wv, x, r, nw if hasarch{'SSSE3'}} = {
  oper // ({a,b}=>floor{a/b}) infix left  40
  def avx2 = hasarch{'AVX2'}
  def vl = if (avx2) 32 else 16
  def V = [vl]u8
  def iV = iota{vl}
  def mkV = make{V, .}
  def selH = sel{[16]u8, ., .}
  def makeTab{t} = selH{mkV{if (avx2) merge{t,t} else t}, .}
  def id{xv} = xv
  def {output, flush} = get_boolvec_writer{V, r, nw}

  def run24{x, proc_xv, exh} = {
    i:usz = 0; while (1) {
      xv := proc_xv{load{*V~~(x+i)}}; ++i
      # Store 1 or 2 result vectors
      def getr = zip128{exh{xv}, exh{V~~(re_el{u16,V}~~xv>>4)}, .}
      output{V~~getr{0}}
      output{V~~getr{1}}
    }
  }
  if (wv == 2) {
    def init = if (avx2) shuf{[4]u64, ., 4b3120} else id
    # Expander for half byte
    def tabr = makeTab{tr_iota{2*iota{4}} * 2b11}
    m4 := V**0xf
    run24{*V~~x, init, {x} => tabr{x & m4}}
  } else if (wv == 4) {
    # Unzip 32-bit elements (result lanes) across AVX2 lanes
    def pre = if (avx2) sel{[8]u32, ., make{[8]u32,tr_iota{1,2,0}}} else id
    def init{xv} = { u:=pre{xv}; zip128{u,u,0} }
    # Expander for two bits in either bottom or next-to-bottom position
    def tabr = makeTab{tr_iota{0,4,0,4} * 2b1111}
    m2 := mkV{2b11 << (2*(iV%2))}
    def exh{x} = re_el{u16, V}~~tabr{x & m2}
    run24{*(if (avx2) [2]u64 else u64)~~x, init, exh}
  } else { # wv == 8
    i:usz = 0; while (1) {
      xh := load{*[16]u8~~(*ty_u{vl}~~x + i)}; ++i
      xv := if (avx2) pair{xh, xh} else xh
      xe := selH{xv, mkV{iV // 8}}
      output{(xe & mkV{1 << (iV % 8)}) > V**0}
    }
  }
  flush{}
}

# Data for the permutation that sends bit i to k*i % width{T}
def modperm_dat{T, k} = {
  def w = width{T}
  def i = iota{lb{w}}
  def bits = ~(1 & (k*iota{w} >> merge{0, replicate{1<<i, i}}))
  match (T) {
    {[_]E} => make{T, each{base{2,.}, split{width{E}, bits}}}
    {_}    => T~~base{2, bits}
  }
}
def modperm_step{x, l, m} = {
  def d = (x ^ x<<l) & m
  x ^ (d | d>>l)
}
def modperm_step{x:T, l=(width{T}/2), m} = {
  def mm = m | m>>l
  (x &~ mm) | ((x<<l | x>>l) & mm) # rotate
}
def modperm_step{x:T=[_](u8), l, m} = {
  def W = re_el{u64, T}
  T~~modperm_step{W~~x, l, W~~m}
}
def swap_elts{x:V=[vl](u8), el_bytes} = { # Reverse each pair of elements
  def swi{len, l} = { def i = iota{len}; i + (l - 2*(i&l)) }
  if (el_bytes >= 4) {
    def wd = max{4, el_bytes/2}
    shuf{[4]ty_u{wd*8}, x, base{4, swi{4, el_bytes/wd}}}
  } else {
    def i = swi{16, el_bytes}
    sel{[16]u8, x, make{V, cycle{vl, i}}}
  }
}
def modperm_step{x, l, m:V=[_]T if l%8==0} = {
  (x & m) | (swap_elts{x, l/8} &~ m)
}
def modperm_get_byteperm{sw_bytes:V=[_](u8) if hasarch{'SSSE3'}} = {
  def shW{op, v, s} = V~~op{re_el{u64,V}~~v, s}
  m4 := V**0xf
  t0 := fold{{v,a}=>modperm_step{v,...a}, make{V,iota{vcount{V}}%16}, tup{
    tup{4, sw_bytes &~ m4},
    tup{2, ({v} => v|shW{<<, v, 4}){sw_bytes&(V**0xc)}}
  }}
  t4 := shW{<<, t0&m4, 4} | shW{>>, t0&~m4, 4}
  def selI = sel{[16]u8, ., .}
  {xv} => selI{t0, xv & m4} | selI{t4, shW{>>, xv, 4} & m4}
}
def fold_multi{f, init, ...ls} = fold{{v,a}=>f{v,...a}, init, flip{ls}}

def advance_spaced_mask{k, m, sh} = m<<(k-sh) | m>>sh

# General-case loop for odd replication factors
def rep_const_bool_odd_mask4{
  M,             # read/write type
  k,             # replication factor
  get_modperm_x, # permuted input
  output, n,     # output, number of writes
  mask:(u64),    # starting mask
  mask_sh        # single iteration shift
} = {
  def ifvec{g} = match (M) { {[_](u64)} => g; {_} => ({v}=>v) }
  def scal = ifvec{{v} => M**v}

  # Fundamental operation: shifts act as order-k cyclic group on masks
  def advance{m, sh} = advance_spaced_mask{k, m, sh}
  # Double a cumulative mask, shift combination
  # If s advances l iterations, mc combines iterations iota{l}
  def double_gen{comb}{{mc, s}} = {
    def mn = comb{mc, advance{mc,s}}
    def ss = s+s
    tup{mn, ss - (k &- (ss>k))}
  }
  # Mask and shift for one iteration
  def fillmask{T} = match (T) {
    {(u64)} => tup{mask, mask_sh}
    {[2]E}  => double_gen{make{T,...}}{fillmask{E}}
    {[4]E}  => double_gen{pair}{fillmask{[2]E}}
  }
  {sm0, s1} := fillmask{M}
  # Combined mask for 4 iterations, and shift to advance 4
  def double = double_gen{|}
  {mc4, s4} := double{double{tup{sm0, s1}}}
  # Submasks pick one mask out of a combination of 4
  def or_adv{m, s} = { m |= advance{m,s} }
  @for (min{k/4 - 1, n/4}) or_adv{sm0,s4}
  submasks := scan{advance, tup{sm0, ...3**s1}}
  mask_tail := advance{sm0, s4} &~ sm0

  # Carry: shifting and word-crossing is done on the initial permuted x
  # No need to carry across input words since they align with output words
  # First bit of each word in xo below is wrong, but it doesn't matter!
  mr := scal{u64~~1<<k - 1}  # Mask out carry bit before output
  def sub_carry{a, c} = match (M) {
    {[l](u64)} => {
      ca := if (hasarch{'SSE4.2'}) { def S = [l]i64; S~~c > S**0 }
            else { def S = [2*l]i32; cm := S~~c; cm != shuf{S, cm, 4b2301} }
      a + M~~ca
    }
    {_} => a - promote{u64, c != 0}
  }

  while (1) {
    x:M = get_modperm_x{}
    def vrot1 = ifvec{{x} => if (w128{M}) vshl{x, x, vcount{type{x}}-1}
                             else shuf{M, x, 4b2103}}
    xo := x<<k | vrot1{x>>(64-k)}
    # Write result word given starting bits
    def step{b, c} = output{sub_carry{c - b, c & mr}}
    def step{b, c, m} = step{b&m, c&m}
    # Fast unrolled iterations
    mask := mc4
    @for (k/4) {
      each{step{x & mask, xo & mask, .}, submasks}
      mask = advance{mask, s4}
    }
    # Single-step for tail
    mask = mask_tail
    @for (k%4) {
      step{x, xo, mask}
      mask = advance{mask, s1}
    }
  }
}

def rep_const_bool_odd{k, xp, rp, nw} = {
  # Every-k-bits mask
  {mask, mask_sh} := unaligned_spaced_mask_mod{k}

  # Transform sending bit i to k*i % 64 by pairwise swaps
  # Swap data goes in a pre-computed table
  swtab:*u64 = each{modperm_dat{u64, .}, 1+2*iota{32}}
  def swap_lens = reverse{2 << iota{5}}
  swap_data := load{swtab, k>>1}
  swsel:u64 = ~u64~~0
  def gsw{l} = {
    swsel ^= swsel << l  # Low l bits out of every 2*l
    sm := swap_data &~ swsel
    swap_data &= swsel; swap_data |= swap_data<<l
    sm
  }
  swap_masks := each{gsw, swap_lens}
  i:usz = 0
  # Load x, send bit i to position k*i % 64
  def swap_x = fold_multi{modperm_step, ., swap_lens, swap_masks}
  def get_swap_x{} = { x := swap_x{load{xp, i}}; ++i; x }

  # Output
  j:usz = 0
  def output{rw} = {
    store{rp, j, rw}
    ++j; if (j==nw) return{1}
  }
  # Dedicated loop for 3, shared for other factors
  if (k == 3) while (1) {
    def unrolled_iter{k} = {
      def reps{f, n, init} = scan{{x,_}=>f{x}, n**init}
      def masks = reps{advance_spaced_mask{k, ., 64%k}, k, mask}
      def step{x}{o, m, n_over} = {
        b := x & m
        def os = (64-k) + 1 + iota{n_over}
        output{fold{|, (b<<k) - b, each{>>{o,.}, os}}}
        b
      }
      fold_multi{step{get_swap_x{}}, 0, masks, (-iota{k}*64)%k}
    }
    unrolled_iter{3}
  } else {
    rep_const_bool_odd_mask4{u64, k, get_swap_x, output, nw, mask, mask_sh}
  }
}

# For odd numbers:
# - permute each byte sending bit i to position k*i % 8
# - replicate each byte by k, making position k*i contain bit i
# - mask out those bits and spread over [ k*i, k*(i+1) )
# - ...except where it crosses words; handle this overhang separately
def rep_const_bool_odd{k, x, r, nw if hasarch{'SSSE3'}} = {
  def avx2 = hasarch{'AVX2'}
  def vl = if (avx2) 32 else 16; def V = [vl]u8
  def iV = iota{vl}
  def mkV = make{V, .}; def selV = sel{[16]u8, ., .}
  def W = re_el{u64, V}
  def {output, flush} = get_boolvec_writer{W, r, nw}

  # Swap data goes in a pre-computed table
  swtab:*V = each{modperm_dat{V, .}, 1+2*iota{32}}
  swap_data := load{swtab, k>>1}
  swap_lane := if (avx2) shuf{[4]u64, swap_data, 4b1010} else swap_data
  # Within-byte transformation
  def perm_x = modperm_get_byteperm{selV{swap_lane, V**0}}
  def sp_max = if (avx2) 4 else 8
  if (k < sp_max) {
    rep_const_bool_odd_special{V, sp_max, k, x, perm_x, {v}=>output{W~~v}}
  } else {
    {m, d} := unaligned_spaced_mask_mod{k}
    # General case
    def get_mask{l} = selV{~swap_lane, mkV{l+iV%l}}
    def get_mask{16} = shuf{[4]u64, ~swap_data, 4b3232}
    def swap_lens = reverse{1 << iota{4 + avx2}}
    swap_masks := each{get_mask, swap_lens}
    def swap_x = fold_multi{modperm_step, ., 8*swap_lens, swap_masks}
    i:usz = 0
    def get_swap_x{} = { xv := W~~perm_x{swap_x{load{*V~~x, i}}}; ++i; xv }
    rep_const_bool_odd_mask4{W, k, get_swap_x, output, cdiv{nw, vcount{W}}, m, d}
  }
  flush{}
}

def rep_const_bool_odd_special{V=[vl](u8), max_wv, wv, x, perm_x, output} = {
  oper // ({a,b}=>floor{a/b}) infix left  40
  def iV = iota{vl}
  def mkV = make{V, .}; def selV = sel{[16]u8, ., .}
  def W = re_el{u64, V}
  if (max_wv <= 4 or wv < 4) {
    def ll = 16; def dup{v} = if (vl>ll) merge{v,v} else v
    # 3: dedicated loop
    i:usz = 0; while (1) {
      # 01234567 to 05316427 on each byte
      xv := perm_x{load{*V~~x, i}}; ++i
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{3},1} // 3  # bits that overhang within a word
      def ib = ix // 8                   # byte index
      def io = 8*ib + 3*ix%8             # where they are in xv
      def wi = split{vl/8, dup{tup{255, ...ib, 255, ...8+ib}}}
      xo := V~~((W~~xv & W**fold{|, 1<<io}) >> (8-3))
      xo += xo > V**0
      # Permute and mask bytes
      def step{jj, oi, ind, mask} = {
        def getv = if (vl==ll or jj==1) ({x}=>x)
                   else shuf{[4]u64, ., 4b1010 + 4b2222*(jj>1)}
        def selx{x, i} = sel{[ll]u8, getv{x}, i}
        b := W~~(selx{xv, ind} & mask)
        r := V~~((b<<3) - b)
        o := selx{xo, mkV{flat_table{max, oi, 255*(0<iota{8})}}}
        output{r|o}
      }
      def make3V{vs} = each{make{V,.}, split{vl, dup{vs}}}
      each{step,
        iota{3}, wi,
        make3V{replicate{3, iota{ll}}},
        make3V{8w2b001 << ((-8)*iota{3*ll} % 3)}
      }
    }
  } else {
    assert{w128{V}}
    def selV = sel{V, ., .}
    # 5, 7: precompute constants, then shared loop
    {xom, xse, ind0, mask0, ind_up, ind_inc, mask_sh} := undef{tup{
     W,   V,   V,    V,     V,      V,       usz   }}
    def set_consts{k} = {
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{k},1} // k  # bits that overhang within a word
      def ib = ix // 8                   # byte index
      def io = 8*ib + k*ix%8             # where they are in xv
      def wi = tup{255, ...ib, 255, ...8+ib}
      xom = W**fold{|, 1<<io}
      xse = mkV{join{flip{split{2, shiftright{wi, vl**255}}}}}
      # Permutation to expand by k bytes, and every-k-bits mask
      ind0  = mkV{iV // k}
      mask0 = mkV{((1<<k|1) << ((-8)*iV % k)) % 256}
      def iu = iV + vl%k; def ia = iu>=vl
      ind_up  = mkV{iu - k*ia}
      ind_inc = mkV{vl//k + ia}
      mask_sh = width{V} % k
    }
    if (wv == 5) set_consts{5} else set_consts{7}
    xv:V = V**0; xo:=xv; ind:=xv; mask:=xv  # state
    i:usz = 0; q:usz = 1
    while (1) {
      --q; if (q == 0) { q = wv
        # Load and permute bytes
        xv = perm_x{load{*V~~x, i}}; ++i
        # Bytes for overhang
        xo = V~~((W~~xv & xom) >> (8 - wv))
        xo += xo > V**0
        xo = selV{xo, xse}
        # Initialize state vectors
        ind  = ind0
        mask = mask0
      } else {
        # Update state vectors
        xo   = shr{V, xo, 1}
        ind  = selV{ind, ind_up} + ind_inc
        mask = V~~((W~~mask << (wv - mask_sh)) | (W~~mask >> mask_sh))
      }
      b := W~~(selV{xv, ind} & mask)
      rv:= V~~((b<<wv) - b)
      o := xo & mkV{255 * (iV%8 == 0)} # overhang
      output{rv | o}
    }
  }
}

export{'si_constrep_bool', rep_const_bool{}}
