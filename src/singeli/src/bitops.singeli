def load_bit{x:(*u64), n:(ux)} = {
  ((load{x,n>>6}>>(n&63)) & 1) != 0
}

def load_bits_lo{sz, x:(*u64), n:(ux)} = match (sz) {
  {2} => (load{*u8~~x, n>>2} >> cast_i{u8, (n&3)*2})
  {4} => (load{*u8~~x, n>>1} >> cast_i{u8, (n&1)*4})
  {sz if sz>=8} => load{*ty_u{sz}~~x, n}
}

def load_bits{sz, x:(*u64), n:(ux)} = match (sz) {
  {2} => load_bits_lo{sz, x, n} & 3
  {4} => load_bits_lo{sz, x, n} & 15
  {sz if sz>=8} => load{*ty_u{sz}~~x, n}
}


def store_bit{x:(*u64), n:(ux), v:(u1)} = {
  m:u64 = cast{u64,1}<<(n&63)
  p:u64 = load{x,n>>6}
  if (v) store{x,n>>6,p |  m}
  else   store{x,n>>6,p & ~m}
}

def store_bits{sz, x:(*u64), n:(ux), v} = match (sz) {
  {4} => {
    x8:= *u8 ~~ x
    
    #w:u64 = cast_i{u64, load{x8,n/2}}
    #sh:u64 = (n&1) * 4
    #w&= ~(15<<sh)
    #w|=  (cast_i{u64,v}<<sh)
    
    w:u8 = load{x8,n/2}
    if ((n&1)==1) {
      w&= ~(cast{u8,15}<<4)
      w|=  (cast_i{u8,v}<<4)
    } else {
      w&= ~(cast{u8,15})
      w|=  (cast_i{u8,v}<<0)
    }
    
    store{x8, n/2, cast_i{u8,w}}
  }
  { 8} => store{*u8  ~~ x, n, cast_i{u8, v}}
  {16} => store{*u16 ~~ x, n, cast_i{u16,v}}
  {32} => store{*u32 ~~ x, n, cast_i{u32,v}}
  {64} => store{        x, n, cast_i{u64,v}}
  {sz} => {
    vc:u64 = promote{u64,v}
    am:u64 = 64/sz
    w:u64 = load{x,n/am}
    sh:u64 = (n&(am-1)) * sz
    w&= ~(u64~~tail{sz} << sh)
    w|= (vc<<sh)
    store{x, n/am, w}
  }
}

def expand_bits{T==[32]u8, a:(u32)} = {
  def idxs = iota{32}
  b:= [8]u32**a
  c:= [32]u8~~b
  d:= shuf{[16]u8, c, make{[32]i8, idxs>>3 + bit{4, idxs}}}
  e:= make{[32]u8, 1<<tail{3, idxs}}
  e == (d&e)
}

def expand_bits{T==[16]u8, a:(u16) if hasarch{'AARCH64'}} = {
  b:= sel{T, T~~[8]u16**a, make{[16]i8, iota{16}>=8}}
  andnz{b, make{T, 1<<(iota{16}&7)}}
}
def expand_bits{T==[16]u8, a:(u16) if hasarch{'X86_64'}} = {
  b:= T~~[8]u16**a
  exp:= T~~shuf{[4]i32, vec_shuffle16_lo{mzip{b,b,0}, tup{0,0,1,1}}, 0,0,1,1}
  (exp & make{T, 1<<(iota{16}&7)}) != T**0
}

def expand_bits{V=[k]T, a:A if k<=width{T} and quality{T}=='u'} = {
  b:= make{V, 1<<iota{k}}
  b == (b & V ~~ re_el{A, V}**a) # not just V**a so that if a is read from RAM, it can use the single instruction for broadcasting from RAM; the extra bits don't matter
}

# vector with type V with each element being either all 0s or 1s
def load_expand_bits{V=[k]T, x:(*u64), n:(ux)} = {
  expand_bits{V, load_bits_lo{k, x, n}}
}

# load bits starting at bit i, leaving garbage at the top. Only the bottom 57 bits are guaranteed correct; 58 and 60 will be correct if `i` is a multiple of it
def loadu_bits_raw{x:(*u64), i} = {
  loadu{*u64~~((*u8~~x) + (i>>3))} >> (i&7)
}
def loadu_bits{x:(*u64), i, n} = {
  assert{n<58 or ((n==58 or n==60) and (i%n == 0))}
  loadu_bits_raw{x, i}
}
def loadu_bits_trunc{x:(*u64), i, n if knum{n}} = trunc_bits{n, loadu_bits{x, i, n}}


def load_expand_bits{T, x:(*u64), {...is}} = {
  # def len = length{is}
  # def [count]_ = T
  # assert{count*len <= 64}
  # bits:= load_bits_lo{count*len, x, select{is,0}}
  # @collect(i to len) expand_bits{T, trunc_bits{count, bits>>(i*count)}}
  each{load_expand_bits{T, x, .}, is}
}
