include './base'
include './sse3'
include './avx'
include './avx2'
include './mask'
include './cbqnDefs'
include 'util/tup'

def preserve_negative_zero = 0

def inRangeLen{x:TS, start, count & issigned{eltype{TS}}} = {
  def TU = ty_u{TS}
  (TU~~(x-TS**start))  <  TU**count
}
def inRangeLen{x:TU, start, count & isunsigned{eltype{TU}}} = {
  def TS = ty_s{TU}
  def h = 1 << (elwidth{TU}-1)
  (TS~~(x-TU**(start-h)))  <  TS**(count-h)
}
def inRangeIncl{x:T, start, end} = inRangeLen{x, start, end-start+1}
def inRangeExcl{x:T, start, end} = inRangeLen{x, start, end-start}

def isSNaN{x:T & isunsigned{eltype{T}}} = inRangeLen{x<<1, (0xFFE<<52)+2, (1<<52)-2}
def q_chr{x:T & isvec{T} & eltype{T}==u64} = inRangeLen{x, cbqn_c32Tag{}<<48, 1<<48}

def fold{F, x:T} = {
  show{'WARNING: using fallback fold'}
  def E = eltype{T}
  r:E = 0
  each{{i} => { r = F{r, cast_i{E, extract{x, i}}, iota{vcount{T}}} }}
  promote{u32, r}
}
def fold{F, x:T & w128{T}} = {
  c:= x
  def EW = elwidth{T}
  if (EW<=64) c = F{c, shuf{[4]u32, c, 4b1032}}
  if (EW<=32) c = F{c, shuf{[4]u32, c, 4b2301}}
  if (EW<=16) c = F{c, sel{[16]u8, c, make{[16]i8, iota{16}^2}}}
  if (EW<=8)  c = F{c, sel{[16]u8, c, make{[16]i8, iota{16}^1}}}
  extract{c, 0}
}
def fold{F, x:T & w256{T}} = fold{F, F{half{x, 0}, half{x, 1}}}

def makeOptBranch{enable, F} = {
  if (enable) {
    def skip = makelabel{}; goto{skip}
    def start = setlabel{}
    F{}
    setlabel{skip}
    start
  } else {
    'not defined'
  }
}

squeeze{vw, X, CHR, B}(x0:*void, len:Size) : u32 = {
  def bulk = vw / width{X}
  def XV = [bulk]X
  def E = tern{X==f64, u32, ty_u{X}}
  def EV = [bulk]E
  # show{XV, EV, CHR, B}
  xp:= *X~~x0
  r1:= EV**0
  if (CHR) { # c8, c16, c32
    def hw = width{E}/2
    maskedLoop{bulk, len, {i, M} => {
      c:= EV~~loadBatch{xp, i, XV}
      if (X!=u16) r1|= M{c} # for u64, just accept the garbage top 32 bits and deal with them at the end
      if (B) {
        if (homAny{M{~q_chr{c}}}) return{3}
      } else {
        if (anynePositive{EV**((1<<hw-1)<<hw) & c, EV**0, M}) return{lb{hw}-2}
      }
    }}
    r2:= cast_i{u32, fold{|, r1}}
    if (X>u32 and r2>=65536) return{2}
    if (X>u16 and r2>=256) return{1}
    0
  } else { # i8, i16, i32, f64
    if (X==i8) { # i8
      maskedLoop{bulk, len, {i, M} => {
        v0:= loadBatch{xp, i, XV}
        if (anynePositive{EV**0xfe & EV~~v0, EV**0, M}) return{2}
      }}
      0
    } else { # i16, i32, f64
      iCont:Size = 0
      def case_B = makeOptBranch{B, {} => {
        maskedLoop{bulk, iCont, len, {i, M} => {
          def XU = [bulk]u64
          v:= XU ~~ loadBatch{xp, i, XV}
          if (homAny{M{isSNaN{v}}}) return{0xffff_fffe} # not even a number
        }}
        return{0xffff_ffff} # float
      }}
      
      maskedLoop{bulk, len, {i, M} => {
        v0:= loadBatch{xp, i, XV}
        def toint{x:T & isint{eltype{T}}} = x
        def toint{flt:T & isfloat{eltype{T}}} = {
          int:= narrow{i32, flt}
          assert{width{type{int}} == width{XV}/2} # we'll be doing operations over it
          
          def conv{x} = tern{preserve_negative_zero, ty_u{x}, x}
          
          if (anynePositive{conv{flt}, conv{widen{T, int}}, M}) { # is any not an integer
            if (B) { iCont=i; goto{case_B} } # if B, need to give an even more special result
            else return{0xffff_ffff} # float
          }
          int
        }
        assert{XV == type{v0}}
        v1:= toint{v0} # TODO do with an if instead
        r1|= M{((EV ** ~E~~1) & EV~~v1)  ^  EV~~(v1 >> (width{X}-1))}
      }}
      
      promote{u32, fold{|, r1}}
    }
  }
}

'avx2_squeeze_i8'  = squeeze{256, i8,  0, 0}
'avx2_squeeze_i16' = squeeze{256, i16, 0, 0}
'avx2_squeeze_i32' = squeeze{256, i32, 0, 0}
'avx2_squeeze_f64' = squeeze{256, f64, 0, 0}
'avx2_squeeze_numB'= squeeze{256, f64, 0, 1}

'avx2_squeeze_c16' = squeeze{256, u16, 1, 0}
'avx2_squeeze_c32' = squeeze{256, u32, 1, 0}
'avx2_squeeze_chrB'= squeeze{256, u64, 1, 1}