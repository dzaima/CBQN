def __shl{(u16)}{a:T, b} = T~~(re_el{u16,a}<<b) # for x86's lack of u8 shift
def __shr{(u16)}{a:T, b} = T~~(re_el{u16,a}>>b)
def pow2_up{v, least} = __max{least, 1<<ceil_log2{v}} # least ⌈ ⌈⌾(2⊸⋆⁼) v

# make a LUT of at least nt elements in tab, to be indexed by [ni_real≥ni]u8
# E must be unsigned
# mode is a hint on expected usage:
#   mode=='i': same table is reused for many index batches (list ⊏ list)
#   mode=='c': same index is reused across many LUTs (list⊸⊏˘ mat)
#   mode=='o': neither table nor indices are reused (mat ⊏˘ mat)
# returns {nt_real≥nt, ni_real≥ni, {tab:*E} => {is:[ni_real]u8} => (tuple of vectors, totalling to ni_real elements)}
def lut_gen{mode, E, nt, ni} = 0

local def loader{G} = {
  def proc{mem:*_} = {
    def loader_mtg{V, i} = load{*V~~mem, i}
    def loader_mtg{'offset', E, en} = proc{en + *E~~mem}
  }
  def proc{TG if kgen{TG}} = TG
  def proc{vs if ktup{vs}} = {
    def S = one_type{vs}
    def loader_vtg{Q, i} = {
      if (width{Q} == width{S}) Q~~select{vs,i}
      else if (width{Q}*2 == width{S}) Q~~half{select{vs, i>>1}, i&1}
      else assert{0, S, Q}
    }
    def loader_vtg{'offset', E, en} = {
      def off = (width{E} * en) / width{S}
      assert{off != length{vs}, vs, E, en}
      if (off==0.5) proc{tup{half{select{vs,0},1}}}
      else proc{slice{vs, off}}
    }
  }
  def load_accepter{...vs} = G{proc{...vs}}
}

def unzip_load{E, n, TG} = each{merge, unzip_load{E, n/2, TG}, unzip_load{E, n/2, TG{'offset', E, n}}}
def unzip_load{E, n, TG if width{E}*n <= arch_defvw{}} = each{tup, unzip{TG{[n]E, 0}, TG{[n]E, 1}}}

def widen_tup{(u32), is:([16]u8)} = tup{ # compiler will deduplicate all the repeated calls of this on the same is
  widen{[8]u32, is},
  widen{[8]u32, shuf{[2]u64, is, 1,1}},
}

def blend_halves{mode, E, nt, ni} = tup{nt, ni, loader{{TG} => {
  def nth = nt/2
  def {(nth), (ni), prev} = lut_gen{mode, E, nth, ni}
  def lo = prev{TG}
  def hi = prev{TG{'offset', E, nth}}
  def me{'raw'} = tup{lo, hi}
  def me{'xor', it} = each{{a,b} => a{'xor',b}, me{'raw'}, it{'raw'}}
  
  # def me{'is', is:[_](u8)} = lo{'is', is}
  # def me{...is} = {
  #   def [_]IE = one_type{is}
  #   def shl = if (IE==u8) __shl{u16} else __shl
  #   def bm = shl{is, width{IE}-1 - lb{nth}}
  #   each{{l,h} => blend_top{l,h,bm}, lo{is}, hi{is}}
  # }
  def me{is:[_](u8) if hasarch{'X86_64'} and E==u32} = {
    each{{l,h,m} => blend_top{l,h,m}, lo{is}, hi{is}, each{{c} => c << (31 - lb{nth}), widen_tup{E, is}}}
  }
  
  def me{is:[_](u8) if hasarch{'X86_64'} and E==u8} = {
    def bm = is <<{u16} (7 - lb{nth}) # TODO for outermost bit could do a cmpgt, increasing port diversity
    each{{l,h} => blend_top{l,h,bm}, lo{is}, hi{is}}
  }
  # def me{is:[_](u8) if hasarch{'AARCH64'} and E==u8} = { # only for one outermost blend
  #   def end = type{is}**(nth-1)
  #   def bm = is > end
  #   each{{l,h} => blend_hom{l,h,bm}, lo{is & end}, hi{is & end}}
  # }
  
  # TODO xor-ing could still be worth it for lower repeated levels where the index transformation can be deduped; and outermost can do a cmpgt
  # if (mode=='c') hi{'xor', lo}
  # def me{is:I=[_](u8) if mode=='c'} = {
  #   def bm = (is & I**lb{nth})
  #   each{{l,h} => blend_top{l,h,bm}, lo{is}, hi{is}}
  # }
}}}

def raw_widen_inds{[k]D, x:[k0]S if k0>=k} = { # : [k*sc]S
  def sc = width{D} / width{S}
  def RV = [k*sc]S
  def add = make{RV, range{k*sc} % sc}
  if (hasarch{'AARCH64'} and [k]D == [2]u64 and S==u8) {
    def x16 = undef_promote{[16]u8, x}
    (RV~~shuf{[16]u8, x16, range{16}>>3}<<3) + add
  } else if (hasarch{'AVX2'} and [k]D == [4]u64 and S==u32) {
    (shuf{[8]u32, undef_promote{[8]u32, x}, range{8}>>1}<<sc) + add
  } else {
    def wd = widen{[k]D, x}
    re_el{S, wd * [k]D**base{1<<width{S}, sc**sc}} + add
  }
}
def raw_widen_inds{k, sc, x:[_](u8)} = raw_widen_inds{[k]primtype{'u', 8<<sc}, x}

def widen_inds{mode, E, nt0, ni0, sc} = match(lut_gen{mode, primtype{'u',width{E}/sc}, nt0*sc, ni0*sc}) { # e.g. sc==2: {a,b,c,d}[w,x,y,z] → {a0,a1, b0,b1, c0,c1, d0,d1}[w*2,w*2+1, x*2,x*2+1, y*2,y*2+1, z*2zw*2+1]
  {{nt1, ni1, G}} => tup{nt1/sc, ni1/sc, loader{{TG} => {
    def prev = G{TG}
    def ni = ni1/sc
    def WV = [ni]primtype{'u', 8*sc}
    {is:([ni]u8)} => {
      each{re_el{E,.}, prev{raw_widen_inds{WV, is}}}
    }
  }}}
  {x} => x
}

def zip_halves{mode, E, nt, ni} = match(lut_gen{mode, w_h{E}, nt, ni}) { # e.g. {a,b,c,d}[w,x,y,z] → zip({a0,b0,c0,d0}[w,x,y,z], {a1,b1,c1,d1}[w,x,y,z])
  {{nt, ni, G}} => tup{nt, ni, loader{{TG} => {
    # show{E, '→', w_h{E}}
    def d = unzip_load{w_h{E}, nt, TG}
    # lprintf{tup{'x0',d}}
    def prevs = each{G, d}
    def run_zip{zipper, is} = {
      def {lo, hi} = each{{prev}=>prev{is}, prevs}
      join{each{zipper, lo, hi}}
    }
    def me{is:([ni]u8)} = {
      run_zip{mzip, is}
    }
    def me{is:([ni]u8) if hasarch{'X86_64'} and E==u64 and ni==16} = {
      def is2 = shuf{u8, is, tr_iota{0,2,1,3}}
      run_zip{mzip128, is2}
    }
    def me{is:([ni]u8) if hasarch{'X86_64'} and E==u16 and ni==32} = {
      def is2 = shuf{u64, is, tr_iota{1,0}}
      run_zip{mzip128, is2}
    }
  }}}
  {x} => x
}

# lut_gen order is very important!
def lut_gen{mode, E==u8, nt, ni if hasarch{'AVX2'} and nt<=128 and ni<=32} = blend_halves{mode, E, 128, 32}
def lut_gen{mode, E==u8, nt, ni if hasarch{'AVX2'} and nt<=64 and ni<=32} = blend_halves{mode, E, 64, 32}
def lut_gen{mode, E==u8, nt, ni if hasarch{'AVX2'} and nt<=32 and ni<=32} = blend_halves{mode, E, 32, 32}

# generate inds to utilize top bit of pshufb zeroing to replace vpblendvb with with vpor
def lut_gen{'c',  E==u8, nt, ni if hasarch{'AVX2'} and nt<=64 and ni<=32} = { def vn=pow2_up{nt,16}/16; tup{vn*16, 32, loader{{TG} => {
  def luts = each{{i} => [32]u8**TG{[16]u8, i}, range{vn}}
  {is:([32]u8)} => {
    
    def bi = range{ceil_log2{vn}}
    def bits = each{{o} => is <<{u16} (3-o), bi} # extract bits 0,1,2,3 (as many as needed) from 2b3210xxxx into top bit (x being bits used by pshufb)
    
    def top = [32]u8**128
    def isp = each{{i} => is | (top &~ tree_fold{&, each{{m, o} => if (bit{o,i}!=0) m else ~m, bits, bi}}), range{vn}}
    
    tup{tree_fold{|, each{shuf{[16]u8,.,.}, luts, isp}}}
  }
}}}}

def lut_gen{mode, E==u8, nt, ni if hasarch{'AVX2'} and nt<=16 and ni<=32} = tup{16, 32, loader{{TG} => {
  lut:[32]u8 = [32]u8**TG{[16]u8, 0}
  {is:([32]u8)} => tup{shuf{[16]u8, lut, is}}
}}}

def lut_gen{mode, E==u32, nt, ni if hasarch{'AVX2'} and nt<=32 and ni<=16} = blend_halves{mode, E, 32, 16}
def lut_gen{mode, E==u32, nt, ni if hasarch{'AVX2'} and nt<=16 and ni<=16} = blend_halves{mode, E, 16, 16}

def lut_gen{mode, E==u32, nt, ni if hasarch{'AVX2'} and nt<=8 and ni<=16} = tup{8, 16, loader{{TG} => {
  def lut = TG{[8]u32, 0}
  def me{'idxs', is:([16]u8)} = each{{wis} => tup{wis, shuf{[8]u32, lut, wis}}, widen_tup{u32,is}} # TODO inline, or properly outline
  def me{is:([16]u8)} = each{{{_,v}}=>v, me{'idxs', is}}
}}}

def has512shuf{E if primt{E}} = 0
if_inline(hasarch{'X86_64'}) {
  def has512shuf{E if primt{E}} = hasarch{match (E) { {8}=>'AVX512VBMI'; {16}=>'AVX512BW'; {_}=>'AVX512F' }}
  def avx512_minw{} = 256
  def avx512_maxw{} = 256
  
  def shuf{L=[k]E, {t0:V     }, idxs:I if has512shuf{E} and width{L}==width{V} and width{V}==width{I}} = emit{V, x86_intrin_i{L, 'permutexvar' }, idxs, t0}
  def shuf{L=[k]E, {t0:V,t1:V}, idxs:I if has512shuf{E} and width{L}==width{V} and width{V}==width{I}} = emit{V, x86_intrin_i{L, 'permutex2var'}, t0, idxs, t1}
  
  def lut_gen{mode, E, nt, ni if has512shuf{E} and width{E}*nt <= avx512_maxw{}*2 and width{E}*__max{16,ni} <= avx512_maxw{}} = {
    def ew = width{E}
    def idx_w0 = pow2_up{__max{16,ni}*ew, avx512_minw{}}
    def tab_w  = pow2_up{nt*ew, idx_w0}
    def idx_w  = __max{idx_w0, tab_w/2}
    def nt = tab_w/ew
    def ni = idx_w/ew
    def k = nt/ni
    assert{k==1 or k==2}
    
    tup{nt, ni, loader{{TG} => {
      def lut = each{TG{[ni]E, .}, range{k}}
      {is:([ni](u8))} => tup{shuf{[ni]E, lut, widen{[ni]E, is}}}
    }}}
  }
}


# def lut_gen{mode, E==u8, nt, ni if hasarch{'AARCH64'} and nt<=128 and ni<=16} = blend_halves{mode, E, 128, 16}
def lut_gen{mode, E==u8, nt, ni if hasarch{'AARCH64'} and nt<=16*4 and ni<=16} = { def vn=pow2_up{nt,16}/16; tup{vn*16, 16, loader{{TG} => { # TODO could maybe accept nt==48
  def lut = each{TG{[16]u8, .}, range{vn}}
  {is:([16]u8)} => tup{sel{lut, is}}
}}}}
# TODO E==u8 nt==128 via tbx4(tbl4,d,i-64)

def lut_gen{mode, E, nt, ni if mode=='i' and hasarch{'AVX2'} and (E==u16 or E==u64) and ~has512shuf{E}} = zip_halves{mode, E, nt, ni}
# def lut_gen{mode, E, nt, ni if mode=='c' and hasarch{'AVX2'} and E==u16} = widen_inds{mode, E, nt, __max{ni,16}, 2}
# def lut_gen{mode, E, nt, ni if mode=='c' and hasarch{'AVX2'} and E==u64} = widen_inds{mode, E, nt, ni, 2}
# def lut_gen{mode, E, nt, ni if mode=='c' and E==u64} = zip_halves{mode, E, nt, ni} # widen_inds{mode, E, nt, __max{ni,16}, 2}

def lut_gen{mode, E==u64, nt, ni if nt>16 and hasarch{'AVX2'}} = 0


def lut_gen{mode, E, nt, ni if hasarch{'AARCH64'} and (E==u16 or E==u32)} = zip_halves{mode, E, nt, ni}
def lut_gen{mode, E, nt, ni if hasarch{'AARCH64'} and E==u64} = widen_inds{mode, E, nt, ni, 2}

def lut_gen{mode, E, nt, ni if hasarch{'AARCH64'} and mode=='c' and E>=u16} = 0
def lut_gen{mode, E, nt, ni if hasarch{'AARCH64'} and mode=='i' and E==u64 and nt>16} = 0


def lut16{tab:([16]u8), idxs:IV} = shuf{[16]u8, IV**tab, idxs}

def shuf_u8bits{inds:(*u8), ni} = 0
def shuf_u8bits{inds:(*u8), ni if has_sel} = {
  def I = [16]u8
  v0:= I**0
  v1:= I**0
  
  iv:= I**1
  @for (ind in inds over ni) {
    c:= (iota{I} & I**(1<<(ind&3))) != I**0
    c&= iv
    iv = iv+iv
    if (ind>=4) v1|= c
    else v0|= c
  }
  
  {x:V=[_](u8)} => {
    def __shr{a:(V), 4 if hasarch{'X86_64'}} = (a >>{u16} 4) & V**0x0f
    def lo = lut16{v0, x & V**0x0f}
    def hi = lut16{v1, x >> 4}
    lo | hi
  }
}
