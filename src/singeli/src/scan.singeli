include './base'
if_inline (hasarch{'X86_64'}) {
  if_inline (hasarch{'PCLMUL'}) include './clmul'
}
include './mask'
include './f64'
include './spaced'
include './scan_common'
if_inline (hasarch{'AARCH64'}) {
  def __shl{a:V=[_]T, b:U if not vect{U}} = a << V**cast_i{T,b}
  def __shr{a:V=[_]T, b:U if not vect{U}} = a << V**cast_i{T,-b}
}

# Initialized scan, generic implementation
fn scan_scal{T, op}(x:*T, r:*T, len:u64, m:T) : void = {
  @for (x, r over len) r = m = op{m, x}
}

def scan_loop{init, x:*T, r:*T, len:(u64), scan, scan_last} = {
  def step = arch_defvw/width{T}
  def V = [step]T
  p:= V**init
  xv:= *V~~x
  rv:= *V~~r
  e:= len/step
  @for (xv, rv over e) rv = scan{xv,p}
  q:= len & (step-1)
  if (q!=0) store_blended_hom{rv+e, mask_of_first{V, q}, scan_last{load{xv,e}, p}}
}
def get_scan_last{op, pre} = {
  def last{v, p} = op{pre{v}, p}
  def scan{v, p} = {
    n:= last{v, p}
    p = broadcast_last{n}
    n
  }
  tup{scan, last}
}

# Associative scan ?` if a?b?a = a?b = b?a, used for ⌊⌈
def scan_idem = scan_scal
fn scan_idem{T, op if has_simd}(x:*T, r:*T, len:u64, init:T) : void = {
  def {scan, last} = get_scan_last{op, make_scan_idem{T, op}}
  def cmp = match (op) { {(__min)} => (>); {(__max)} => (<) }
  def step = arch_defvw/width{T}
  def V = [step]T
  p:= V**init
  xv:= *V ~~ x
  rv:= *V ~~ r
  e:= len/step
  # Check k vectors at a time to see if they can all be ignored
  def k = 6
  ek := e / k
  @for (ik to ek) { i := ik * k
    def ii = iota{k}
    xvi := each{load{xv + i, .}, ii}
    if (not any_hom{cmp{p, tree_fold{op, xvi}}}) {
      each{store{rv+i, ., p}, ii}
    } else @unroll (rv in rv+i over j to k) {
      rv = scan{select{xvi,j}, p}
    }
  }
  @for (xv, rv over _ from ek*k to e) rv = scan{xv,p}
  q:= len & (step-1)
  if (q!=0) store_blended_hom{rv+e, mask_of_first{V, q}, last{load{xv,e}, p}}
}

export{'si_scan_min_init_i8',  scan_idem{i8 , __min}}; export{'si_scan_max_init_i8',  scan_idem{i8 , __max}}
export{'si_scan_min_init_i16', scan_idem{i16, __min}}; export{'si_scan_max_init_i16', scan_idem{i16, __max}}
export{'si_scan_min_init_i32', scan_idem{i32, __min}}; export{'si_scan_max_init_i32', scan_idem{i32, __max}}
export{'si_scan_min_init_f64', scan_idem{f64, __min}}; export{'si_scan_max_init_f64', scan_idem{f64, __max}}

fn scan_idem_id{T, op}(x:*T, r:*T, len:u64) : void = {
  scan_idem{T, op}(x, r, len, get_id{op, T})
}
export{'si_scan_min_i8',  scan_idem_id{i8 , __min}}; export{'si_scan_max_i8',  scan_idem_id{i8 , __max}}
export{'si_scan_min_i16', scan_idem_id{i16, __min}}; export{'si_scan_max_i16', scan_idem_id{i16, __max}}
export{'si_scan_min_i32', scan_idem_id{i32, __min}}; export{'si_scan_max_i32', scan_idem_id{i32, __max}}

# Assumes identity is 0
def scan_plus = scan_assoc_id0{+}

# Associative scan
def scan_assoc_0 = scan_scal
fn scan_assoc_0{T, op if has_simd}(x:*T, r:*T, len:u64, init:T) : void = {
  # Prefix op on entire SIMD register
  scan_loop{init, x, r, len, ...get_scan_last{op, scan_plus}}
}
export{'si_scan_pluswrap_u8',  scan_assoc_0{u8 , +}}
export{'si_scan_pluswrap_u16', scan_assoc_0{u16, +}}
export{'si_scan_pluswrap_u32', scan_assoc_0{u32, +}}

def rotate_right{x:[l]_} = shuf{x, (iota{l}-1)%l}

def blend_first{x:V=[l]_, y:V} = blend{x, y, 0 < iota{l}}
def shift_first{c:V=[l]_, p:V} = {
  if (l==2) zip{c, p, 0}
  else blend_first{c, rotate_right{p}}
}


# Strided scans
fn scan_stride_assoc{op, T, Ret, check_over}(xv:*void, rv:*void, ia:usz, l:usz) : Ret = {
  def minvalue{(f64)} = -1/0; def maxvalue{(f64)} = 1/0
  def id = match (op) {
    {(__min)} => maxvalue; {(__max)} => minvalue
    {(+)} => ({_}=>0)
  }
  x:= *T~~xv; r:= *T~~rv
  # Architecture determination
  # Use largest vector width with a full-width shuffle
  def has_shuf = hasarch{'SSSE3'} or hasarch{'AARCH64'}
  def I = if (hasarch{'AVX2'} and T>=i32) [8]i32 else [16]i8
  def [il]IE = I; def selI = shuf{IE, ...}
  def wT = width{T}
  def f = wT/width{IE}
  def vl = width{I}/wT
  def V = [vl]T
  if (has_shuf and l < vl) {
    # Small stride: power-of-two shifts
    def small{k} = {
      iv:= iota{I}; j:= I**cast_i{IE,l*f}
      spr:= I**il - j + iv
      def inds = @collect (k) {
        v:= iv - (j &~ lt_hom{I, iv, j})
        spr = selI{spr, v}
        js:= j; j+= j
        if (not same{op, +}) selI{., v}
        else if (same{IE,i8}) selI{., iv - js}
        else { m:= ge_hom{V, iv, js}; {x} => selI{x, v} & m }
      }
      c:= V**id{T}
      @for_masked{vl} (x in tup{V, x}, r in tup{V, r}, M in 'm' over ia) {
        xs:= fold{{v, i} => op{i{v}, v}, x, inds}
        r = c = op{shuf{IE, c, spr}, xs}
        check_over{M, x, r} # For +, infers other argument as r-x
      }
    }
    if (not (same{op,+} and V==[4]f64)) {
      def max_k = lb{vl/2} # Divide by two from assuming l≥2
      if (max_k<3 or l<4) small{max_k} else small{max_k-1} # l=2 and l=3 are the only cases needing the full max_k iterations; max_k<3 limits specialization to where it's significant
    } else { # Non-associative!
      c:= V**0
      if (l==2) {
        @for_masked{vl} (x in tup{V, x}, r in tup{V, r} over ia) {
          a:= c + shuf{x, 0,1,0,1}
          c = a + shuf{x, 2,3,2,3}
          r = blend{a, c, 0,0,1,1}
        }
      } else {
        assert{l==3}
        @for_masked{vl} (x in tup{V, x}, r in tup{V, r} over ia) {
          a:= shuf{c, 1,1,2,3} + blend{x, V**0, 0,1,1,1}
          r = c = x + shuf{a, 1,2,3,0}
        }
      }
    }
  } else {
    # Large stride: single shift, with saved register or memory
    def op_chk{M, p, x} = { r:= op{p, x}; check_over{M, p, x, r}; r }
    @for (r, x over l) r = x
    if (has_shuf and l<256/(wT/8)) {
      # Make sure to load the previous row data at the same alignment to not hit bad store-to-load forwarding
      def [il]IE = I
      q:= l%vl; fq:= cast_i{IE, q*f}
      def rot = shuf{IE, ., (iota{I} - I**fq) & I**(il-1)}
      bv:= iota{I} >= I**fq; def bl = blend_hom{..., bv}
      c:= V**id{T}
      o:= l - q
      if (l == 2*vl) { o = vl; bv = ~bv }
      if (o == vl) {
        p:= load{*V~~x}; store{*V~~r, 0, p}
        @for_masked{vl} (x in tup{V, x+o}, r in tup{V, r+o}, M in 'm' over ia-o) {
          p = rot{p}
          r = op_chk{M, bl{c, p}, x}
          c = p; p = r
        }
      } else {
        @for_masked{vl} (x in tup{V, x+o}, r in tup{V, r+o}, p in tup{V, r}, M in 'm' over ia-o) {
          q:= rot{p}
          r = op_chk{M, bl{c, q}, x}
          c = q
        }
      }
    } else if (same{op, +} and T<=i32 and has_simd and (has_shuf or l>=vl)) {
      def vl = arch_defvw/wT; def V = [vl]T
      @for_masked{vl} (x in tup{V, x+l}, r in tup{V, r+l}, p in tup{V, r}, M in 'm' over ia-l) {
        r = op_chk{M, p, x}
      }
    } else {
      @for (r, x, p in r-l over _ from l to ia) r = op_chk{0, p, x}
    }
  }
  1
}
def scan_stride_assoc{op, T} = scan_stride_assoc{op, T, void, {..._}=>{}}
def check_add_over{_, w:T, x:T, r:T} = { if ((w^r) & (x^r) < 0) return{0} }
def check_add_over{M, w:V=[_]E, x:V, r:V} = {
  o:= (if (not hasarch{'X86_64'} or width{E}<=16) any_hom{M, __subs{r,w} != x}
       else any_top{M, (w^r) & (x^r)})
  if (o) return{0}
}
def check_add_over{M, x, r} = check_add_over{M, r-x, x, r}
export_tab{'si_scan_stride_minmax',
  flat_table{scan_stride_assoc, tup{__min,__max}, tup{i8,i16,i32,f64}}
}
export_tab{'si_scan_stride_add', tup{
  ...each{scan_stride_assoc{+, ., u1, check_add_over}, tup{i8,i16,i32}},
  scan_stride_assoc{+, f64, u1, {..._}=>{}}
}}


# xor scan
def vec_prefix_byshift{op, sh} = {
  def pre{v:V, k} = if (k < elwidth{V}) pre{op{v, sh{v,k}}, 2*k} else v
  {v:T} => pre{v, 1}
}
def scan_word_ne = prefix_byshift{^, <<}
def scan_words_ne = vec_prefix_byshift{^, <<}

fn scan_neq{}(c:u64, x:*u64, r:*u64, nw:usz) : void = {
  @for (x, r over nw) {
    r = c ^ scan_word_ne{x}
    c = -(r>>63) # repeat sign bit
  }
}
fn scan_neq{if has_simd}(c0:u64, x:*u64, r:*u64, nw:usz) : void = {
  def vl = arch_defvw / 64
  def V = [vl]u64
  c := V**c0
  @for_masked{vl} (x in tup{V, x}, r in tup{V, r} over nw) {
    s:= scan_words_ne{x}
    p:= scan_assoc_id0{^}{-(s>>63)} ^ c
    r = s ^ shift_first{c, p}
    c = broadcast_last{p}
  }
}
fn clmul_scan_ne_any{if hasarch{'PCLMUL'}}(x:*void, r:*void, init:u64, words:usz, mark:u64) : void = {
  def V = [2]u64
  m := V**mark
  def xor64{a, i, carry} = { # carry is 64-bit broadcasted current total
    p := clmul{a, m, i}
    t := vec_shift_left_128{p, 1}
    s := p ^ carry
    carry = s ^ t
    s
  }
  xv := *V ~~ x
  rv := *V ~~ r
  e := words/2
  c := V**init
  @for (rv, xv over e) {
    rv = zip{...(@collect (j to 2) xor64{xv, j, c}), 0}
  }
  if ((words & 1) == 1) {
    store{*u64~~(rv+e), clmul{load{V, *u64~~(xv+e), 1}, m, 0} ^ c, 1}
  }
}
fn scan_neq{if hasarch{'PCLMUL'}}(init:u64, x:*u64, r:*u64, nw:usz) : void = {
  clmul_scan_ne_any{}(*void~~x, *void~~r, init, nw, -(u64~~1))
}
fn scan_neq{if hasarch{'AVX512BW', 'VPCLMULQDQ', 'GFNI'}}(init:u64, x:*u64, r:*u64, nw:usz) : void = {
  def V = [8]u64
  def sse{a} = make{[2]u64, a, 0}
  carry := sse{init}
  # xor-scan on bytes
  xmat := V**base{256, tail{8-iota{8}}}
  def xor8 = emit{V, '_mm512_gf2p8affine_epi64_epi8', ., xmat, 0}
  # Exclusive xor-scan on one word
  def exor64 = clmul{., sse{1<<64 - 2}, 0}
  @for (xv in *V~~x, rv in *V~~r over i to cdiv{nw,vcount{V}}) {
    x8 := xor8{xv}
    hb := sse{top_to_int{[64]u8~~x8}}
    xh := exor64{hb}  # Exclusive xor of high bits (xh ^ hb for inclusive)
    xc := xh ^ carry
    v  := x8 ^ V~~mask_to_hom{[64]u8, [64]u1~~extract{xc,0}}
    carry = (xc ^ hb) ^ shuf{u64, xh, 1,1}
    rem:= nw - 8*i
    if (rem < 8) {
      store_masked_hom{*V~~r + i, [8]u1~~(~(u8~~0xff<<rem)), v}
      return{}
    }
    rv = v
  }
}
export{'si_scan_ne', scan_neq{}}

# Boolean cumulative sum
fn bcs{T}(x:*u64, r:*T, l:u64) : void = {
  def bitp_get{arr, n} = (load{arr, n>>6} >> (n&63)) & 1
  c:T = 0
  @for (r over i to l) { c+= cast_i{T, bitp_get{x,i}}; r = c }
}
fn bcs{T if hasarch{'AVX2'}}(x:*u64, r:*T, l:u64) : void = {
  def U = ty_u{T}
  def w = width{T}
  def vl= 256 / w
  def V = [vl]U
  rv:= *V~~r
  xv:= *u32~~x
  c:= V**0
  
  def ii32 = iota{32}; def bit{k}=bit{k,ii32}; def tail{k}=tail{k,ii32}
  def sums{n} = (if (n==0) tup{0} else { def s=sums{n-1}; merge{s,s+1} })
  def widen{v:T} = mzip128{shuf{[4]u64, v, 0,2,1,3}, T**0}
  
  def sumlanes{x:(u32)} = {
    b:= [8]u32**x >> make{[8]u32, 4*tail{1, iota{8}}}
    s:= shuf{16, [32]u8~~b, ii32>>3 + bit{2}}
    p:= s & make{[32]u8, (1<<(1+tail{2})) - 1}  # Prefixes
    d:= shuf{[16]u8, make{[32]u8, merge{sums{4},sums{4}}}, [32]i8~~p}
    d+= shuf{16, d, bit{2}*(1+bit{3}>>2)-1}
    d + shuf{16, d, bit{3}-1}
  }
  def step{x:(u32), i, store1} = {
    d:= sumlanes{x}
    if (w==8) d+= [32]u8~~shuf{[4]u64, [8]i32~~sel8{d, bit{3}<<4-1}, 0,0,1,1}
    j:= (w/8)*i
    def out{v, k} = each{out, widen{v}, 2*k+iota{2}}
    def out{v0:([vl]T), k} = {
      v := V~~v0 + c
      # Update carry at the lane boundary
      if (w!=32 or tail{1,k}) {
        c = sel{[8]u32, spread{v}, make{[8]i32, 8**7}}
      }
      store1{rv, j+k, v}
    }
    out{[32]i8~~d, 0}
  }

  e:= l/32
  @for (xv over i to e) {
    step{xv, i, store}
  }

  if (e*32 != l) {
    def st{p, j, v} = {
      jv := vl*j
      if (jv+vl <= l) {
        store{p, j, v}
      } else {
        if (jv < l) store_blended_hom{rv+j, mask_of_first{V, l - jv}, v}
        return{}
      }
    }
    step{load{xv, e}, e, st}
  }
}
export{'si_bcs8',  bcs{i8}}
export{'si_bcs16', bcs{i16}}
export{'si_bcs32', bcs{i32}}



def addChk{a:T, b:T} = {
  mem:*T = tup{a}
  def bad = emit{u1, '__builtin_add_overflow', a, b, mem}
  tup{bad, load{mem}}
}
def addChk{a:T, b:T==f64} = tup{0, a+b}

def widenFull{E, xs} = {
  merge{...each{{x:X=[k]T} => {
    def tb = width{E} * k
    if (tb<=arch_defvw) tup{widen{[k]E, x}}
    else if (1) {
      assert{tb == 2*arch_defvw}
      tup{
        widen{[k/2]E, half{x,0}},
        widen{[k/2]E, half{x,1}}
      }
    }
  }, xs}}
}

def __floor{x if knum{x}} = x - (x%1)
def maxabsval{T if issigned{T}} = -minvalue{T}
def maxsafeint{T if issigned{T}} = maxvalue{T}
def maxsafeint{T==f64} = 1<<53

fn plus_scan{X, R, O}(x:*X, c:R, r:*R, len:u64) : O = {
  i:u64 = 0
  if (hasarch{'AVX2'}) simd_plus_scan_part{x, c, r, len, i}
  @for_unroll_block{1,1} (js from i to len) {
    def vs = eachx{load, x, js}
    each{{j, v} => {
      def {b,n} = addChk{c, promote{R, v}}
      if (rare{b}) return{j}
      store{r, j, n}
      c = n
    }, js, vs}
  }
  len
}
# Sum as many vector registers as possible; modifies c and i
def simd_plus_scan_part{x:*X, c:R, r:*R, len:(u64), i:(u64)} = {
  def b = __max{width{R}/2, width{X}}
  def bulk = arch_defvw/b
  
  def wd = (X!=R) & (width{X}<32) # whether to widen the working copy one size
  def WE = tern{wd, w_d{X}, X} # working copy element type
  
  # maxFastA: max absolute value for accumulator
  # maxFastE: max absolute value for vector elements (not used if ~wd)
  def maxFastE = if (wd) maxabsval{X} else maxabsval{X}/(bulk*tern{R==f64, 1, 4}) # 4 to give maxFastA some range
  def maxFastA = maxsafeint{R} - maxFastE*bulk
  
  if (R!=f64) { def m = maxFastA + maxFastE*bulk; assert{m<=maxvalue{R}}; assert{-m>=minvalue{R}} }
  
  cv:= [arch_defvw/width{R}]R ** c
  
  if (R==f64 and c != __floor{c}) goto{'end'}
  while (1) {
    if (R==f64) { if (rare{__abs{extract{cv,0}} >= maxFastA}) goto{'end'} }
    else        { if (rare{extract{absu{half{cv,0}},0} > maxFastA}) goto{'end'} }
    
    i2:= i+bulk
    if (i2>len) goto{'end'}
    
    def cx0 = tup{load{*[bulk]X~~(x+i)}}
    def cx = if(wd) widenFull{WE,cx0} else cx0
    if (~wd) { # within-vector overflow check; widening gives range space for this to not happen
      if (rare{any_hom{...each{{c:T} => absu{c} >= (ty_u{T}**maxFastE), cx}}}) goto{'end'}
    }
    
    def s0 = each{scan_plus, cx}
    
    def s1{v0} = tup{v0}
    def s1{v0,v1} = tup{v0,v1+broadcast_last{v0}}
    
    def cr = eachx{+, widenFull{R, s1{...s0}}, cv}
    cv = broadcast_last{select{cr, -1}}
    
    assert{type{cv} == one_type{cr}}
    assert{vcount{type{cv}} * length{cr} == bulk}
    
    each{{c:T, j} => store{*T~~(r+i), j, c}, cr, iota{length{cr}}}
    i = i2
  }
  
  setlabel{'end'}
  c = extract{cv, 0}
}
def plus_scanG{X, R} = plus_scan{X, R, void}
def plus_scanC{X, R} = plus_scan{X, R, u64}

export{'si_scan_plus_i8_i32',  plus_scanC{i8,  i32}}
export{'si_scan_plus_i16_i32', plus_scanC{i16, i32}}
export{'si_scan_plus_i32_i32', plus_scanC{i32, i32}}

export{'si_scan_plus_i16_f64', plus_scanG{i16, f64}}
export{'si_scan_plus_i32_f64', plus_scanG{i32, f64}}



# Row-wise boolean scan
# Create masks of the given type with spacing l>=64
def loose_mask_gen{(u64), l} = {
  q:usz = 0   # distance to next row boundary
  {} => {
    b:= q<64  # whether there's a boundary
    p:= q%64  # its position
    q-= 64 - (l &- b)
    promote{u64, b} << p
  }
}
def loose_mask_gen{V=[vl]T, l} = { # Slow, for ≠` only
  def get = loose_mask_gen{T, l}
  {} => make{V, @collect (vl) get{}}
}
def has_vecshift = hasarch{'AVX2'} or hasarch{'AARCH64'}
def loose_mask_gen{V=[vl](u64), l if has_vecshift} = {
  l64 := promote{u64, l}
  q := -make{V, 64*iota{vl}}  # distance to next row boundary
  def q_mod{} = { q+= V**l64 & -(q>>63) }
  def q_mod{if hasarch{'SSE4.1'}} = { q = blend_top{q,q+V**l64, q} }
  o:u64 = width{V}; while (o>l64) { o-=l64; q_mod{} }
  {} => {
    m:= V**1 << q; if (not hasarch{'AVX2'}) m&= q < V**64
    q-= V**o; q_mod{}
    m
  }
}

def loop_with_unaligned_mask{x, r, nw, l, step} = {
  {m, d} := unaligned_spaced_mask_mod{l}
  c:u64 = 0  # carry (initial value never matters)
  @for (x, r over nw) {
    match (step{x, c, m}) {
      {{nr,nc}} => { r=nr; c=nc }
       {nr}     => { r=nr; c = -(r>>63) }
    }
    m = m>>d | m<<(l-d)
  }
}
def vec_loop_with_unaligned_mask{xp, rp, nw, l, scan_words, apply_carry} = {
  def vl = arch_defvw / 64
  def V = [vl]u64
  {ms, d} := unaligned_spaced_mask_mod{l}
  d4:usz = width{V} % l
  m:= make{V, scan{{a,_} => a>>d | a<<(l-d), tup{ms, ...iota{vl-1}}}}
  c:= V**0
  @for_masked{vl} (x in tup{V, xp}, r in tup{V, rp} over promote{u64,nw}) {
    s := scan_words{x, m}
    # Each result word can be modified based on top bit of previous
    t := -(s>>63)
    pc:= c; c = rotate_right{t}
    b := (if (vl==2) zip{pc, t, 0} else blend_first{pc, c})
    # Carry applies to bits below any mask bit
    r  = apply_carry{s, b, (m-V**1)&~m}
    m  = m>>d4 | m<<(l-d4)
  }
}
def vec_loop_with_loose_mask{xp, rp, nw, l, id, scan_words, propagate, fix_carry, apply_carry} = {
  assert{l >= 64}
  def vl = arch_defvw / 64
  def V = [vl]u64
  def get_m = loose_mask_gen{V, l}
  c := V**id  # carry, 0 or 1
  @for_masked{vl} (x in tup{V, xp}, r in tup{V, rp} over nw) {
    # Get mask; <=1 bit per word
    m:= get_m{}
    # Within-word scan and carry info
    ml:= m - V**1
    {s, k}:= scan_words{x, m, ml}
    # Propagate carries and adjust result
    p:= propagate{k, c}
    t:= shift_first{c, p}
    r = apply_carry{s, -fix_carry{t}, ml}
    c = broadcast_last{p}
  }
}
def vec_loop_with_loose_mask{...a={xp, rp, nw, l, id, scan_words}, apply_carry} = {
  def passthrough{k, c} = {
    def bl{b,a} = b ^ ((b^a) & -(b>>63))
    def bl{b:B,a if hasarch{'AARCH64'}} = blend_bit{b,a, ty_s{b} < 0}
    def bl{b,a if hasarch{'SSE4.1'}} = blend_top{b,a, b}
    bl{make_scan_idem{f64, bl}{k}, c}  # Can't be -1 now
  }
  vec_loop_with_loose_mask{...a, passthrough, {k}=>k, apply_carry}
}

fn scan_rows_andor{id}(src:*u64, dst:*u64, nl:usz, l:usz) : void = {
  def qand = not id
  assert{l > 0}
  nw := cdiv{nl, 64}
  def scan_mask{x:T, m:T} = {
    if (qand) { p:= (x &~ m) >> 1; (x - p) ^ p }
    else      { p:= (x |  m) >> 1; (p - x) ^ p }
  }
  def res_m1{x,c,m} = { # result word with carry c, popc{m}<=1
    if (qand) x &~ ((x+c) &  (x+m))
    else      x | ((-x-c) &~ (x-m))
  }
  def apply_carry{s, c, f} = {
    if (qand) s & (~f | c) else s | (f & c)
  }
  if (l < 64) {
    if (is_pow2{l}) {
      if (l == 2) {
        @for (r in dst, x in src over nw) {
          r = (if (qand) x & (x<<1 | 64w2b01) else x | (x<<1 & 64w2b10))
        }
      } else {
        m:u64 = aligned_spaced_mask{l}
        @for (r in dst, x in src over nw) r = scan_mask{x, m}
      }
      # could use for l>=8; not much faster and takes up space
      # def rowwise{T} = @for (r in *T~~dst, x in *T~~src over (64/width{T})*nw) r = x &~ (x+1)
    } else if (has_simd) {
      vec_loop_with_unaligned_mask{src, dst, nw, l, scan_mask, apply_carry}
    } else {
      loop_with_unaligned_mask{src, dst, nw, l, {x, c, m} => {
        s:= (if (qand) (x &~ m) >> 1 else ~(x | m) >> 1   )
        a:= (if (qand) s + (x&(m|c)) else s + ((m|c) &~ x))
        tup{(if (qand) s ^ a         else ~(s ^ a)        ),
            a >> 63} # new c
      }}
    }
  } else if ((hasarch{'SSE4.1'} or hasarch{'AARCH64'}) and l < (if (hasarch{'AVX2'}) 256 else 160)) {
    if (hasarch{'SSE4.1'}) {
      def scan_words{x:V, m:V, _} = {
        s:= (if (qand) x &~ ((x+V**1) &  (x+m))
             else      x |  ((-x)     &~ (x-m)))
        p:= (if (qand) x&~m == ~V**0
             else      x| m ==  V**0)
        k:= s>>63 | p  # Carry of 0 or 1, but -1 to propagate previous
        tup{s, k}
      }
      vec_loop_with_loose_mask{src, dst, nw, l, id, scan_words, apply_carry}
    } else {
      def get_m = loose_mask_gen{u64, l}
      c:u64 = id  # carry
      @for (r in dst, x in src over nw) {
        r = res_m1{x, c, get_m{}}
        c = r >> 63
      }
    }
  } else {
    i :usz = 0  # row bit index
    wn:usz = 0  # starting word of next row
    c:u64 = id  # carry
    def word{bit} = bit * tail{64}
    we:= nl/64; while (wn < we) {
      iw:= wn
      r := res_m1{load{src, iw}, c, u64~~1 << (i%64)}
      store{dst, iw, r}; ++iw
      i+= l; wn = i/64
      c = r>>63
      if (c != id) while (iw < wn) {
        x:= load{src, iw}
        if (x != word{not id}) {
          c = id
          store{dst, iw, if (qand) x &~ (x+1) else x | -x}; ++iw
          goto{'shortcut'}
        }
        store{dst, iw, x}; ++iw
      }
      setlabel{'shortcut'}
      @for (r in dst over _ from iw to wn) r = word{id}
    }
    if (i%64 != 0) {
      x:= load{src, wn}
      store{dst, wn, if (qand) x &~ (x+c) else x | (-x-c)}
    }
  }
}

fn scan_rows_neq(x:*u64, r:*u64, nl:usz, l:usz) : void = {
  def scan_word  = scan_word_ne
  def scan_words = scan_words_ne
  def apply_carry{s, c, f} = s ^ (f & c)
  assert{l > 0}
  nw := cdiv{nl, 64}
  if (l < 64) {
    def apply_mask{s, m} = {
      b:= s<<1 & m  # last bit of previous row
      s ^ (b<<l - b)
    }
    if (is_pow2{l}) {
      m:u64 = aligned_spaced_mask{l}
      @for (r, x over nw) r = apply_mask{scan_word{x}, m}
    } else if (has_simd) {
      def scan_words{x, m} = apply_mask{scan_words{x}, m}
      vec_loop_with_unaligned_mask{x, r, nw, l, scan_words, apply_carry}
    } else {
      loop_with_unaligned_mask{x, r, nw, l, {x, c, m} => {
        s:= scan_word{x}
        f:= (m-1)&~m  # bits before first full row
        b:= s<<1 & m  # last bit of previous row
        s ^ ((c & f) | (b<<l - b))
      }}
    }
  } else if (has_simd and l < (if (hasarch{'AVX2'}) 320 else 192)) {
    def scan_words{x:V, m:V, ml:V} = {
      s:= scan_words{x}
      s^= -(s<<1 & m)
      k:= s>>63 | (V**(1<<63) &~ ml)  # Top bit 1 to stop, so 0 is identity
      tup{s, k}
    }
    def propagate{k:V=[vl]_, c:V} = {
      def bl{b,a} = b ^ (a &~ -(b>>63))
      def bl{b,a if hasarch{'AVX2'}} = blend_top{a^b,b, b}
                k = bl{k, vec_shift_right_128{k, 1}}
      if (vl>2) k = bl{k, shuf{V, blend{V**0, k, 0,1,0,1}, 0,0,1,1}}
                    bl{k, c}
    }
    def fix_carry{t:V} = t & V**1
    vec_loop_with_loose_mask{x, r, nw, l, 0, scan_words, propagate, fix_carry, apply_carry}
  } else {
    i :usz = 0  # row bit index
    iw:usz = 0  # starting word
    c:u64 = 0   # carry
    while (1) {
      i+= l; ii := iw; iw = cdiv{i, 64}
      scan_neq{}(c, x+ii, r+ii, iw-ii)
      if (i == nl) return{}
      s:= load{r, iw-1}
      q := i%64
      s^= -(s<<1 & u64~~1<<q)
      store{r, iw-1, s}
      c = -((s &- (q>0)) >> 63)
    }
  }
}

fn scan_rows_left(x:*u64, r:*u64, nl:usz, l:usz) : void = {
  assert{l > 0}
  nw := cdiv{nl, 64}
  def apply_carry{s, c, f} = s | (f & c)
  if (l < 64) {
    def apply_mask{x, m} = { b:= x & m; b<<l - b }
    if (is_pow2{l}) {
      m:u64 = aligned_spaced_mask{l}
      @for (r, x over nw) r = apply_mask{x, m}
    } else if (has_simd) {
      vec_loop_with_unaligned_mask{x, r, nw, l, apply_mask, apply_carry}
    } else {
      loop_with_unaligned_mask{x, r, nw, l, {x, c, m} => {
        f:= (m-1)&~m  # bits before first full row
        (c & f) | apply_mask{x, m}
      }}
    }
  } else if (hasarch{'AVX2'} and l < 176) {
    def scan_words{x:V, m:V, _} = {
      s:= -(x & m)
      tup{s, s>>63 | (m == V**0)}
    }
    vec_loop_with_loose_mask{x, r, nw, l, 0, scan_words, apply_carry}
  } else {
    assert{l >= 64}
    k:= l/64 - 1 # at least k full aligned words in a row
    i :usz = 0   # row bit index
    wn:usz = 0   # starting word of next row
    c:u64 = 0    # carry
    we:= nl/64; while (wn < we) {
      iw:= wn
      m := u64~~1 << (i%64)
      xw:= -(load{x, iw} & m)
      r0:= (c & (m-1)) | xw
      c = -(xw>>63)
      i+= l; wn = i/64
      store{r, wn-1, c}
      store{r, iw, r0}
      @for (r in r+iw+1 over k) r = c
    }
    if (i%64 != 0) store{r, wn, c}
  }
}

export{'si_scan_rows_and',   scan_rows_andor{0}}
export{'si_scan_rows_or',    scan_rows_andor{1}}
export{'si_scan_rows_ne',    scan_rows_neq}
export{'si_scan_rows_ltack', scan_rows_left}


# Strided boolean scans
fn scan_stride_bool_assoc{op}(x:*u64, r:*u64, nl:usz, l:usz) : void = {
  assert{l > 1}
  def {flip,opf} = if (same{op, &}) tup{~,|} else tup{{x}=>x,op} # such that identity of opf is 0
  nw:= cdiv{nl, 64}
  if (l <= 64) {
    if (same{op, ^} and hasarch{'PCLMUL'} and is_pow2{l}) {
      clmul_scan_ne_any{}(*void~~x, *void~~r, 0, nw, aligned_spaced_mask{l})
      return{}
    }
    c:u64 = 0  # carry l bits, no matter the alignment
    @for (r, x over nw) {
      c = opf{flip{x}, c >> (64-l)}
      s:= l; while (s < 64) { c = opf{c, c<<s}; s += s }
      r = flip{c}
    }
  } else if (l < 128) {
    q:= l%64
    c:= flip{u64~~0}
    p:= load{x,0}
    store{r,0, p}
    @for (r, x over _ from 1 to nw) {
      r = op{x, c>>(64-q) | p<<q}
      c = p; p = r
    }
  } else {
    fw:= cdiv{l, 64}  # words in first cell
    @for (r, x over fw) r = x
    if (l%64 == 0) {
      @for (r, x, p in r-fw over _ from fw to nw) r = op{x, p}
    } else {
      q:= l%64
      c:= flip{u64~~0}
      @for (r, x, p in r-(fw-1) over _ from fw-1 to nw) {
        r = op{x, c>>(64-q) | p<<q}
        c = p
      }
    }
  }
}
export_tab{
  'si_scan_bool_stride', 
  each{scan_stride_bool_assoc, tup{|, &, ^}}
}
