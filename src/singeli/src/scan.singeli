include './base'
if_inline (hasarch{'X86_64'}) {
  if_inline (hasarch{'PCLMUL'}) include './clmul'
  if_inline (hasarch{'AVX512BW', 'VPCLMULQDQ', 'GFNI'}) include './avx512'
}
include './mask'
include './f64'
include './spaced'
include 'util/tup'
include './scan_common'

# Initialized scan, generic implementation
fn scan_scal{T, op}(x:*T, r:*T, len:u64, m:T) : void = {
  @for (x, r over len) r = m = op{m, x}
}

def scan_loop{init, x:*T, r:*T, len:(u64), scan, scan_last} = {
  def step = arch_defvw/width{T}
  def V = [step]T
  p:= V**init
  xv:= *V ~~ x
  rv:= *V ~~ r
  e:= len/step
  @for (xv, rv over e) rv = scan{xv,p}
  q:= len & (step-1)
  if (q!=0) homMaskStoreF{rv+e, maskOf{V, q}, scan_last{load{xv,e}, p}}
}
def scan_post{init, x:*T, r:*T, len:(u64), op, pre} = {
  def last{v, p} = op{pre{v}, p}
  def scan{v, p} = {
    n:= last{v, p}
    p = toLast{n}
    n
  }
  scan_loop{init, x, r, len, scan, last}
}

# Associative scan ?` if a?b?a = a?b = b?a, used for ⌊⌈
def scan_idem = scan_scal
fn scan_idem{T, op if hasarch{'X86_64'}}(x:*T, r:*T, len:u64, init:T) : void = {
  scan_post{init, x, r, len, op, make_scan_idem{T, op}}
}

export{'si_scan_min_init_i8',  scan_idem{i8 , min}}; export{'si_scan_max_init_i8',  scan_idem{i8 , max}}
export{'si_scan_min_init_i16', scan_idem{i16, min}}; export{'si_scan_max_init_i16', scan_idem{i16, max}}
export{'si_scan_min_init_i32', scan_idem{i32, min}}; export{'si_scan_max_init_i32', scan_idem{i32, max}}
export{'si_scan_min_init_f64', scan_idem{f64, min}}; export{'si_scan_max_init_f64', scan_idem{f64, max}}

fn scan_idem_id{T, op}(x:*T, r:*T, len:u64) : void = {
  scan_idem{T, op}(x, r, len, get_id{op, T})
}
export{'si_scan_min_i8',  scan_idem_id{i8 , min}}; export{'si_scan_max_i8',  scan_idem_id{i8 , max}}
export{'si_scan_min_i16', scan_idem_id{i16, min}}; export{'si_scan_max_i16', scan_idem_id{i16, max}}
export{'si_scan_min_i32', scan_idem_id{i32, min}}; export{'si_scan_max_i32', scan_idem_id{i32, max}}

# Assumes identity is 0
def scan_assoc{op} = {
  def shl0{v, k} = shl{[16]u8, v, k/8} # Lanewise
  def shl0{v:V, k==128 if hasarch{'AVX2'}} = {
    # Broadcast end of lane 0 to entire lane 1
    l:= V~~make{[8]i32,0,0,0,-1,0,0,0,0} & spread{v}
    sel{[8]i32, l, make{[8]i32, 3*(3<iota{8})}}
  }
  prefix_byshift{op, shl0}
}
def scan_plus = scan_assoc{+}

# Associative scan
def scan_assoc_0 = scan_scal
fn scan_assoc_0{T, op if hasarch{'X86_64'}}(x:*T, r:*T, len:u64, init:T) : void = {
  # Prefix op on entire AVX register
  scan_post{init, x, r, len, op, scan_plus}
}
export{'si_scan_pluswrap_u8',  scan_assoc_0{u8 , +}}
export{'si_scan_pluswrap_u16', scan_assoc_0{u16, +}}
export{'si_scan_pluswrap_u32', scan_assoc_0{u32, +}}

# xor scan
fn scan_neq{}(p:u64, x:*u64, r:*u64, nw:u64) : void = {
  @for (x, r over nw) {
    r = p ^ prefix_byshift{^, <<}{x}
    p = -(r>>63) # repeat sign bit
  }
}
fn clmul_scan_ne_any{if hasarch{'PCLMUL'}}(x:*void, r:*void, init:u64, words:u64, mark:u64) : void = {
  def V = [2]u64
  m := V**mark
  def xor64{a, i, carry} = { # carry is 64-bit broadcasted current total
    p := clmul{a, m, i}
    t := shr{[16]u8, p, 8}
    s := p ^ carry
    carry = s ^ t
    s
  }
  xv := *V ~~ x
  rv := *V ~~ r
  e := words/2
  c := V**init
  @for (rv, xv over e) {
    rv = apply{zipLo, (@collect (j to 2) xor64{xv, j, c})}
  }
  if ((words & 1) == 1) {
    storeLow{*u64~~(rv+e), 64, clmul{loadLow{xv+e, 64}, m, 0} ^ c}
  }
}
fn scan_neq{if hasarch{'PCLMUL'}}(init:u64, x:*u64, r:*u64, nw:u64) : void = {
  clmul_scan_ne_any{}(*void~~x, *void~~r, init, nw, -(u64~~1))
}
fn scan_neq{if hasarch{'AVX512BW', 'VPCLMULQDQ', 'GFNI'}}(init:u64, x:*u64, r:*u64, nw:u64) : void = {
  def V = [8]u64
  def sse{a} = make{[2]u64, a, 0}
  carry := sse{init}
  # xor-scan on bytes
  xmat := V**base{256, 1<<(8-iota{8}) - 1}
  def xor8 = emit{V, '_mm512_gf2p8affine_epi64_epi8', ., xmat, 0}
  # Exclusive xor-scan on one word
  def exor64 = clmul{., sse{1<<64 - 2}, 0}
  @for (xv in *V~~x, rv in *V~~r over i to cdiv{nw,vcount{V}}) {
    x8 := xor8{xv}
    hb := sse{topMask{[64]u8~~x8}}
    xh := exor64{hb}  # Exclusive xor of high bits (xh ^ hb for inclusive)
    xc := xh ^ carry
    v  := x8 ^ V~~maskToHom{[64]u8, [64]u1~~extract{xc,0}}
    carry = (xc ^ hb) ^ shuf{[4]u32, xh, 4b3232}
    rem:= nw - 8*i
    if (rem < 8) {
      maskStore{*V~~r+i, [8]u1~~(~(u8~~0xff<<rem)), v}
      return{}
    }
    rv = v
  }
}
export{'si_scan_ne', scan_neq{}}

# Boolean cumulative sum
fn bcs{T}(x:*u64, r:*T, l:u64) : void = {
  def bitp_get{arr, n} = (load{arr, n>>6} >> (n&63)) & 1
  c:T = 0
  @for (r over i to l) { c+= cast_i{T, bitp_get{x,i}}; r = c }
}
fn bcs{T if hasarch{'AVX2'}}(x:*u64, r:*T, l:u64) : void = {
  def U = ty_u{T}
  def w = width{T}
  def vl= 256 / w
  def V = [vl]U
  rv:= *V~~r
  xv:= *u32~~x
  c:= V**0
  
  def ii32 = iota{32}; def bit{k}=bit{k,ii32}; def tail{k}=tail{k,ii32}
  def sums{n} = (if (n==0) tup{0} else { def s=sums{n-1}; merge{s,s+1} })
  def widen{v:T} = unpackQ{shuf{[4]u64, v, 4b3120}, T**0}
  
  def sumlanes{x:(u32)} = {
    b:= [8]u32**x >> make{[8]u32, 4*tail{1, iota{8}}}
    s:= sel8{[32]u8~~b, ii32>>3 + bit{2}}
    p:= s & make{[32]u8, (1<<(1+tail{2})) - 1}  # Prefixes
    d:= sel{[16]u8, make{[32]u8, merge{sums{4},sums{4}}}, [32]i8~~p}
    d+= sel8{d, bit{2}*(1+bit{3}>>2)-1}
    d + sel8{d, bit{3}-1}
  }
  def step{x:(u32), i, store1} = {
    d:= sumlanes{x}
    if (w==8) d+= [32]u8~~shuf{[4]u64, [8]i32~~sel8{d, bit{3}<<4-1}, 4b1100}
    j:= (w/8)*i
    def out{v, k} = each{out, widen{v}, 2*k+iota{2}}
    def out{v0:([vl]T), k} = {
      v := V~~v0 + c
      # Update carry at the lane boundary
      if (w!=32 or tail{1,k}) {
        c = sel{[8]u32, spread{v}, make{[8]i32, 8**7}}
      }
      store1{rv, j+k, v}
    }
    out{[32]i8~~d, 0}
  }

  e:= l/32
  @for (xv over i to e) {
    step{xv, i, store}
  }

  if (e*32 != l) {
    def st{p, j, v} = {
      jv := vl*j
      if (jv+vl <= l) {
        store{p, j, v}
      } else {
        if (jv < l) homMaskStoreF{rv+j, maskOf{V, l - jv}, v}
        return{}
      }
    }
    step{load{xv, e}, e, st}
  }
}
export{'si_bcs8',  bcs{i8}}
export{'si_bcs16', bcs{i16}}
export{'si_bcs32', bcs{i32}}



def addChk{a:T, b:T} = {
  mem:*T = tup{a}
  def bad = emit{u1, '__builtin_add_overflow', a, b, mem}
  tup{bad, load{mem}}
}
def addChk{a:T, b:T==f64} = tup{0, a+b}

def widenFull{E, xs} = {
  merge{...each{{x:X=[k]T} => {
    def tb = width{E} * k
    if (tb<=arch_defvw) tup{widen{[k]E, x}}
    else if (1) {
      assert{tb == 2*arch_defvw}
      tup{
        widen{[k/2]E, half{x,0}},
        widen{[k/2]E, half{x,1}}
      }
    }
  }, xs}}
}

def floor{x if knum{x}} = x - (x%1)
def maxabsval{T if issigned{T}} = -minvalue{T}
def maxsafeint{T if issigned{T}} = maxvalue{T}
def maxsafeint{T==f64} = 1<<53

fn plus_scan{X, R, O}(x:*X, c:R, r:*R, len:u64) : O = {
  i:u64 = 0
  if (hasarch{'AVX2'}) simd_plus_scan_part{x, c, r, len, i}
  @forUnroll{1,1} (js from i to len) {
    def vs = eachx{load, x, js}
    each{{j, v} => {
      def {b,n} = addChk{c, promote{R, v}}
      if (rare{b}) return{j}
      store{r, j, n}
      c = n
    }, js, vs}
  }
  len
}
# Sum as many vector registers as possible; modifies c and i
def simd_plus_scan_part{x:*X, c:R, r:*R, len:(u64), i:(u64)} = {
  def b = max{width{R}/2, width{X}}
  def bulk = arch_defvw/b
  
  def wd = (X!=R) & (width{X}<32) # whether to widen the working copy one size
  def WE = tern{wd, w_d{X}, X} # working copy element type
  
  # maxFastA: max absolute value for accumulator
  # maxFastE: max absolute value for vector elements (not used if ~wd)
  def maxFastE = if (wd) maxabsval{X} else maxabsval{X}/(bulk*tern{R==f64, 1, 4}) # 4 to give maxFastA some range
  def maxFastA = maxsafeint{R} - maxFastE*bulk
  
  if (R!=f64) { def m = maxFastA + maxFastE*bulk; assert{m<=maxvalue{R}}; assert{-m>=minvalue{R}} }
  
  cv:= [arch_defvw/width{R}]R ** c
  
  if (R==f64 and c != floor{c}) goto{'end'}
  while (1) {
    if (R==f64) { if (rare{abs{extract{cv,0}} >= maxFastA}) goto{'end'} }
    else        { if (rare{extract{absu{half{cv,0}},0} > maxFastA}) goto{'end'} }
    
    i2:= i+bulk
    if (i2>len) goto{'end'}
    
    def cx0 = tup{load{*[bulk]X~~(x+i)}}
    def cx = if(wd) widenFull{WE,cx0} else cx0
    if (~wd) { # within-vector overflow check; widening gives range space for this to not happen
      if (rare{homAny{tree_fold{|, each{{c:T} => absu{c} >= (ty_u{T}**maxFastE), cx}}}}) goto{'end'}
    }
    
    def s0 = each{scan_plus, cx}
    
    def s1{v0} = tup{v0}
    def s1{v0,v1} = tup{v0,v1+toLast{v0}}
    
    def cr = eachx{+, widenFull{R, s1{...s0}}, cv}
    cv = toLast{select{cr, -1}}
    
    assert{type{cv} == oneType{cr}}
    assert{vcount{type{cv}} * length{cr} == bulk}
    
    each{{c:T, j} => store{*T~~(r+i), j, c}, cr, iota{length{cr}}}
    i = i2
  }
  
  setlabel{'end'}
  c = extract{cv, 0}
}
def plus_scanG{X, R} = plus_scan{X, R, void}
def plus_scanC{X, R} = plus_scan{X, R, u64}

export{'si_scan_plus_i8_i32',  plus_scanC{i8,  i32}}
export{'si_scan_plus_i16_i32', plus_scanC{i16, i32}}
export{'si_scan_plus_i32_i32', plus_scanC{i32, i32}}

export{'si_scan_plus_i16_f64', plus_scanG{i16, f64}}
export{'si_scan_plus_i32_f64', plus_scanG{i32, f64}}



# Row-wise boolean scan
def loop_with_unaligned_mask{x, r, nw, l, step} = {
  {m, d} := unaligned_spaced_mask_mod{l}
  c:u64 = 0  # carry (initial value never matters)
  @for (x, r over nw) {
    match (step{x, c, m}) {
      {{nr,nc}} => { r=nr; c=nc }
       {nr}     => { r=nr; c = -(r>>63) }
    }
    m = m>>d | m<<(l-d)
  }
}
def avx2_loop_with_unaligned_mask{xp, rp, nw, l, scan_words, apply_carry} = {
  {ms, d} := unaligned_spaced_mask_mod{l}
  def V = [4]u64
  d4:usz = width{V} % l
  m:= make{V, scan{{a,_} => a>>d | a<<(l-d), tup{ms, ...iota{3}}}}
  c:= V**0
  @maskedLoop{4} (x in tup{V, xp},
                  r in tup{V, rp} over promote{u64,nw}) {
    s := scan_words{x, m}
    pc:= c; c = shuf{V, -(s>>63), 4b2103}
    r  = apply_carry{s, blend{V, c, pc, 2b0001}, (m-V**1)&~m}
    m  = m>>d4 | m<<(l-d4)
  }
}

fn scan_rows_andor{id}(src:*u64, dst:*u64, nl:usz, l:usz) : void = {
  def qand = not id
  assert{l > 0}
  nw := cdiv{nl, 64}
  def res_m1{x,c,m} = { # result word with carry c, popc{m}<=1
    if (qand) x &~ ((x+c) &  (x+m))
    else      x | ((-x-c) &~ (x-m))
  }
  if (l < 64) {
    if ((l & (l-1)) == 0) {
      if (l == 2) {
        @for (r in dst, x in src over nw) {
          r = (if (qand) x & (x<<1 | 64w2b01) else x | (x<<1 & 64w2b10))
        }
      } else {
        m:u64 = aligned_spaced_mask{l}
        t := m << (l-1)
        @for (r in dst, x in src over nw) {
          r = (if (qand) x &~ ((t&x) ^ ((x&~t) + m))
               else      x | ~((t&~x) ^ ((x|t) - m)))
        }
      }
      # could use for l>=8; not much faster and takes up space
      # def rowwise{T} = @for (r in *T~~dst, x in *T~~src over (64/width{T})*nw) r = x &~ (x+1)
    } else if (hasarch{'AVX2'}) {
      def scan_words{x, m:V} = {
        mb:= m | V**1
        p:= if (qand) (x &~ m) >> 1 else ~(x | m) >> 1
        a:= if (qand) p + (mb & x)  else p + (mb &~ x)
            if (qand) p ^ a         else ~(p ^ a)
      }
      def apply_carry{s, c, f} = {
            if (qand) s & (~f | c) else s | (f & c)
      }
      avx2_loop_with_unaligned_mask{src, dst, nw, l, scan_words, apply_carry}
    } else {
      loop_with_unaligned_mask{src, dst, nw, l, {x, c, m} => {
        s:= (if (qand) (x &~ m) >> 1 else ~(x | m) >> 1   )
        a:= (if (qand) s + (x&(m|c)) else s + ((m|c) &~ x))
        tup{(if (qand) s ^ a         else ~(s ^ a)        ),
            a >> 63} # new c
      }}
    }
  } else if (l < 160) {
    q:usz = 0   # distance to next row boundary
    c:u64 = id  # carry
    @for (r in dst, x in src over nw) {
      b:= q<64  # whether there's a boundary
      p:= q%64  # its position
      q-= 64 - (l &- b)
      r = res_m1{x, c, promote{u64, b} << p}
      c = r >> 63
    }
  } else {
    i :usz = 0  # row bit index
    wn:usz = 0  # starting word of next row
    c:u64 = id  # carry
    def word{bit} = bit * ((1<<64) - 1)
    we:= nl/64; while (wn < we) {
      iw:= wn
      r := res_m1{load{src, iw}, c, u64~~1 << (i%64)}
      store{dst, iw, r}; ++iw
      i+= l; wn = i/64
      c = r>>63
      if (c != id) while (iw < wn) {
        x:= load{src, iw}
        if (x != word{not id}) {
          c = id
          store{dst, iw, if (qand) x &~ (x+1) else x | -x}; ++iw
          goto{'shortcut'}
        }
        store{dst, iw, x}; ++iw
      }
      setlabel{'shortcut'}
      @for (r in dst over _ from iw to wn) r = word{id}
    }
    if (i%64 != 0) {
      x:= load{src, wn}
      store{dst, wn, if (qand) x &~ (x+c) else x | (-x-c)}
    }
  }
}

fn scan_rows_neq(x:*u64, r:*u64, nl:usz, l:usz) : void = {
  def scan_word = prefix_byshift{^, <<}
  assert{l > 0}
  nw := cdiv{nl, 64}
  if (l < 64) {
    if ((l & (l-1)) == 0) {
      m:u64 = aligned_spaced_mask{l}
      @for (r, x over nw) {
        s:= scan_word{x}
        b:= s<<1 & m  # last bit of previous row
        r = s ^ (b<<l - b)
      }
    } else if (hasarch{'AVX2'}) {
      def scan_words{x, m} = {
        def vec_prefix_byshift{op, sh} = {
          def pre{v:V, k} = if (k < elwidth{V}) pre{op{v, sh{v,k}}, 2*k} else v
          {v:T} => pre{v, 1}
        }
        s:= vec_prefix_byshift{^, <<}{x}
        b:= s<<1 & m  # last bit of previous row
        s ^ (b<<l - b)
      }
      def apply_carry{s, c, f} = s ^ (f & c)
      avx2_loop_with_unaligned_mask{x, r, nw, l, scan_words, apply_carry}
    } else {
      loop_with_unaligned_mask{x, r, nw, l, {x, c, m} => {
        s:= scan_word{x}
        f:= (m-1)&~m  # bits before first full row
        b:= s<<1 & m  # last bit of previous row
        s ^ ((c & f) | (b<<l - b))
      }}
    }
  } else {
    i :usz = 0  # row bit index
    iw:usz = 0  # starting word
    c:u64 = 0   # carry
    while (1) {
      i+= l; ii := iw; iw = cdiv{i, 64}
      scan_neq{}(c, x+ii, r+ii, promote{u64,iw-ii})
      if (i == nl) return{}
      s:= load{r, iw-1}
      q := i%64
      s^= -(s<<1 & u64~~1<<q)
      store{r, iw-1, s}
      c = -((s &- (q>0)) >> 63)
    }
  }
}

fn scan_rows_left(x:*u64, r:*u64, nl:usz, l:usz) : void = {
  def scan_word = prefix_byshift{^, <<}
  assert{l > 0}
  nw := cdiv{nl, 64}
  if (l < 64) {
    if ((l & (l-1)) == 0) {
      m:u64 = aligned_spaced_mask{l}
      @for (r, x over nw) { b:= x & m; r = b<<l - b }
    } else if (hasarch{'AVX2'}) {
      def scan_words{x, m} = { b:= x&m; b<<l - b }
      def apply_carry{s, c, f} = s | (f & c)
      avx2_loop_with_unaligned_mask{x, r, nw, l, scan_words, apply_carry}
    } else {
      loop_with_unaligned_mask{x, r, nw, l, {x, c, m} => {
        f:= (m-1)&~m  # bits before first full row
        b:= x & m
        (c & f) | (b<<l - b)
      }}
    }
  } else {
    i :usz = 0  # row bit index
    wn:usz = 0  # starting word of next row
    c:u64 = 0   # carry
    we:= nl/64; while (wn < we) {
      iw:= wn
      m := u64~~1 << (i%64)
      xw:= -(load{x, iw} & m)
      store{r, iw, (c & (m-1)) | xw}
      c = -(xw>>63)
      i+= l; wn = i/64
      @for (r in r over _ from iw+1 to wn) r = c
    }
    if (i%64 != 0) store{r, wn, c}
  }
}

export{'si_scan_rows_and',   scan_rows_andor{0}}
export{'si_scan_rows_or',    scan_rows_andor{1}}
export{'si_scan_rows_ne',    scan_rows_neq}
export{'si_scan_rows_ltack', scan_rows_left}
