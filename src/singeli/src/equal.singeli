include './base'
include './cbqnDefs'
include './f64'
include './mask'
include './bitops'

def swap{w,x} = {
  t:= w
  w = x
  x = t
}

# width{W} ≤ width{X}
fn equal{W, X}(w:*void, x:*void, l:u64, d:u64) : u1 = {
  def vw = arch_defvw{}
  def bulk = vw / width{X}
  if (W!=X) if (d!=0) swap{w,x}
  assert{l>0}
  
  if (W==u1) {
    if (X==u1) { # bitarr ≡ bitarr
      def BT = [vw/8]u8
      @for_masked{vw}(w in *BT~~w, x in *BT~~x, M in 'm' over l) if (anyne_bit{w,x,M}) return{0}
    } else if (X==f64) { # bitarr ≡ f64arr
      def TF = [vw/64]f64
      def TU = [vw/64]u64
      f0:= TF**0.0
      f1:= TF**1.0
      @for_masked_pos{bulk}(M in 'm' over i to l) {
        wu:= (if (hasarch{'AVX2'}) {
          cw:= load_bits_lo{bulk, *u64~~w, i}
          blend_top{f0, f1, TU**cw << make{TU,63-iota{vcount{TU}}}}
        } else {
          cw:= load_expand_bits{TU, *u64~~w, i}
          blend_hom{f0, f1, cw}
        })
        cx:= load{*TF ~~ x, i}
        if (anyne_positive{wu, cx, M}) return{0}
      }
    } else { # bitarr ≡ i8/i16/i32arr
      def T = [bulk]X
      def sh{c} = c << (width{X}-1)
      def sh{c if X==u8} = T ~~ (re_el{u16,c}<<7)
      def mask{x:X if hasarch{'X86_64'}} = top_to_int{x}
      def mask{x:X if hasarch{'AARCH64'}} = hom_to_int{andnz{x, ~T**0}}
      
      # TODO compare with doing the comparison in vector registers
      badBits:= T ** ~(X~~1)
      @for_masked{bulk}(cw in tup{'b',w}, x in *T~~x, M in 'm' over i to l) {
        if (~and_bit_none{M{x}, badBits}) return{0}
        if (anyne{promote{u64,mask{sh{x}}}, promote{u64,cw}, M}) return{0}
      }
      1
    }
  } else { # everything not involving bitarrs (i.e. floats/ints, and chars)
    if (W==i8 and X==i8) l<<= d
    
    def R = [bulk]X
    
    @for_masked_pos{bulk}(M in 'm' over i to l) {
      cw:= load_widen{*W~~w, i, R}
      cx:= load_widen{*X~~x, i, R}
      if (anyne_positive{cw,cx,M}) return{0}
    }
  }
  1
}

def eq_reflexive{a:V=[k](f64), b:V} = (a==b) | ((a!=a) & (b!=b))

def any_ne_reflexive_qnan{M, a:V=[k](f64), b:V} = { # (a==b) | (isQNaN{a} & isQNaN{b}); assumes at least one arg isn't sNaN
  def U = ty_u{V}
  def t1 = U~~a & U~~b
  
  def ne = a != b
  def t2 = ne & U**0x7FF8_0000_0000_0000
  
  def andn_bit_none{M, x:T, y:T} = ~any_bit{M{x&~y}}
  def andn_bit_none{M, x:T, y:T if M{0}==0 and hasarch{'X86_64'}} = andn_bit_none{x,y}
  ~andn_bit_none{M, t2, t1}
}

fn equal_reflexive{}(w:*void, x:*void, l:u64, d:u64) : u1 = {
  def w = *f64~~w
  def x = *f64~~x
  def bulk = arch_defvw{} / width{f64}
  def unr = if (hasarch{'AARCH64'}) 2 else 1
  def V = [bulk]f64
  @for_mu{bulk, unr}(w in tup{V,w}, x in tup{V,x}, M in 'm' over i to l) {
    if (hasarch{'SSE4.1'}) {
      each{{w,x} => {
        if (any_ne_reflexive_qnan{M,w,x}) return{0}
      }, w, x}
    } else {
      if (any_hom{M, ...each{{w,x} => ~eq_reflexive{w,x}, w, x}}) return{0}
    }
  }
  1
}

export{'simd_equal_1_1',     equal{u1, u1}}
export{'simd_equal_1_8',     equal{u1, u8}}
export{'simd_equal_1_16',    equal{u1, u16}}
export{'simd_equal_1_32',    equal{u1, u32}}
export{'simd_equal_1_f64',   equal{u1, f64}}

export{'simd_equal_8_8',     equal{i8, i8}}

export{'simd_equal_s8_16',   equal{i8, i16}}
export{'simd_equal_s8_32',   equal{i8, i32}}
export{'simd_equal_s16_32',  equal{i16, i32}}

export{'simd_equal_s8_f64',  equal{i8,  f64}}
export{'simd_equal_s16_f64', equal{i16, f64}}
export{'simd_equal_s32_f64', equal{i32, f64}}
export{'simd_equal_f64_f64', equal{f64, f64}}
export{'simd_equal_f64_f64_reflexive', equal_reflexive{}}

export{'simd_equal_u8_16',   equal{u8, u16}}
export{'simd_equal_u8_32',   equal{u8, u32}}
export{'simd_equal_u16_32',  equal{u16, u32}}
