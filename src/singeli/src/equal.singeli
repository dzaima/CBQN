include './base'
include './cbqnDefs'
include './f64'
if (hasarch{'X86_64'}) {
  include './sse3'
  include './avx'
  include './avx2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './mask'
include './bitops'

def swap{w,x} = {
  t:= w
  w = x
  x = t
}

# width{W} ≤ width{X}
equal{W, X}(w:*void, x:*void, l:u64, d:u64) : u1 = {
  def vw = arch_defvw
  def bulk = vw / width{X}
  if (W!=X) if (d!=0) swap{w,x}
  
  if (W==u1) {
    if (X==u1) { # bitarr ≡ bitarr
      maskedLoop{vw, l, {i, M} => {
        cw:= load{*[vw/8]u8 ~~ w, i}
        cx:= load{*[vw/8]u8 ~~ x, i}
        if (anyneBit{cw,cx,M}) return{0}
      }}
    } else if (X==f64) { # bitarr ≡ f64arr
      def TF = [vw/64]f64
      def TU = [vw/64]u64
      f0:= TF**0.0
      f1:= TF**1.0
      maskedLoopPositive{bulk, l, {i, M} => {
        wu:= (if (hasarch{'X86_64'}) {
          cw:= b_getBatchLo{bulk, *u64~~w, i}
          topBlend{f0, f1, TU**cw << make{TU,63,62,61,60}}
        } else {
          cw:= loadBatchBit{TU, *u64~~w, i}
          homBlend{f0, f1, cw}
        })
        cx:= load{*TF ~~ x, i}
        if (anynePositive{wu, cx, M}) return{0}
      }}
    } else { # bitarr ≡ i8/i16/i32arr
      def T = [bulk]X
      def sh{c} = c << (width{X}-1)
      def sh{c & X==u8} = T ~~ (to_el{u16,c}<<7)
      def mask{x:X & hasarch{'X86_64'}} = topMask{x}
      def mask{x:X & hasarch{'AARCH64'}} = homMask{andnz{x, ~T**0}}
      
      # TODO compare with doing the comparison in vector registers
      badBits:= T ** ~(X~~1)
      maskedLoop{bulk, l, {i, M} => {
        cw:= b_getBatch{bulk, *u64~~w, i}
        cx:= load{*T ~~ x, i}
        if (~andAllZero{M{cx}, badBits}) return{0}
        if (anyne{promote{u64,mask{sh{cx}}}, promote{u64,cw}, M}) return{0}
      }}
      1
    }
  } else { # everything not involving bitarrs (i.e. floats/ints, and chars)
    if (W==i8 and X==i8) l<<= d
    
    def R = [bulk]X
    
    maskedLoopPositive{bulk, l, {i, M} => {
      cw:= loadBatch{*W~~w, i, R}
      cx:= loadBatch{*X~~x, i, R}
      if (anynePositive{cw,cx,M}) return{0}
    }}
  }
  1
}

'avx2_equal_1_1'    = equal{u1, u1}
'avx2_equal_1_8'    = equal{u1, u8}
'avx2_equal_1_16'   = equal{u1, u16}
'avx2_equal_1_32'   = equal{u1, u32}
'avx2_equal_1_f64'  = equal{u1, f64}

'avx2_equal_8_8'    = equal{i8, i8}

'avx2_equal_s8_16'  = equal{i8, i16}
'avx2_equal_s8_32'  = equal{i8, i32}
'avx2_equal_s16_32' = equal{i16, i32}

'avx2_equal_s8_f64' = equal{i8,  f64}
'avx2_equal_s16_f64'= equal{i16, f64}
'avx2_equal_s32_f64'= equal{i32, f64}
'avx2_equal_f64_f64'= equal{f64, f64}

'avx2_equal_u8_16'  = equal{u8, u16}
'avx2_equal_u8_32'  = equal{u8, u32}
'avx2_equal_u16_32' = equal{u16, u32}
