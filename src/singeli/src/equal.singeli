include './base'
include './cbqnDefs'
include './f64'
include './mask'
include './bitops'

def swap{w,x} = {
  t:= w
  w = x
  x = t
}

# width{W} ≤ width{X}
fn equal{W, X}(w:*void, x:*void, l:u64, d:u64) : u1 = {
  def vw = arch_defvw
  def bulk = vw / width{X}
  if (W!=X) if (d!=0) swap{w,x}
  assert{l>0}
  
  if (W==u1) {
    if (X==u1) { # bitarr ≡ bitarr
      def BT = [vw/8]u8
      @maskedLoop{vw}(w in *BT~~w, x in *BT~~x, M in 'm' over l) if (anyneBit{w,x,M}) return{0}
    } else if (X==f64) { # bitarr ≡ f64arr
      def TF = [vw/64]f64
      def TU = [vw/64]u64
      f0:= TF**0.0
      f1:= TF**1.0
      @maskedLoopPositive{bulk}(M in 'm' over i to l) {
        wu:= (if (hasarch{'AVX2'}) {
          cw:= b_getBatchLo{bulk, *u64~~w, i}
          blend_top{f0, f1, TU**cw << make{TU,63-iota{vcount{TU}}}}
        } else {
          cw:= loadBatchBit{TU, *u64~~w, i}
          blend_hom{f0, f1, cw}
        })
        cx:= load{*TF ~~ x, i}
        if (anynePositive{wu, cx, M}) return{0}
      }
    } else { # bitarr ≡ i8/i16/i32arr
      def T = [bulk]X
      def sh{c} = c << (width{X}-1)
      def sh{c if X==u8} = T ~~ (re_el{u16,c}<<7)
      def mask{x:X if hasarch{'X86_64'}} = top_to_int{x}
      def mask{x:X if hasarch{'AARCH64'}} = hom_to_int{andnz{x, ~T**0}}
      
      # TODO compare with doing the comparison in vector registers
      badBits:= T ** ~(X~~1)
      @maskedLoop{bulk}(cw in tup{'b',w}, x in *T~~x, M in 'm' over i to l) {
        if (~andAllZero{M{x}, badBits}) return{0}
        if (anyne{promote{u64,mask{sh{x}}}, promote{u64,cw}, M}) return{0}
      }
      1
    }
  } else { # everything not involving bitarrs (i.e. floats/ints, and chars)
    if (W==i8 and X==i8) l<<= d
    
    def R = [bulk]X
    
    @maskedLoopPositive{bulk}(M in 'm' over i to l) {
      cw:= loadBatch{*W~~w, i, R}
      cx:= loadBatch{*X~~x, i, R}
      if (anynePositive{cw,cx,M}) return{0}
    }
  }
  1
}

export{'simd_equal_1_1',     equal{u1, u1}}
export{'simd_equal_1_8',     equal{u1, u8}}
export{'simd_equal_1_16',    equal{u1, u16}}
export{'simd_equal_1_32',    equal{u1, u32}}
export{'simd_equal_1_f64',   equal{u1, f64}}

export{'simd_equal_8_8',     equal{i8, i8}}

export{'simd_equal_s8_16',   equal{i8, i16}}
export{'simd_equal_s8_32',   equal{i8, i32}}
export{'simd_equal_s16_32',  equal{i16, i32}}

export{'simd_equal_s8_f64',  equal{i8,  f64}}
export{'simd_equal_s16_f64', equal{i16, f64}}
export{'simd_equal_s32_f64', equal{i32, f64}}
export{'simd_equal_f64_f64', equal{f64, f64}}

export{'simd_equal_u8_16',   equal{u8, u16}}
export{'simd_equal_u8_32',   equal{u8, u32}}
export{'simd_equal_u16_32',  equal{u16, u32}}
