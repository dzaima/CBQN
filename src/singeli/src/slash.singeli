include './base'
if_inline (hasarch{'PCLMUL'}) include './clmul'
include './mask'

def popc_rand{x:T if isint{T} and width{T}==64} = emit{u8, 'rand_popc64', x} # under valgrind, return a random result in the range of possible ones
def popc_rand{x:T if isint{T} and width{T}<=32} = emit{u8, 'rand_popc64', x}

# Table from l bits to w-bit indices, shifted left by s, and G applied afterwards
def maketab{l,w,s,G} = {
  def bot = fold{
    {t,k} => join{each{tup, t, G{k} + t<<w}},
    tup{0},
    reverse{iota{l}<<s}
  }
  # Store popcnt-1 in the high element
  def top = (fold{flat_table{+, ...}, l**iota{2}} - 1)%(1<<(w-s))
  top<<(l*w-w+s) | bot  # Overlaps for all-1 value only
}
def maketab{l,w,s} = maketab{l,w,s,{x}=>x}
def maketab{l,w} = maketab{l,w,0}

itab:*u64 = maketab{8,8} # 256 elts, 2KB; shared by many methods

# Recover popcount, for when POPCNT isn't there
def has_popc = hasarch{'POPCNT'}
def tab_popc{i:I, w} = tail{w, i>>(width{I}-w) + 1}
def popc_alt{v, i, w} = if (has_popc) popc{v} else tab_popc{i, w}

# slash{c, T} defines:
#   if c==1: w/x
#   if c==0 & (T==i8 or T==i16): /w
#   if c==0 & T==i32: x + /w (assumes x is a multiple of 8 for topper)
# if sum(w) < len/thresh{c,T}, sparse Where will be used
def arg{c,T} = if (c) *T else if (T==i32) T else tup{} # type of x

# Modifies the input variable r
# Assumes iter{} will increment r, by at most write_len
def for_special_buffered{r:*T, write_len}{vars,begin,sum,iter} = {
  assert{kreg{r}}; assert{begin==0}
  def tw = width{T}
  def ov = write_len-1
  def bufn = 2*(ov+1)
  buf := undefined{T, bufn}
  r0 := r
  end := r + sum - ov
  i:u64 = 0; buf_used:u1 = 0
  def restart = setlabel{}
  while (r < end) {
    iter{i, vars}
    ++i
  }
  if (not buf_used) {
    end += buf - r + ov
    if (buf < end) {
      r0 = r
      r = buf
      buf_used = 1; goto{restart}
    }
  } else {
    if (has_simd) {
      def bufw = bufn * tw
      def vc = tern{hasarch{'X86_64'} and bufw==128, 128, arch_defvw} / tw;
      def R = [vc]T
      @unroll ((ov/vc)>>0) if (end-buf>vc) { store{r0, load{R, buf}}; r0+=vc; buf+=vc }
      assert{bufw % width{R} == 0} # to make sure the below doesn't read out-of-bounds on the stack
      store_blended_hom{r0, mask_of_first{R, end-buf}, load{R, buf}}
    } else {
      @for (r0, buf over u64~~(end-buf)) r0 = buf
    }
  }
}

# Assumes w is trimmed, so the last 1 appears at index l-1
# Unused because an index buffer and select is faster
def thresh{c, T} = 1
fn slash{c, T}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def bitp_get{arr, n} = (load{arr,n>>6} >> (n&63)) & 1
  @for (i to l) {
    store{r, 0, if (c) load{x,i} else if (T==i32) cast_i{T,i}+x else i}
    r+= bitp_get{w,i}
  }
}

def getter{c, V=[k]T, x} = {
  if (c) {
    i:u64 = 0
    {} => { v:=load{*V~~x, i}; ++i; v }
  } else {
    i := iota{V}
    if (kreg{x}) i += V**cast_i{T,x}
    ii := V**k
    {} => { v:=i; i+=ii; v }
  }
}

# Top bits to convert 1-byte indices to 2 or 4
# These can only change between loop iterations, provided the
# given x for i32 is a multiple of the loop step
def topper{T, U, k, x} = {
  def make_top{S} = re_el{S,U}**(if (T<i32) 0 else cast_i{S, x>>width{S}})
  top := each{make_top, replicate{{S}=>S<T, tup{i8,i16}}}
  def i_off = if (T<i32) 0 else { assert{x%k==0}; cast_i{u64, x/k} }
  # Increment top vector when i*k passes width of bottom vector
  def vb = lb{k}
  def inc{i, {}} = {}
  def inc{i, {t:V, ...ts}} = {
    if ((i+1+i_off)%(1<<(elwidth{V}-vb)) == 0) { t += V**1; inc{i,ts} }
  }
  tup{top, inc}
}

# i8 & i16 /w; 64 bits/iter; SWAR
itab_4_16:*u64 = maketab{4,16} # 16 elts, 128B
def thresh{c==0, T==i8 } = 32
def thresh{c==0, T==i16} = 16
fn slash{c==0, T if T<=i16}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def n = 64/tw
  def tab = if (tw==8) itab else itab_4_16
  j:u64 = 0
  def inc = base{1<<tw, n**n}
  @for_special_buffered{r,8} (w in *u8~~w over sum) {
    def rn = if (has_popc) r+popc{w} else 0
    def step{w} = {
      i := load{tab, w}
      storeu{*u64~~r, j + i}
      r += popc_alt{w, i, tw}
      j += inc
    }
    if (tw==8) { step{w} }
    else { step{w&0xf}; step{w>>4} }
    if (has_popc) r = rn  # Shorter dependency chain
  }
}

# i16 /w & i32 x+/w; 8 elts/iter; 64 bit table input, expanded to 128 or 256 via topper
def simd128{} = hasarch{'X86_64'} | hasarch{'AARCH64'}
def thresh{c==0, T==i16 if simd128{}} = 32
def thresh{c==0, T==i32 if simd128{}} = 16
fn slash{c==0, T if simd128{} and i16<=T and T<=i32}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def I = [16]i8
  j := I**(if (T==i16) 0 else cast_i{i8,x})
  def {top, inctop} = topper{T, I, 8, x}
  @for_special_buffered{r,8} (w in *u8~~w over i to sum) {
    ind := load{itab, w}
    pc := popc_alt{w, ind, 8}
    v := mzip{j + I~~make{[2]u64, ind, 0}, select{top,0}, 0}
    def st{k, v:V} = store{*V~~r, k, v}
    if (T==i16) st{0, v}
    else each{st, iota{2}, mzip{v, select{top,1}}}
    r += pc
    j += I**8
    inctop{i, top}
  }
}

# i8 & i16 w/x; 128 bits/iter; [16]i8 shuffle
def shufb128{} = hasarch{'SSSE3'} | hasarch{'AARCH64'}
def thresh{c==1, T==i8  if shufb128{}} = 64
def thresh{c==1, T==i16 if shufb128{}} = 32
fn slash{c==1, T if T<=i16 and shufb128{}}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def V = [16]i8
  @for_special_buffered{r,8} (w in *u8~~wp over i to sum) {
    ind := load{itab, w}
    pc := popc_alt{w, ind, 8}
    s := V~~make{[2]u64, ind,0}
    if (T==i16) { s+=s; s = V~~mzip{s, s+V**1, 0} }
    res := sel{V, load{*V~~(x+8*i)}, s}
    if (T==i8) storeu{*u64~~r, extract{[2]u64~~res, 0}}
    else store{*V~~r, 0, res}
    r+= pc
  }
}

# i32 w/x; 8 elts/iter into 2 steps; [16]i8 shuffle
i32tab:*u32 = maketab{4,8,2} # 16 elts, 64B
def thresh{c==1, T==i32 if shufb128{}} = 8
fn slash{c==1, T==i32 if shufb128{}}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def V = [16]i8
  expander := make{V, iota{16}>>2}
  trail := make{V, tail{2,iota{16}}}
  def step{w,i} = {
    ind := load{i32tab, w}
    pc := popc_alt{w, ind, 6}
    s := sel{[16]i8, V~~make{[4]u32, ind, ... 3**0}, expander} | trail
    res := sel{V, load{*V~~(x+4*i)}, s}
    store{*V~~r, 0, res}
    r+= pc
  }
  @for_special_buffered{r,8} (w in *u8~~wp over i to sum) {
    def rn = if (has_popc) r+popc{w} else 0
    step{w&0xf, 2*i}
    step{w>>4, 2*i+1}
    if (has_popc) r = rn
  }
}

# i32 & i64 w/x & x+/w; 256 bits/step, 8 elts/iter; [8]i32 shuffle
i64tab:*u64 = maketab{4,16,1,{x}=>(1+x)*0x100 + x} # 16 elts, 128B
def thresh{c, T==i32 if hasarch{'AVX2'}} = 32
def thresh{c, T==i64 if hasarch{'AVX2'}} = 8
fn slash{c, T if hasarch{'AVX2'} and T>=i32}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def V = [8]u32
  expander := make{[32]u8, merge{...each{tup{., ... 3**128}, iota{8}>>lb{tw/32}}}}
  def from_ind = if (c) {
    i:u64 = 0
    {j} => { v:=load{*V~~x, i}; ++i; sel{V, v, j} }
  } else if (T==i32) {
    i := V ** ty_u{x}
    ii := V ** 8
    {j} => { v:=i+V~~j; i+=ii; v }
  }
  def tab = if (tw==32) itab else i64tab
  def step{r, w} = {
    s:= load_widen{*u8~~(tab+w), 0, V}
    store{*V~~r, 0, from_ind{s}}
  }
  @for_special_buffered{r,8} (w in *u8~~wp over sum) {
    pc := popc{w}
    if (tw==32) {
      step{r,w}
    } else {
      h := w&0xf
      step{r, h}
      step{r+popc_rand{h}, w>>4}
    }
    r += pc
  }
}

# everything; 512 bits/iter; AVX-512 compress
def thresh{c, T==i8  if hasarch{'AVX512VBMI2'}} = 256
def thresh{c, T==i16 if hasarch{'AVX512VBMI2'}} = 128
def thresh{c, T==i32 if hasarch{'AVX512F'}}     = 64
def thresh{c, T==i64 if hasarch{'AVX512F'}}     = 16
fn slash{c, T if hasarch{if (width{T}>=32) 'AVX512F' else 'AVX512VBMI2'}}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def f = fmtnat
  def wt = width{T}
  def vl = 512/wt
  def V = [vl]T
  def X = getter{c, V, x}
  def I = ty_u{vl}
  @for (w in *(ty_u{vl})~~w over cdiv{l,vl}) {
    def emitT{O, name, ...a} = emit{O, merge{'_mm512_',name,'_epi',f{wt}}, ...a}
    m := [vl]u1~~w
    c := popc{w}
    x := X{}
    # The compress-store instruction performs very poorly on Zen4,
    # and is also a lot worse than the following on Tiger Lake
    # emitT{void, 'mask_compressstoreu', r, m, x}
    cs := cast_i{I,promote{i64,1}<<(c%64) - 1}
    if (vl==64) cs -= cast_i{I,c}>>6
    v := emitT{V, 'mask_compress', x, m, x}
    store_masked_hom{r, [vl]u1~~cs, v}
    r += c
  }
}

export{'si_1slash8' , slash{0, i8 }}
export{'si_1slash16', slash{0, i16}}; export{'si_thresh_1slash16', u64~~thresh{0, i16}}
export{'si_1slash32', slash{0, i32}}; export{'si_thresh_1slash32', u64~~thresh{0, i32}}
export{'si_2slash8' , slash{1, i8 }}; export{'si_thresh_2slash8' , u64~~thresh{1, i8 }}
export{'si_2slash16', slash{1, i16}}; export{'si_thresh_2slash16', u64~~thresh{1, i16}}
export{'si_2slash32', slash{1, i32}}; export{'si_thresh_2slash32', u64~~thresh{1, i32}}
export{'si_2slash64', slash{1, i64}}; export{'si_thresh_2slash64', u64~~thresh{1, i64}}

def scalwidth{T} = if (vect{T}) elwidth{T} else width{T}

# pext, or boolean compress
def pext_width{}  = if (hasarch{'AVX2'}) 4 else 1
def thresh_bool{} = if (hasarch{'AVX2'}) 128 else 16
def pext_popc{x:T, m:T} = {
  def w = scalwidth{T}
  def scal{v} = if (vect{T}) T**v else v
  def lowbits{w,k} = base{1<<k, cdiv{w,k}**1}
  # At each step, x and z are split into groups of length k
  # - z tells how many bits in the group are NOT used
  # - x contains the bits, with z zeros above
  def build{k==1} = tup{x&m, ~m}
  def build{k if k > 1} = {
    def h = k>>1          # Increase size from h to k
    {x,z} := build{h}
    def low_s = lowbits{w,k}  # Low bit in each new group
    def low = scal{low_s}
    if (k == 2) {
      z0 := z & low
      zm := z>>1 & low
      tup{ x - (x>>1 & z0), zm + z0 }
    } else if (hasarch{'AVX2'} and vect{T} and k >= 32) {
      # We have variable shifts at these sizes
      lh := scal{low_s*tail{h}}
      zl := z & lh
      def S = re_el{ty_u{k}, T}
      tup{T~~(S~~(x&~lh) >> S~~zl) | (x&lh), T~~(S~~z >> h) + zl}
    } else {
      # SWAR shifter: shift x by sh*o, in length-k groups
      def shift{sh, o, x} = {
        l := o & low; m := l<<k - l
        s := (x & m)>>sh | (x &~ m)
        if (2*sh<k/2) shift{2*sh, o>>1, s} else s
      }
      # Shift high x group down by low z, then add halves of z
      odd:T = scal{low_s*(1<<k - 1<<h)} # Top half
      ze := z&~odd
      z1 := ze + scal{low_s*tail{k-1}}  # z-1, as signed k-bit
      move := odd &~ (z1<<1)         # Only groups where z>0 move
      tup{
        (x&~move) | shift{1, z1, x&move}>>1,
        (z&odd)>>h + ze
      }
    }
  }
  # Compose k/g groups with k/g-1 regular shifts
  def multi_shift{x, z, g, k, sc} = {
    o := z * sc{lowbits{k,g}}  # Offsets by prefix sum
    def s = tail{g}
    def s0 = sc{s}
    def oo{sh} = if (sh==g) z else o>>(sh-g)      # Offset for group
    def gr{sh} = (x & sc{s<<sh}) >> (oo{sh} & s0) # Shifted group
    pe := fold{|, x&s0, each{gr, g*slice{iota{k/g},1}}}
    tup{pe, o>>(k-g)}
  }
  def build{k==32 if hasarch{'AVX2'} and vect{T}} = {
    def S = re_el{ty_u{k}, T}
    def c{T,vs} = each{{v}=>T~~v, vs}
    c{T, multi_shift{...c{S, build{8}}, 8, k, {s}=>S**s}}
  }
  def build{k if not vect{T} and k > 8} = {
    multi_shift{...build{8}, 8, k, {s}=>s}
  }
  # Final result
  def {pe, z} = build{w}
  tup{pe, scal{w} - z}
}

def pext_width {if hasarch{'PCLMUL'} > hasarch{'AVX2'}} = 2
def thresh_bool{if hasarch{'PCLMUL'} > hasarch{'AVX2'}} = 32
def pext_popc{x0:V, m0:V if hasarch{'PCLMUL'} and V==[2]u64} = {
  def clmul{a, b} = zip{...@collect (j to 2) clmul{a,b,j}, 0}
  m := m0
  x := x0 & m
  d := ~m << 1           # One bit of the position difference at x
  c := V**tail{64}
  @unroll (i to lb{scalwidth{V}}) {
    def sh = 1 << i
    def shift_at{v, s} = { v = (v&~s) | (v&s)>>sh }
    p := clmul{d, c}     # xor-scan
    d = d &~ p           # Remove even bits
    p &= m
    shift_at{m, p}
    shift_at{x, p}
  }
  tup{x, @collect (j to 2) popc{extract{m0,j}}}
}

def pext_width {if fast_BMI2{}} = 1
def thresh_bool{if fast_BMI2{}} = 512
def pext_popc{x:T, m:T if fast_BMI2{} and T==u64} = tup{pext{x, m}, popc{m}}

fn compress_bool(w:*u64, x:*u64, r:*u64, n:u64) : void = {
  cw:u64 = 0; # current word
  ro:u64 = 0; # offset in word where next bit should be written; never 64
  def add_bits{{v, c}} = {
    cw |= v<<ro
    ro2 := ro+c
    if (ro2 >= 64) {
      store{r, 0, cw}; ++r
      cw = 0; if (ro>0) cw = v>>(64-ro)
    }
    ro = ro2%64
  }
  def extract{t, i if ktup{t}} = select{t,i}
  def v = pext_width{}
  if (v > 1) {
    def V = [v]u64
    d := cdiv{n,64}; e := d/v
    @for (w in *V~~w, x in *V~~x over i to cdiv{d,v}) {
      vc := pext_popc{x, w}
      def add{j} = add_bits{each{extract{., j}, vc}}
      if (i < e) {
        @unroll (j to v) add{j}
      } else {
        # last write: between 1 and v-1 words
        m := d%v
        def ar{j} = { add{j}; def jn=j+1; if (jn<v-1 and jn<m) ar{jn} }
        ar{0}
      }
    }
  } else {
    @for (w, x over i to cdiv{n,64}) add_bits{pext_popc{x, w}}
  }
  if (ro > 0) store{r, 0, cw}
}

export{'si_compress_bool', compress_bool}
export{'si_thresh_compress_bool', u64~~thresh_bool{}}

# Spaced boolean masks, such as 2b000001000001...0000010000 for 6
# Mask i is the smallest possible mask containing 1 every i bits:
# there would be a 1 just past the top bit.
# The number of trailing zeros is 64%i , and the popcount is 64/i .
def get_spaced_masks{i} = (1<<64 - 1<<(64%i)) / tail{i}
spaced_masks:*u64 = get_spaced_masks{1 + iota{64}}
export{'si_spaced_masks', spaced_masks}
