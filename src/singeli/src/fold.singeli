include './base'
include './mask'
if_inline (hasarch{'BMI2'}) include './bmi2'
include './spaced'
include './scan_common'

def opsh64{op}{v:([4]f64), perm} = op{v, shuf{[4]u64, v, perm}}
def opsh32{op}{v:([2]f64), perm} = op{v, shuf{[4]u32, v, perm}}
def mix{op, v:([4]f64) if hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 4b2301}, 4b1032} }
def mix{op, v:([2]f64) if hasarch{'X86_64'}} = opsh32{op}{v, 4b1032}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T, op}(x:*T, len:u64) : T = {
  assert{len > 0}
  a := load{x, 0}
  @for (x over _ from 1 to len) a = op{a, x}
  a
}
fn fold_idem{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4b2222}
                     r = opsh64{op}{r, 4b1111}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'si_fold_min_f64', fold_idem{f64,min}}
export{'si_fold_max_f64', fold_idem{f64,max}}

fn fold_assoc_0{T, op}(x:*T, len:u64) : T = {
  a:T = 0
  @for (x over len) a = op{a, x}
  a
}
fn fold_assoc_0{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~maskOf{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'si_sum_f64', fold_assoc_0{f64,+}}


# Short-row boolean folds: main challenge is bit packing
def fold_rows_bit_lt64{
  op, run_loop2, run_loop4, pext_res, mult_in,
  off,    # mask offset for generic methods
  xx, rx, # input and output xor for cases not specialized to individual functions
  rxs,    # output xor only, where and/or are specialized
  xp, rp, n, l
} = {
  nw := cdiv{n*l,64}
  def loop_T{T, proc_word} = {
    @for (x in xp, r in *T~~rp over nw) r = cast_i{T, rxs{proc_word{x}}}
  }
  if (l == 2) {
    def extract = if (fast_BMI2{}) pext{., u64~~64w2b01} else {
      # repeated shift-or-mask
      def step{x, {m, sh}} = { def a=x&m; a|a>>sh }
      def ss = 1<<iota{5}
      def ms = (1<<64 - 1) / (1 + 1<<ss)
      fold{step, ., each{tup, ms, ss}}
    } 
    run_loop2{{op} => loop_T{u32, {x} => extract{op{x, x>>1}}}}
  } else if (l == 4) {
    m:u64 = 64w2b0001; t := m<<3
    def extract = if (fast_BMI2{}) pext{., t} else {
      {x} => {
        a:= x & t
        a = (a * 4r2b001) & 64w0xf000
        a = (a * 4r0x001) >> (64-16)
      }
    }
    run_loop4{m, t, {get} => loop_T{u16, {x} => extract{get{x}}}}
  } else { # generic width<64
    {e0, d} := unaligned_spaced_mask_mod{l}
    el:= e0 << (l-1)                        # ending bit of each row
    e := if (same{off,-1}) el else e0<<off  # or selected bit
    c:u64 = 0; c|0  # carry, use depends on algorithm (unused for select)
    def {write_bits, flush_bits} = {
      r:u64 = 0
      rh := *u32~~rp
      ri:ux = 0
      def write{rb, n_bits} = {
        r |= rb << ri
        ri += promote{ux, n_bits}
        if (ri >= 32) {
          store{rh, 0, cast_i{u32,rx{r}}}; ++rh;
          r >>= 32; ri -= 32
        }
      }
      def flush{} = if (ri > 0) store{rh, 0, cast_i{u32,rx{r}}}
      tup{write, flush}
    }
    def finish_step{...a} = {
      write_bits{...a}
      e = e>>d | e<<(l-d) # update end mask for next iteration
    }
    if (fast_BMI2{}) {
      @for (x in xp over nw) finish_step{pext_res{xx{x}, e, c}, popc{e}}
    } else {
      # Emulate pext with 1, 2, or 3 multiply/mask steps.
      # To move size-a groups spaced at distance b together,
      # the multiplier has up to b/a bits spaced by b-a.
      dm:= cast_i{usz, popc{el}}      # minimum output bits per word
      dm-= promote{usz, l&(l-1) == 0} # for divisors of 64, e0 effectively overflows; subtract 1 to correct
      def loop{...par} = {
        et:= e0 << clz{e0}
        @for (xo in xp over nw) {
          x:= xx{xo}
          def {s, mod_rb} = if (length{select{par,0}} == 3) {
            # fast path for l==3
            s:= op{op{x, x<<1}, op{x<<2, c}}
            c = op{x>>63, x>>62}
            tup{s, {rb}=>rb}
          } else {
            mult_in{x, e, c}
          }
          nb:= dm + promote{usz, e>=e0} # = popc{e}
          # the multiply-mask sequence (last "mask" is the shift by (64 - nb))
          def extract = match {
            {       {q},         {}} => (et &~ (et - (s & e))) * q
            {{...qs, q}, {...bs, b}} => (extract{qs, bs} & b) * q
          }
          rb:= extract{...par} >> (64 - nb)
          finish_step{mod_rb{rb}, nb}
        }
      }
      if (l == 3) {
        mult0:u64 = base{1<< 2, 3**1};  top3:u64 = base{1<<9,  8**(1<<3-1)}>>2
        mult1:u64 = base{1<< 6, 3**1};  top9:u64 = base{1<<27, 3**(1<<9-1)}<<1
        mult2:u64 = base{1<<18, 3**1}
        loop{tup{mult0,mult1,mult2}, tup{top3, top9}}
      } else if (l < 8) {
        assert{l > 4}
        ld:= l-1; lld:= l*ld
        {mult0, _} := unaligned_spaced_mask_mod{ld}
        mult0 &= u64~~1<<lld - 1
        {mult1, _} := unaligned_spaced_mask_mod{lld}
        ll:= l*l
        {tk, tkd} := unaligned_spaced_mask_mod{ll}; tk <<= tkd
        topk := tk - tk>>l; topk|= topk<<ll | topk>>ll
        loop{tup{mult0,mult1}, tup{topk}}
      } else {
        {mult, _} := unaligned_spaced_mask_mod{l-1}
        if (l==8) mult &= 1<<(7*8) - 1
        loop{tup{mult}, tup{}}
      }
    }
    flush_bits{}
  }
}

fn select_rows_bit_lt64(xp:*u64, rp:*u64, n:usz, l:usz, o:usz) : void = {
  assert{l < 64}; assert{o < l} # Row length, and offset within row
  def run_loop2{loop} = loop{{a,b} => a>>o}
  def run_loop4{m, t, loop} = loop{{x} => x<<(l-1-o)}
  def pext_res{x, e, c} = pext{x, e}
  def mult_in{x, e, c} = tup{x, {r}=>r}
  def id{x} = x
  fold_rows_bit_lt64{
    {a,b}=>a, run_loop2, run_loop4, pext_res, mult_in, o, id, id, id,
    xp, rp, n, l
  }
}

fn xor_words(init:u64, x:*u64, l:usz):u64 = {
  @for (x over l) init ^= x
  init
}
def bit_output{rp:*T} = {
  buf:u64 = 0                    # Buffer for result bits
  def output{i, bit, mod} = {
    buf = buf>>1 | promote{u64, bit}<<63
    if ((i+1)%64==0) { store{rp, 0, mod{buf}}; ++rp }
  }
  def fixbuf{mod} = { buf = mod{buf} }
  def flush_bits{n, mod} = {
    q:=(-n)%64; if (q!=0) store{rp, 0, mod{buf} >> q}
  }
  def flush_bits{n} = flush_bits{n, {b}=>b}
  tup{output, fixbuf, flush_bits}
}
# word and alignment of start of next row
def next_start{i, m} = {
  bn := promote{u64, i+1} * promote{u64, m}
  tup{bn/64, bn%64}
}
fn xor_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, eq:u1) : void = {
  def p64 = promote{u64, .}
  rx:= -(p64{eq} &~ p64{l})  # ne to eq conversion
  if (l <= 64 and not (l%8==0 and l>16)) {
    def run_loop2{loop} = loop{^}
    def run_loop4{m, t, loop} = loop{{x} => { x2:= x^(x<<1); x2^(x2<<2) }}
    def xor_word = prefix_byshift{^, <<}
    def pext_in{xo, e, c} = {
      x := xor_word{xo}
      rb:= x ^ (x<<l | c)
      c  = (x ^ -(x>>63))>>(64-l)
      rb
    }
    def pext_res{x, e, c} = pext{pext_in{x, e, c}, e}
    def mult_in{x, e, c} = tup{pext_in{x, e, c}, {r}=>r}
    fold_rows_bit_lt64{
      ^, run_loop2, run_loop4, pext_res, mult_in, -1, {x}=>x, ^{rx,.}, ^{rx,.},
      xp, rp, n, l
    }
  } else {
    def fixout = ^{rx, .}
    def {add_bit, _, flush_bits} = bit_output{rp}
    def add_bit{i, bit} = add_bit{i, bit, fixout}
    def xor_loop{len} = {
      o:u64 = 0  # Carry
      j:u64 = 0; @for (i to n) {
        def {jn, sh} = next_start{i, l}
        s := xor_words(o, xp + j, len)
        e := load{xp, jn}
        s ^= e & (if (not same{len,1}) -p64{jn >= j + p64{len}} else j - jn)
        o  = e >> sh
        add_bit{i, popc{s ^ o}}
        j = jn+1
      }
    }
    ll := l/64
    if (l <= 64) {
      bm:= u64~~2<<(l-1) - 1
      k:= l/8
      @for (i to n) add_bit{i, popc{bm & loadu{*u64~~(*u8~~xp + k*i)}}}
    } else if (l < 128) xor_loop{1}
    else if (l%64==0) {
      @for (i to n) add_bit{i, popc{xor_words(0, xp+ll*i, ll)}}
    }
    else xor_loop{ll}
    flush_bits{n, fixout}
  }
}

fn or_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, op_and:u1) : void = {
  def {add_bit, set_out, flush_bits} = bit_output{rp}
  if (l < 64) {
    def run_loop2{loop} = if (op_and) loop{&} else loop{|}
    def run_loop4{m, t, loop} = {
      if (op_and) loop{{x} => x & ((x&~t) + m)}
      else        loop{{x} => x | ((x| t) - m)}
    }
    def xor_word = prefix_byshift{^, <<}
    def pext_in{x, e, c} = {
      m := e<<1 | 1
      t := e | 1<<63
      tup{x | ((x|t) - m), t}
    }
    def pext_res{x, e, c} = pext{...pext_in{x, e, c}}
    def mult_in{x, e, c} = {
      {s, t} := pext_in{x, e, c}
      cs:= c; c = (s&~e) >> 63
      tup{s, {rb}=>rb|cs}
    }
    def xx = ^{-promote{u64, op_and}, .}
    fold_rows_bit_lt64{
      |, run_loop2, run_loop4, pext_res, mult_in, -1, xx, xx, {r}=>r,
      xp, rp, n, l
    }
    return{}
  } else if (l < 128) {
    c:u64 = (promote{u64, l}-1) &- op_and # a row gives 1 if its sum is >c
    o:u64 = 0
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      s := o + popc{load{xp,j}}
      e := load{xp,jn}
      s += popc{e & (j - jn)}  # mask is 0 if j==jn, or -1
      o  = popc{e >> sh}
      add_bit{i, s > c+o, {rw}=>rw}
      j = jn+1
    }
  } else {
    rx := -promote{u64, op_and}
    def fixout = ^{rx, .}
    o:u64 = 0  # Saved bits
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      e := load{xp,jn} ^ rx
      l := ~(u64~~0) << sh
      rb:u64 = 1
      if ((o | (e &~ l)) == 0) { # Search for shortcut
        @for (i from j to jn) if (load{xp,i} != rx) goto{'found'}
        rb = 0; setlabel{'found'}
      }
      o = e & l
      add_bit{i, rb, fixout}
      j = jn+1
    }
    set_out{fixout}
  }
  flush_bits{n}
}
export{'si_xor_rows_bit', xor_rows_bit}
export{'si_or_rows_bit', or_rows_bit}
export{'si_select_cells_bit_lt64', select_rows_bit_lt64}
