include './base'
include './mask'
include './spaced'
include './scan_common'
include './vecfold'

def opsh64{op}{v:[_](f64), ...perm} = op{v, shuf{v, ...perm}}
def mix{op, v:([4]f64) if hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 1,0}, 2,3,0,1} }
def mix{op, v:([2]f64) if hasarch{'X86_64'}} = opsh64{op}{v, 1,0}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T, op}(x:*T, len:u64) : T = {
  assert{len > 0}
  a := load{x, 0}
  @for (x over _ from 1 to len) a = op{a, x}
  a
}
fn fold_idem{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4**2}
                     r = opsh64{op}{r, 4**1}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'si_fold_min_f64', fold_idem{f64,__min}}
export{'si_fold_max_f64', fold_idem{f64,__max}}

fn fold_assoc_0{T, op}(x:*T, len:u64) : T = {
  a:T = 0
  @for (x over len) a = op{a, x}
  a
}
fn fold_assoc_0{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~mask_of_first{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'si_sum_f64', fold_assoc_0{f64,+}}


def extract_column_pow2{T, x0, r0, nv, k} = {
  def V = [arch_defvw / width{T}]T
  xv := *V~~x0
  @for (r in *V~~r0 over i to nv) {
    xs := each{load{xv, .}, iota{k}}
    def unzip0{w} = if (not hasarch{'X86_64'}) {
      unzip{..., 0} # Sane instruction set
    } else {
      if (w <= 16) {
        # Pack instructions
        m := make{V, - (iota{vcount{V}}%k == 0)}
        xs = each{&{m, .}, xs} # Mask off high bits
        def D = el_m{V}
        {a, b} => packs128{D~~a, D~~b}
      } else {
        # Two-vector shuffles
        # Could also be used for 1/2-byte with ending gap >= 4 bytes,
        # less instructions but it doesn't seem faster
        def c = 128/w
        def sh = shuf{[c]ty_f{w}, ., 2*iota{c} % c}
        {...ab} => sh{ab}
      }
    }
    if (not hasarch{'X86_64'} or T != u16 or hasarch{'SSE4.1'}) {
      r = tree_fold{unzip0{width{T}}, xs}
    } else {
      # No unsigned saturation: sign-extend then use unsigned
      def D = [4]i32
      def f = tree_fold{unzip0{32}, .}
      def proc{hx} = {
        ri := D~~f{hx}
        top := D**(1<<15); m := D**tail{16}
        (ri & m) | (D~~(ri&top == top) &~ m)
      }
      r = V~~packs128{each{proc, split{k/2, xs}}}
    }
    if (width{V} > 128) { # Lane axis wasn't packed, need to shuffle to bottom
      def tr{E,a, r} = shuf{[1<<a]E, r, tr_iota{shiftright{a-1, iota{a}}}}
      def lc = k > 4
      if (lc) r = tr{u64,2, r}
      r = tr{ty_u{128/k}, lb{k} + (not lc), r}
    }
    xv += k
  }
}

def extract_column_modperm{x0, r0, nv, l, el, vl} = {
  # Build modular permutations
  def V = [vl]u8
  def H = [vl/2]u16
  p2 := ctz{l}; l >>= p2    # Decompose into l<<p2 with odd l
  e  := p2 + promote{ux,el} # Absorb into element size for most computation
  l8 := cast_i{u8, l}
  li := cast_i{u8, l + 2 * ((l-1) + (l&2))}  # Inverse mod vl
  elo:= V**tail{u8, e}
  ie := iota{V} & elo
  kmul := make{H, 2*iota{vl/2}} &~ H~~elo
  def mu16{k} = {
    k16 := H ** k
    prd := shuf{V~~(kmul * k16), 0,0}
    if (e == 0) prd += V~~(k16 << 8)
    (prd & V**(vl-1)) + ie
  }
  si := mu16{l8}
  sii := mu16{li}
  def swap_ms = if (vl == 16) ({x}=>x) else {
    ms := (V**16 & sii) == (V**16 &~ iota{V})
    {x} => blend_hom{x, shuf{[4]u64, x, 2,3,0,1}, ms}
  }

  # Blend masks
  def mg = { # Iteration i should select where mg == V**i
    ss := (si < V**(l8<<e)) & (ie == V**0)
    vs := V**0xff - scan_assoc_id0{+}{ss}
    swap_ms{shuf{[16]u8, vs, sii}}
  }
  mgo := mg - V**(l8 & 3)
  mgm := (mgo - V**1) & V**3
  m4s := @collect (i to 3) mgm == V**i

  # Main loop
  def loop{output} = {
    xv := *V~~x0
    rv := *V~~r0
    # Each iteration handles l vectors from x
    @for (i to nv<<p2) {
      # 1 or 3 initial vectors
      r := load{xv,0}; ++xv
      if ((l & 2) != 0) {
        def {m0, _, m2} = m4s
        re := blend_hom{load{xv,0}, load{xv,1}, m2}
        r = blend_hom{re, r, m0}
        xv += 2
      }
      # Then the rest in groups of 4
      mh := mgo
      @for (l/4) {
        {l0, ...ls} := each{load{xv,.}, iota{4}}
        r4 := fold{blend_hom, l0, ls, m4s}
        r = blend_hom{r, r4, mh < V**4}
        mh -= V**4; xv += 4
      }
      def write{r} = {
        store{rv, 0, shuf{[16]u8, r, si}}
        ++rv
      }
      output{r, i, write}
    }
  }

  # Handle odd and even strides separately
  if (p2 == 0) {
    loop{{r, i, write} => write{swap_ms{r}}}
  } else {
    # Store results 
    def add_res = {
      ra := V**0  # Accumulator
      i  := make{V, iota{vl}%16}
      o  := V**(u8~~1<<el)
      bl := i & elo < o
      sh := i - o
      {r} => { ra = blend_hom{shuf{[16]u8, ra, sh}, r, bl} }
    }
    il := make{V, iota{vl} % 16}
    # Shuffle to undo interleaving of add_res
    def __shr{x:(V), sh if hasarch{'X86_64'}} = V~~(H~~x >> sh)
    def __shl{x:(V), sh if hasarch{'X86_64'}} = V~~(H~~x << sh)
    def __shr{x:(V), sh:T if hasarch{'AARCH64'} and not vect{T}} = x << V**cast_i{u8,-sh}
    def __shl{x:(V), sh:T if hasarch{'AARCH64'} and not vect{T}} = x << V**cast_i{u8, sh}
    def uz_lane = {
      l  := V**tail{u8, el}
      h  := V**(16 - u8~~16>>p2)
      dz := (il & l) | (h &~ il)>>(4-e) # low, high->middle
      dz |= (il &~ (l | h))<<p2         # middle->high
      shuf{[16]u8, ., dz}
    }
    # Adjust modular permutation to apply after unzipping
    si = uz_lane{(si & V**(vl - u8~~1<<e)) >> p2}
    si ^= il &~ V**((16 - u8~~1<<e) >> p2)
    # Cross-lane follow-up
    def cross = if (vl == 16) { {x}=>x } else {
      si = shuf{[4]u64, si, 0,1,0,1}
      assert{p2 <= 2}
      cr            := make{[8]u32, tr_iota{0,2,1}}
      if (p2 > 1) cr = make{[8]u32, tr_iota{2,0,1}}
      shuf{[8]u32, ., cr}
    }
    # Run, writing every 1<<p2 steps
    plo := tail{usz, p2}
    loop{{r, i, write} => {
      def ra = add_res{r}
      if ((plo &~ i) == 0) write{cross{uz_lane{ra}}}
    }}
  }
}

# Select one element out of every l, element width 1<<el bytes
# Maximum of n result values, return actual number written
fn extract_column(x0:*void, r0:*void, n:usz, l:usz, el:u8) : usz = {
  n <<= el
  def vl = arch_defvw / 8
  def thr = __min{vl+2, 20}
  if ((not has_simd) or n < vl or l > usz~~thr>>el or l<<el >= thr) return{0}
  nv := n / vl
  if (has_simd and is_pow2{l}) {
    def try_unzip{T, k} = if (k < thr and l == k) {
      extract_column_pow2{T, x0, r0, nv, k}
      goto{'ret'}
    }
    # 10 loops: i8 2,4,8,16; i16 2,4,8; i32 2,4; i64 2
    @unroll (ek to 4) if (el == ek) {
      def T = ty_u{8<<ek}
      @unroll (p from 1 to 5-ek) try_unzip{T, 1<<p}
    }
    return{0}
    setlabel{'ret'}
  } else if (hasarch{'SSE4.1'} or hasarch{'AARCH64'}) {
    extract_column_modperm{x0, r0, nv, l, el, vl}
  } else return{0}
  (usz~~vl>>el) * nv
}


# Short-row boolean folds: main challenge is bit packing
def fold_rows_bit_lt64{
  op, run_loop2, run_loop4, pext_res, mult_in,
  off,    # mask offset for generic methods
  xx, rx, # input and output xor for cases not specialized to individual functions
  rxs,    # output xor only, where and/or are specialized
  xp, rp, n, l
} = {
  nw := cdiv{n*l,64}
  def loop_T{T, proc_word} = {
    @for (x in xp, r in *T~~rp over nw) r = cast_i{T, rxs{proc_word{x}}}
  }
  if (l == 2) {
    def extract = if (fast_BMI2{}) pext{., u64~~64w2b01} else {
      # repeated shift-or-mask
      def step{x, {m, sh}} = { def a=x&m; a|a>>sh }
      def ss = 1<<iota{5}
      def ms = tail{64} / (1 + 1<<ss)
      fold{step, ., each{tup, ms, ss}}
    } 
    run_loop2{{op} => loop_T{u32, {x} => extract{op{x, x>>1}}}}
  } else if (l == 4) {
    m:u64 = 64w2b0001; t := m<<3
    def extract = if (fast_BMI2{}) pext{., t} else {
      {x} => {
        a:= x & t
        a = (a * 4r2b001) & 64w0xf000
        a = (a * 4r0x001) >> (64-16)
      }
    }
    run_loop4{m, t, {get} => loop_T{u16, {x} => extract{get{x}}}}
  } else { # generic width<64
    {e0, d} := unaligned_spaced_mask_mod{l}
    el:= e0 << (l-1)                        # ending bit of each row
    e := if (same{off,-1}) el else e0<<off  # or selected bit
    c:u64 = 0; c|0  # carry, use depends on algorithm (unused for select)
    def {write_bits, flush_bits} = {
      r:u64 = 0
      rh := *u32~~rp
      ri:ux = 0
      def write{rb, n_bits} = {
        r |= rb << ri
        ri += promote{ux, n_bits}
        if (ri >= 32) {
          store{rh, 0, cast_i{u32,rx{r}}}; ++rh;
          r >>= 32; ri -= 32
        }
      }
      def flush{} = if (ri > 0) store{rh, 0, cast_i{u32,rx{r}}}
      tup{write, flush}
    }
    def finish_step{...a} = {
      write_bits{...a}
      e = e>>d | e<<(l-d) # update end mask for next iteration
    }
    if (fast_BMI2{}) {
      @for (x in xp over nw) finish_step{pext_res{xx{x}, e, c}, popc{e}}
    } else {
      # Emulate pext with 1, 2, or 3 multiply/mask steps.
      # To move size-a groups spaced at distance b together,
      # the multiplier has up to b/a bits spaced by b-a.
      dm:= cast_i{usz, popc{el}}    # minimum output bits per word
      dm-= promote{usz, is_pow2{l}} # for divisors of 64, e0 effectively overflows; subtract 1 to correct
      def loop{...par} = {
        et:= e0 << clz{e0}
        @for (xo in xp over nw) {
          x:= xx{xo}
          def {s, mod_rb} = if (length{select{par,0}} == 3) {
            # fast path for l==3
            s:= op{op{x, x<<1}, op{x<<2, c}}
            c = op{x>>63, x>>62}
            tup{s, {rb}=>rb}
          } else {
            mult_in{x, e, c}
          }
          nb:= dm + promote{usz, e>=e0} # = popc{e}
          # the multiply-mask sequence (last "mask" is the shift by (64 - nb))
          def extract = match {
            {       {q},         {}} => (et &~ (et - (s & e))) * q
            {{...qs, q}, {...bs, b}} => (extract{qs, bs} & b) * q
          }
          rb:= extract{...par} >> (64 - nb)
          finish_step{mod_rb{rb}, nb}
        }
      }
      if (l == 3) {
        mult0:u64 = base{1<< 2, 3**1};  top3:u64 = base{1<<9,  8**(1<<3-1)}>>2
        mult1:u64 = base{1<< 6, 3**1};  top9:u64 = base{1<<27, 3**(1<<9-1)}<<1
        mult2:u64 = base{1<<18, 3**1}
        loop{tup{mult0,mult1,mult2}, tup{top3, top9}}
      } else if (l < 8) {
        assert{l > 4}
        ld:= l-1; lld:= l*ld
        {mult0, _} := unaligned_spaced_mask_mod{ld}
        mult0 &= tail{u64, lld}
        {mult1, _} := unaligned_spaced_mask_mod{lld}
        ll:= l*l
        {tk, tkd} := unaligned_spaced_mask_mod{ll}; tk <<= tkd
        topk := tk - tk>>l; topk|= topk<<ll | topk>>ll
        loop{tup{mult0,mult1}, tup{topk}}
      } else {
        {mult, _} := unaligned_spaced_mask_mod{l-1}
        if (l==8) mult &= tail{7*8}
        loop{tup{mult}, tup{}}
      }
    }
    flush_bits{}
  }
}

fn extract_column_bit_lt64(xp:*u64, rp:*u64, n:usz, l:usz, o:usz) : void = {
  assert{l < 64}; assert{o < l} # Row length, and offset within row
  def run_loop2{loop} = loop{{a,b} => a>>o}
  def run_loop4{m, t, loop} = loop{{x} => x<<(l-1-o)}
  def pext_res{x, e, c} = pext{x, e}
  def mult_in{x, e, c} = tup{x, {r}=>r}
  def id{x} = x
  fold_rows_bit_lt64{
    {a,b}=>a, run_loop2, run_loop4, pext_res, mult_in, o, id, id, id,
    xp, rp, n, l
  }
}

fn xor_words(init:u64, x:*u64, l:usz):u64 = {
  @for (x over l) init ^= x
  init
}
def bit_output{rp:*T} = {
  buf:u64 = 0                    # Buffer for result bits
  def output{i, bit, mod} = {
    buf = buf>>1 | promote{u64, bit}<<63
    if ((i+1)%64==0) { store{rp, 0, mod{buf}}; ++rp }
  }
  def fixbuf{mod} = { buf = mod{buf} }
  def flush_bits{n, mod} = {
    q:=(-n)%64; if (q!=0) store{rp, 0, mod{buf} >> q}
  }
  def flush_bits{n} = flush_bits{n, {b}=>b}
  tup{output, fixbuf, flush_bits}
}
# word and alignment of start of next row
def next_start{i, m} = {
  bn := promote{u64, i+1} * promote{u64, m}
  tup{bn/64, bn%64}
}
fn xor_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, eq:u1) : void = {
  def p64 = promote{u64, .}
  rx:= -(p64{eq} &~ p64{l})  # ne to eq conversion
  if (l <= 64 and not (l%8==0 and l>16)) {
    def run_loop2{loop} = loop{^}
    def run_loop4{m, t, loop} = loop{{x} => { x2:= x^(x<<1); x2^(x2<<2) }}
    def xor_word = prefix_byshift{^, <<}
    def pext_in{xo, e, c} = {
      x := xor_word{xo}
      rb:= x ^ (x<<l | c)
      c  = (x ^ -(x>>63))>>(64-l)
      rb
    }
    def pext_res{x, e, c} = pext{pext_in{x, e, c}, e}
    def mult_in{x, e, c} = tup{pext_in{x, e, c}, {r}=>r}
    fold_rows_bit_lt64{
      ^, run_loop2, run_loop4, pext_res, mult_in, -1, {x}=>x, ^{rx,.}, ^{rx,.},
      xp, rp, n, l
    }
  } else {
    def fixout = ^{rx, .}
    def {add_bit, _, flush_bits} = bit_output{rp}
    def add_bit{i, bit} = add_bit{i, bit, fixout}
    def xor_loop{len} = {
      o:u64 = 0  # Carry
      j:u64 = 0; @for (i to n) {
        def {jn, sh} = next_start{i, l}
        s := xor_words(o, xp + j, len)
        e := load{xp, jn}
        s ^= e & (if (not same{len,1}) -p64{jn >= j + p64{len}} else j - jn)
        o  = e >> sh
        add_bit{i, popc{s ^ o}}
        j = jn+1
      }
    }
    ll := l/64
    if (l <= 64) {
      bm:= u64~~2<<(l-1) - 1
      k:= l/8
      @for (i to n) add_bit{i, popc{bm & loadu{*u64~~(*u8~~xp + k*i)}}}
    } else if (l < 128) xor_loop{1}
    else if (l%64==0) {
      @for (i to n) add_bit{i, popc{xor_words(0, xp+ll*i, ll)}}
    }
    else xor_loop{ll}
    flush_bits{n, fixout}
  }
}

fn or_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, op_and:u1) : void = {
  def {add_bit, set_out, flush_bits} = bit_output{rp}
  if (l < 64) {
    def run_loop2{loop} = if (op_and) loop{&} else loop{|}
    def run_loop4{m, t, loop} = {
      if (op_and) loop{{x} => x & ((x&~t) + m)}
      else        loop{{x} => x | ((x| t) - m)}
    }
    def xor_word = prefix_byshift{^, <<}
    def pext_in{x, e, c} = {
      m := e<<1 | 1
      t := e | 1<<63
      tup{x | ((x|t) - m), t}
    }
    def pext_res{x, e, c} = pext{...pext_in{x, e, c}}
    def mult_in{x, e, c} = {
      {s, t} := pext_in{x, e, c}
      cs:= c; c = (s&~e) >> 63
      tup{s, {rb}=>rb|cs}
    }
    def xx = ^{-promote{u64, op_and}, .}
    fold_rows_bit_lt64{
      |, run_loop2, run_loop4, pext_res, mult_in, -1, xx, xx, {r}=>r,
      xp, rp, n, l
    }
    return{}
  } else if (l < 128) {
    c:u64 = (promote{u64, l}-1) &- op_and # a row gives 1 if its sum is >c
    o:u64 = 0
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      s := o + popc{load{xp,j}}
      e := load{xp,jn}
      s += popc{e & (j - jn)}  # mask is 0 if j==jn, or -1
      o  = popc{e >> sh}
      add_bit{i, s > c+o, {rw}=>rw}
      j = jn+1
    }
  } else {
    rx := -promote{u64, op_and}
    def fixout = ^{rx, .}
    o:u64 = 0  # Saved bits
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      e := load{xp,jn} ^ rx
      l := ~(u64~~0) << sh
      rb:u64 = 1
      if ((o | (e &~ l)) == 0) { # Search for shortcut
        @for (i from j to jn) if (load{xp,i} != rx) goto{'found'}
        rb = 0; setlabel{'found'}
      }
      o = e & l
      add_bit{i, rb, fixout}
      j = jn+1
    }
    set_out{fixout}
  }
  flush_bits{n}
}
export{'si_xor_rows_bit', xor_rows_bit}
export{'si_or_rows_bit', or_rows_bit}
export{'si_select_cells_bit_lt64', extract_column_bit_lt64}
export{'si_select_cells_byte', extract_column}


fn sum_small{T}(xv:*void, ia:usz) : i64 = {
  xv:= *T~~xv
  def A = if (width{T}<=16) i32 else i64
  r:A = 0
  @for (xv over ia) r += promote{A, xv}
  promote{i64, r}
}

include './accumulator'
fn sum_small{T if has_simd}(xv:*void, ia:usz) : i64 = {
  def A = if (width{T}<=16) i32 else i64
  xv:= *T~~xv
  def unr = 4
  def bulk = arch_defvw / width{T}
  def V = [bulk]T
  
  def acc = sum_accumulator{A, unr, V}
  @for_mu{bulk,unr,mu_extra{acc}}(x in tup{V,xv}, M in 'm' over ia) {
    acc{'acc', M, x}
  }
  promote{i64, acc{'vec_result', ia}}
}

each{{w} => export{merge{'sum_small_i', fmtnat{w}}, sum_small{primtype{'i', w}}}, tup{8, 16, 32}}
