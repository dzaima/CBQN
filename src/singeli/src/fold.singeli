include './base'
include './mask'
include './spaced'
include './scan_common'
include './vecfold'

def opsh64{op}{v:[_](f64), ...perm} = op{v, shuf{v, ...perm}}
def mix{op, v:([4]f64) if hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 1,0}, 2,3,0,1} }
def mix{op, v:([2]f64) if hasarch{'X86_64'}} = opsh64{op}{v, 1,0}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T, op}(x:*T, len:u64) : T = {
  assert{len > 0}
  a := load{x, 0}
  @for (x over _ from 1 to len) a = op{a, x}
  a
}
fn fold_idem{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw{}/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4**2}
                     r = opsh64{op}{r, 4**1}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'si_fold_min_f64', fold_idem{f64,__min}}
export{'si_fold_max_f64', fold_idem{f64,__max}}

fn fold_assoc_0{T, op}(x:*T, len:u64) : T = {
  a:T = 0
  @for (x over len) a = op{a, x}
  a
}
fn fold_assoc_0{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw{}/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~mask_of_first{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'si_sum_f64', fold_assoc_0{f64,+}}


def extract_column_pow2{T, x0, r0, nv, k} = {
  def V = [arch_defvw{} / width{T}]T
  xv := *V~~x0
  @for (r in *V~~r0 over i to nv) {
    xs := each{load{xv, .}, iota{k}}
    def unzip0{w} = if (not hasarch{'X86_64'}) {
      unzip{..., 0} # Sane instruction set
    } else {
      if (w <= 16) {
        # Pack instructions
        m := make{V, - (iota{vcount{V}}%k == 0)}
        xs = each{&{m, .}, xs} # Mask off high bits
        def D = el_m{V}
        {a, b} => packs128{D~~a, D~~b}
      } else {
        # Two-vector shuffles
        # Could also be used for 1/2-byte with ending gap >= 4 bytes,
        # less instructions but it doesn't seem faster
        def c = 128/w
        def sh = shuf{[c]ty_f{w}, ., 2*iota{c} % c}
        {...ab} => sh{ab}
      }
    }
    if (not hasarch{'X86_64'} or T != u16 or hasarch{'SSE4.1'}) {
      r = tree_fold{unzip0{width{T}}, xs}
    } else {
      # No unsigned saturation: sign-extend then use unsigned
      def D = [4]i32
      def f = tree_fold{unzip0{32}, .}
      def proc{hx} = {
        ri := D~~f{hx}
        top := D**(1<<15); m := D**tail{16}
        (ri & m) | (D~~(ri&top == top) &~ m)
      }
      r = V~~packs128{each{proc, split{k/2, xs}}}
    }
    if (width{V} > 128) { # Lane axis wasn't packed, need to shuffle to bottom
      def tr{E,a, r} = shuf{[1<<a]E, r, tr_iota{shiftright{a-1, iota{a}}}}
      def lc = k > 4
      if (lc) r = tr{u64,2, r}
      r = tr{ty_u{128/k}, lb{k} + (not lc), r}
    }
    xv += k
  }
}

def extract_column_modperm{x0, r0, nv, l, el, vl} = {
  # Build modular permutations
  def V = [vl]u8
  def H = [vl/2]u16
  p2 := ctz{l}; l >>= p2    # Decompose into l<<p2 with odd l
  e  := p2 + promote{ux,el} # Absorb into element size for most computation
  l8 := cast_i{u8, l}
  li := cast_i{u8, l + 2 * ((l-1) + (l&2))}  # Inverse mod vl
  elo:= V**tail{u8, e}
  ie := iota{V} & elo
  kmul := make{H, 2*iota{vl/2}} &~ H~~elo
  def mu16{k} = {
    k16 := H ** k
    prd := shuf{V~~(kmul * k16), 0,0}
    if (e == 0) prd += V~~(k16 << 8)
    (prd & V**(vl-1)) + ie
  }
  si := mu16{l8}
  sii := mu16{li}
  def swap_ms = if (vl == 16) ({x}=>x) else {
    ms := (V**16 & sii) == (V**16 &~ iota{V})
    {x} => blend_hom{x, shuf{[4]u64, x, 2,3,0,1}, ms}
  }

  # Blend masks
  def mg = { # Iteration i should select where mg == V**i
    ss := (si < V**(l8<<e)) & (ie == V**0)
    vs := V**0xff - scan_assoc_id0{+}{ss}
    swap_ms{shuf{[16]u8, vs, sii}}
  }
  mgo := mg - V**(l8 & 3)
  mgm := (mgo - V**1) & V**3
  m4s := @collect (i to 3) mgm == V**i

  # Main loop
  def loop{output} = {
    xv := *V~~x0
    rv := *V~~r0
    # Each iteration handles l vectors from x
    @for (i to nv<<p2) {
      # 1 or 3 initial vectors
      r := load{xv,0}; ++xv
      if ((l & 2) != 0) {
        def {m0, _, m2} = m4s
        re := blend_hom{load{xv,0}, load{xv,1}, m2}
        r = blend_hom{re, r, m0}
        xv += 2
      }
      # Then the rest in groups of 4
      mh := mgo
      @for (l/4) {
        {l0, ...ls} := each{load{xv,.}, iota{4}}
        r4 := fold{blend_hom, l0, ls, m4s}
        r = blend_hom{r, r4, mh < V**4}
        mh -= V**4; xv += 4
      }
      def write{r} = {
        store{rv, 0, shuf{[16]u8, r, si}}
        ++rv
      }
      output{r, i, write}
    }
  }

  # Handle odd and even strides separately
  if (p2 == 0) {
    loop{{r, i, write} => write{swap_ms{r}}}
  } else {
    # Store results
    def add_res = {
      ra := V**0  # Accumulator
      i  := make{V, iota{vl}%16}
      o  := V**(u8~~1<<el)
      bl := i & elo < o
      sh := i - o
      {r} => { ra = blend_hom{shuf{[16]u8, ra, sh}, r, bl} }
    }
    il := make{V, iota{vl} % 16}
    # Shuffle to undo interleaving of add_res
    def __shr{x:(V), sh if hasarch{'X86_64'}} = V~~(H~~x >> sh)
    def __shl{x:(V), sh if hasarch{'X86_64'}} = V~~(H~~x << sh)
    def __shr{x:(V), sh:T if hasarch{'AARCH64'} and not vect{T}} = x << V**cast_i{u8,-sh}
    def __shl{x:(V), sh:T if hasarch{'AARCH64'} and not vect{T}} = x << V**cast_i{u8, sh}
    def uz_lane = {
      l  := V**tail{u8, el}
      h  := V**(16 - u8~~16>>p2)
      dz := (il & l) | (h &~ il)>>(4-e) # low, high->middle
      dz |= (il &~ (l | h))<<p2         # middle->high
      shuf{[16]u8, ., dz}
    }
    # Adjust modular permutation to apply after unzipping
    si = uz_lane{(si & V**(vl - u8~~1<<e)) >> p2}
    si ^= il &~ V**((16 - u8~~1<<e) >> p2)
    # Cross-lane follow-up
    def cross = if (vl == 16) { {x}=>x } else {
      si = shuf{[4]u64, si, 0,1,0,1}
      assert{p2 <= 2}
      cr            := make{[8]u32, tr_iota{0,2,1}}
      if (p2 > 1) cr = make{[8]u32, tr_iota{2,0,1}}
      shuf{[8]u32, ., cr}
    }
    # Run, writing every 1<<p2 steps
    plo := tail{usz, p2}
    loop{{r, i, write} => {
      def ra = add_res{r}
      if ((plo &~ i) == 0) write{cross{uz_lane{ra}}}
    }}
  }
}

# Select one element out of every l, element width 1<<el bytes
# Maximum of n result values, return actual number written
fn extract_column(x0:*void, r0:*void, n:usz, l:usz, el:u8) : usz = {
  n <<= el
  def vl = arch_defvw{} / 8
  def thr = __min{vl+2, 20}
  if ((not has_simd) or n < vl or l > usz~~thr>>el or l<<el >= thr) return{0}
  nv := n / vl
  if (has_simd and is_pow2{l}) {
    def try_unzip{T, k} = if (k < thr and l == k) {
      extract_column_pow2{T, x0, r0, nv, k}
      goto{'ret'}
    }
    # 10 loops: i8 2,4,8,16; i16 2,4,8; i32 2,4; i64 2
    @unroll (ek to 4) if (el == ek) {
      def T = ty_u{8<<ek}
      @unroll (p from 1 to 5-ek) try_unzip{T, 1<<p}
    }
    return{0}
    setlabel{'ret'}
  } else if (hasarch{'SSE4.1'} or hasarch{'AARCH64'}) {
    extract_column_modperm{x0, r0, nv, l, el, vl}
  } else return{0}
  (usz~~vl>>el) * nv
}


# Short-row boolean folds: main challenge is bit packing
def fold_rows_bit_lt64{
  op, run_loop2, run_loop4, pext_res, mult_in,
  off,    # mask offset for generic methods
  xx, rx, # input and output xor for cases not specialized to individual functions
  rxs,    # output xor only, where and/or are specialized
  xp, rp, n, l
} = {
  nw := cdiv{n*l,64}
  def loop_T{T, proc_word} = {
    @for (x in xp, r in *T~~rp over nw) r = cast_i{T, rxs{proc_word{x}}}
  }
  if (l == 2) {
    def extract = if (fast_BMI2{}) pext{., u64~~64w2b01} else {
      # repeated shift-or-mask
      def step{x, {m, sh}} = { def a=x&m; a|a>>sh }
      def ss = 1<<iota{5}
      def ms = tail{64} / (1 + 1<<ss)
      fold{step, ., each{tup, ms, ss}}
    }
    run_loop2{{op} => loop_T{u32, {x} => extract{op{x, x>>1}}}}
  } else if (l == 4) {
    m:u64 = 64w2b0001; t := m<<3
    def extract = if (fast_BMI2{}) pext{., t} else {
      {x} => {
        a:= x & t
        a = (a * 4r2b001) & 64w0xf000
        a = (a * 4r0x001) >> (64-16)
      }
    }
    run_loop4{m, t, {get} => loop_T{u16, {x} => extract{get{x}}}}
  } else { # generic width<64
    {e0, d} := unaligned_spaced_mask_mod{l}
    el:= e0 << (l-1)                        # ending bit of each row
    e := if (same{off,-1}) el else e0<<off  # or selected bit
    c:u64 = 0; c|0  # carry, use depends on algorithm (unused for select)
    def {write_bits, flush_bits} = {
      r:u64 = 0
      rh := *u32~~rp
      ri:ux = 0
      def write{rb, n_bits} = {
        r |= rb << ri
        ri += promote{ux, n_bits}
        if (ri >= 32) {
          store{rh, 0, cast_i{u32,rx{r}}}; ++rh;
          r >>= 32; ri -= 32
        }
      }
      def flush{} = if (ri > 0) store{rh, 0, cast_i{u32,rx{r}}}
      tup{write, flush}
    }
    def finish_step{...a} = {
      write_bits{...a}
      e = e>>d | e<<(l-d) # update end mask for next iteration
    }
    if (fast_BMI2{}) {
      @for (x in xp over nw) finish_step{pext_res{xx{x}, e, c}, popc{e}}
    } else {
      # Emulate pext with 1, 2, or 3 multiply/mask steps.
      # To move size-a groups spaced at distance b together,
      # the multiplier has up to b/a bits spaced by b-a.
      dm:= cast_i{usz, popc{el}}    # minimum output bits per word
      dm-= promote{usz, is_pow2{l}} # for divisors of 64, e0 effectively overflows; subtract 1 to correct
      def loop{...par} = {
        et:= e0 << clz{e0}
        @for (xo in xp over nw) {
          x:= xx{xo}
          def {s, mod_rb} = if (length{select{par,0}} == 3) {
            # fast path for l==3
            s:= op{op{x, x<<1}, op{x<<2, c}}
            c = op{x>>63, x>>62}
            tup{s, {rb}=>rb}
          } else {
            mult_in{x, e, c}
          }
          nb:= dm + promote{usz, e>=e0} # = popc{e}
          # the multiply-mask sequence (last "mask" is the shift by (64 - nb))
          def extract = match {
            {       {q},         {}} => (et &~ (et - (s & e))) * q
            {{...qs, q}, {...bs, b}} => (extract{qs, bs} & b) * q
          }
          rb:= extract{...par} >> (64 - nb)
          finish_step{mod_rb{rb}, nb}
        }
      }
      if (l == 3) {
        mult0:u64 = base{1<< 2, 3**1};  top3:u64 = base{1<<9,  8**(1<<3-1)}>>2
        mult1:u64 = base{1<< 6, 3**1};  top9:u64 = base{1<<27, 3**(1<<9-1)}<<1
        mult2:u64 = base{1<<18, 3**1}
        loop{tup{mult0,mult1,mult2}, tup{top3, top9}}
      } else if (l < 8) {
        assert{l > 4}
        ld:= l-1; lld:= l*ld
        {mult0, _} := unaligned_spaced_mask_mod{ld}
        mult0 &= tail{u64, lld}
        {mult1, _} := unaligned_spaced_mask_mod{lld}
        ll:= l*l
        {tk, tkd} := unaligned_spaced_mask_mod{ll}; tk <<= tkd
        topk := tk - tk>>l; topk|= topk<<ll | topk>>ll
        loop{tup{mult0,mult1}, tup{topk}}
      } else {
        {mult, _} := unaligned_spaced_mask_mod{l-1}
        if (l==8) mult &= tail{7*8}
        loop{tup{mult}, tup{}}
      }
    }
    flush_bits{}
  }
}

fn extract_column_bit_lt64(xp:*u64, rp:*u64, n:usz, l:usz, o:usz) : void = {
  assert{l < 64}; assert{o < l} # Row length, and offset within row
  def run_loop2{loop} = loop{{a,b} => a>>o}
  def run_loop4{m, t, loop} = loop{{x} => x<<(l-1-o)}
  def pext_res{x, e, c} = pext{x, e}
  def mult_in{x, e, c} = tup{x, {r}=>r}
  def id{x} = x
  fold_rows_bit_lt64{
    {a,b}=>a, run_loop2, run_loop4, pext_res, mult_in, o, id, id, id,
    xp, rp, n, l
  }
}

fn xor_words(init:u64, x:*u64, l:usz):u64 = {
  @for (x over l) init ^= x
  init
}
def bit_output{rp:*T} = {
  buf:u64 = 0                    # Buffer for result bits
  def output{i, bit, mod} = {
    buf = buf>>1 | promote{u64, bit}<<63
    if ((i+1)%64==0) { store{rp, 0, mod{buf}}; ++rp }
  }
  def fixbuf{mod} = { buf = mod{buf} }
  def flush_bits{n, mod} = {
    q:=(-n)%64; if (q!=0) store{rp, 0, mod{buf} >> q}
  }
  def flush_bits{n} = flush_bits{n, {b}=>b}
  tup{output, fixbuf, flush_bits}
}
# word and alignment of start of next row
def next_start{i, m} = {
  bn := promote{u64, i+1} * promote{u64, m}
  tup{bn/64, bn%64}
}
fn xor_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, eq:u1) : void = {
  def p64 = promote{u64, .}
  rx:= -(p64{eq} &~ p64{l})  # ne to eq conversion
  if (l <= 64 and not (l%8==0 and l>16)) {
    def run_loop2{loop} = loop{^}
    def run_loop4{m, t, loop} = loop{{x} => { x2:= x^(x<<1); x2^(x2<<2) }}
    def xor_word = prefix_byshift{^, <<}
    def pext_in{xo, e, c} = {
      x := xor_word{xo}
      rb:= x ^ (x<<l | c)
      c  = (x ^ -(x>>63))>>(64-l)
      rb
    }
    def pext_res{x, e, c} = pext{pext_in{x, e, c}, e}
    def mult_in{x, e, c} = tup{pext_in{x, e, c}, {r}=>r}
    fold_rows_bit_lt64{
      ^, run_loop2, run_loop4, pext_res, mult_in, -1, {x}=>x, ^{rx,.}, ^{rx,.},
      xp, rp, n, l
    }
  } else {
    def fixout = ^{rx, .}
    def {add_bit, _, flush_bits} = bit_output{rp}
    def add_bit{i, bit} = add_bit{i, bit, fixout}
    def xor_loop{len} = {
      o:u64 = 0  # Carry
      j:u64 = 0; @for (i to n) {
        def {jn, sh} = next_start{i, l}
        s := xor_words(o, xp + j, len)
        e := load{xp, jn}
        s ^= e & (if (not same{len,1}) -p64{jn >= j + p64{len}} else j - jn)
        o  = e >> sh
        add_bit{i, popc{s ^ o}}
        j = jn+1
      }
    }
    ll := l/64
    if (l <= 64) {
      bm:= u64~~2<<(l-1) - 1
      k:= l/8
      @for (i to n) add_bit{i, popc{bm & loadu{u64, *u8~~xp + k*i}}}
    } else if (l < 128) xor_loop{1}
    else if (l%64==0) {
      @for (i to n) add_bit{i, popc{xor_words(0, xp+ll*i, ll)}}
    }
    else xor_loop{ll}
    flush_bits{n, fixout}
  }
}

fn or_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, op_and:u1) : void = {
  def {add_bit, set_out, flush_bits} = bit_output{rp}
  if (l < 64) {
    def run_loop2{loop} = if (op_and) loop{&} else loop{|}
    def run_loop4{m, t, loop} = {
      if (op_and) loop{{x} => x & ((x&~t) + m)}
      else        loop{{x} => x | ((x| t) - m)}
    }
    def xor_word = prefix_byshift{^, <<}
    def pext_in{x, e, c} = {
      m := e<<1 | 1
      t := e | 1<<63
      tup{x | ((x|t) - m), t}
    }
    def pext_res{x, e, c} = pext{...pext_in{x, e, c}}
    def mult_in{x, e, c} = {
      {s, t} := pext_in{x, e, c}
      cs:= c; c = (s&~e) >> 63
      tup{s, {rb}=>rb|cs}
    }
    def xx = ^{-promote{u64, op_and}, .}
    fold_rows_bit_lt64{
      |, run_loop2, run_loop4, pext_res, mult_in, -1, xx, xx, {r}=>r,
      xp, rp, n, l
    }
    return{}
  } else if (l < 128) {
    c:u64 = (promote{u64, l}-1) &- op_and # a row gives 1 if its sum is >c
    o:u64 = 0
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      s := o + popc{load{xp,j}}
      e := load{xp,jn}
      s += popc{e & (j - jn)}  # mask is 0 if j==jn, or -1
      o  = popc{e >> sh}
      add_bit{i, s > c+o, {rw}=>rw}
      j = jn+1
    }
  } else {
    rx := -promote{u64, op_and}
    def fixout = ^{rx, .}
    o:u64 = 0  # Saved bits
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      e := load{xp,jn} ^ rx
      l := ~(u64~~0) << sh
      rb:u64 = 1
      if ((o | (e &~ l)) == 0) { # Search for shortcut
        @for (i from j to jn) if (load{xp,i} != rx) goto{'found'}
        rb = 0; setlabel{'found'}
      }
      o = e & l
      add_bit{i, rb, fixout}
      j = jn+1
    }
    set_out{fixout}
  }
  flush_bits{n}
}
export{'si_xor_rows_bit', xor_rows_bit}
export{'si_or_rows_bit', or_rows_bit}
export{'si_select_cells_bit_lt64', extract_column_bit_lt64}
export{'si_select_cells_byte', extract_column}


fn sum_small{T}(xv:*void, ia:usz) : i64 = {
  xv:= *T~~xv
  def A = if (width{T}<=16) i32 else i64
  r:A = 0
  @for (xv over ia) r += promote{A, xv}
  promote{i64, r}
}

include './accumulator'
fn sum_small{T if has_simd}(xv:*void, ia:usz) : i64 = {
  def A = if (width{T}<=16) i32 else i64
  xv:= *T~~xv
  def unr = 4
  def bulk = arch_defvw{} / width{T}
  def V = [bulk]T
  
  def acc = sum_accumulator{A, unr, V}
  @for_mu{bulk,unr,mu_extra{acc}}(x in tup{V,xv}, M in 'm' over ia) {
    acc{'acc', M, x}
  }
  promote{i64, acc{'vec_result', ia}}
}

each{{w} => export{merge{'sum_small_i', fmtnat{w}}, sum_small{primtype{'i', w}}}, tup{8, 16, 32}}


# Insert
def assoc_num_reg = 6
def insert_assoc_reg{T, op, id:T, x0:(*void), len:(usz), wid:(usz)} = {
  def n = assoc_num_reg
  def vl = arch_defvw{} / width{T}
  def V = [vl]T
  total := len*wid             # Total elements
  set := len; step := total; numr:usz = 0
  if (total > n*vl) {
    set  = n*vl / wid          # Rows in one set of vectors
    assert{set > 0}
    step = set * wid           # Elements in one set
    numr = cdiv{total-n*vl, step} * step # Elements handled by main loop
  }

  # rv: masked load of up to n vectors starting at end (rem elements)
  x := *T~~x0; end := x + numr
  rem := total - numr
  rv  := undef{V, n}
  idv := V**id
  # Last index and corresponding masked vector
  li  := (rem-1)/vl
  last:= blend_hom{idv, load{*V~~end, li}, mask_of_first{V, rem-li*vl}}
  def id_labels = @collect (n) makelabel{}
  each{each{., iota{n}, rv, id_labels}, tup{
    # Read or use last vector
    {i, v, lbl} => {
      if (i<n-1 and i<li) { v = load{*V~~end, i} }
      else                { v = last; goto{lbl} }
    },
    # Set to identity
    {i,v,lbl} => { if (i>0) v=idv; setlabel{lbl} }
  }}

  while (x < end) {
    rv = each{{v,i} => op{v, load{*V~~x, i}}, rv, iota{n}}
    x += step
  }

  insert_assoc_finish{op, idv, rv, set, wid*(width{T}/8)}
}

def insert_assoc_finish{op, idv:V=[_]T, rv, set, ws} = {
  # Reduction on shape set,wid using scratch space
  # Assumes idempotent op{}
  def n = length{rv}
  mem := undefined{u8, (n+1)*width{V} / 8}
  def on_first{l, f} = each{f, iota{l}, slice{rv,0,l}}
  def store_vec = store{*V~~mem, ., .}

  on_first{n, store_vec}
  if (set > 1) {
    nr:= set              # Number of active rows
    def nv = cdiv{2*n, 3} # Number of vectors to be updated
    while (nr > 1) {
      k:= nr / 2; nr-= k
      def load_hi = load{*V~~(mem + ws*k), .}
      on_first{nv, {i,a} => store_vec{i, a = op{a, load_hi{i}}}}
    }
  }
  mem  # Caller copies
}

def insert_generic{op, rev, r:*T, x0:*T, len, wid} = {
  x := x0; if (rev) x += wid*(len-1)
  s := x  # Source; don't copy initial row
  @for (len - 1) {
    if (not rev) x += wid else x -= wid
    @for (r, s, x over wid) r = (if (rev) op{x, s} else op{s, x})
    s = r
  }
}

# Break into columns of 4 vectors and maybe one of 2; a c-vector column is composed of two potentially-overlapping c/2-vector parts, each fully in-bounds
def iter_columns_4{vl, wid, on_col} = {
  def run_col{c, j, o} = { # c: column count; j: first element offset; o: additional offset of second half, giving o + (c/2)*vl total elements
    def h = c/2; def ih = iota{h}
    assert{o <= h*vl}
    def offset{   p:*E} = { def pj =      p + j ; tup{pj, pj + o} } # Start pointers to the c/2-vector parts
    def offset{T, p:*E} = { def pj = *T~~(p + j); tup{pj, pj + o} } # Used with width{T}==width{E}/2 for interleaved data
    def each_i{V, f, p, ...a} = {
      def pis = flat_table{tup, each{~~{*V,.},p}, ih}
      each{{pi,...v} => f{...pi,...v}, pis, ...a}
    }
    on_col{offset, each_i, {} => h*vl + o}
  }
  j:usz = 0; o:usz = 2*vl
  def break = makelabel{}; while (1) {
    rem := wid - j
    if (rem < 5*vl) {  # 5*vl is full 4 plus squashed 2
      if (rem <= 2*vl) {
        if (rem > 0) run_col{2, j, rem - vl}
        goto{break}
      }
      o = (rem-1)%(2*vl) + 1  # Leave a full 2 if rem > 4*vl
    }
    run_col{4, j, o}
    j += o + 2*vl
  }
  setlabel{break}
}
# Accumulates from s..s+wid to r..r+wid with rows from x0..xend
def fold_columns_4{inc,lt, op,V=[vl]_, s,x0,xend,r, wid} = {
  iter_columns_4{vl,wid, {offset, each_i_g, _} => {
    # Reduce one (multi-vector) column
    def each_i = each_i_g{V, ...}
    rv := each_i{load, offset{s}}
    x := offset{x0}
    do {
      rv = each{op, each_i{load, x}, rv}
      each{inc{.,wid}, x}
    } while (lt{select{x,0}, xend})
    each_i{store, offset{r}, rv}
  }}
}

fn insert_assoc{T, op, id0}(r0:*void, x0:*void, len:usz, wid:usz) : void = {
  assert{len > 1}
  if (not has_simd) {
    insert_generic{op, 0, *T~~r0, *T~~x0, len, wid}
  } else {
    def vl = arch_defvw{} / width{T}
    def V = [vl]T
    if (wid <= assoc_num_reg * vl) {
      def wb = wid*(width{T}/8)
      match (insert_assoc_reg{T, op, T~~id0, x0, len, wid}) {
        {mem:*T} => emit{void, 'memcpy', r0, mem, wb}
      }
    } else {
      r := *T~~r0; s := *T~~x0; x := s + wid
      final_end := s + len*wid
      def b = 32  # Maximum rows at a time, for cache reuse between columns
      end := s + (((len-2) & (b-1)) + 2)*wid
      while (1) {
        fold_columns_4{+=,<, op,V, s,x,end,r, wid}
        if (end == final_end) return{}
        s = r
        x = end; end += b*wid
      }
    }
  }
}

fn insert_non_assoc{T, op}(rp:*void, xp:*void, len:usz, wid:usz) : void = {
  assert{len > 1}
  r := *T~~rp; x0 := *T~~xp
  if (not has_simd) {
    insert_generic{op, 1, r, x0, len, wid}
  } else {
    def vl = arch_defvw{} / width{T}; def V = [vl]T
    s := x0 + (len-1)*wid
    if (wid <= vl) {
      x := s
      rv := load{*V~~x}; x -= wid
      while (x >= x0) { rv = op{rv, load{*V~~x}}; x -= wid }
      store_blended_hom{*V~~r, mask_of_first{V, wid}, rv}
    } else {
      x := s - wid; end := x0
      # Group rows only if there will be multiple columns
      def b = 32; if (wid > 4*vl) end += ((len-2) &~ (b-1))*wid
      while (1) {
        fold_columns_4{-=,>=, op,V, s,x,end,r, wid}
        if (end == x0) return{}
        s = r
        x = end - wid
        end -= b*wid
      }
    }
  }
}

# Integer addition: result type is twice as wide as argument type
def mzip{a:V, b:V if w256{V}} = {
  def z = mzip128{a, b}
  each{vec_select{128, z, .}, table{+, iota{2}, tup{0,2}}}
}
def insert_generic{op, 0, r:*R, x0:*T, len, wid if width{R}==2*width{T}} = {
  f := x0
  x := x0 + wid
  def pr = promote{R, .}
  @for (r, f, x over wid) r = op{pr{f}, pr{x}}
  @for (len - 2) {
    x += wid
    @for (r, x over wid) r = op{r, pr{x}}
  }
}
def insert_add_wide{op, V=[vl]T, r:*R, x0:*T, len, wid} = {
  s := x0; xc := s + wid
  final_end := x0 + len*wid
  def b = 32
  end := x0 + (((len-2) & (b-1)) + 2)*wid
  def {exp, corr} = if (width{T} == 8) {
    def exp{v} = mzip128{v, V~~(v < V**0)}
    def uz = if (hasarch{'X86_64'}) unzip128 else unzip
    def ec{T,vs} = each{~~{T,.}, vs}
    def corr{vs} = ec{V, uz{...ec{ty_u{V}, vs}}}
    tup{exp, corr}
  } else {
    def sh = width{T}/2
    def m = V**(1<<sh - 1)
    def exp{v} = tup{v, v>>sh}
    def corr{{lo,hi}} = tup{lo, (hi + m&(lo>>sh - hi)) >> sh}
    tup{exp, corr}
  }
  # To avoid interleaving after each row, we treat r as *T
  # but stripe results as (lo, hi) in columns
  ds:usz = 0  # Whether s is striped
  while (1) {
    iter_columns_4{vl,wid, {offset, each_i_g, get_cw} => {
      def each_i = each_i_g{V, ...}
      def cw = get_cw{}; def hi_off{p} = *V~~(*T~~p + cw)
      def load_exp{...a} = exp{load{...a}}
      # Fold using expanded form
      rv := each_i{load_exp, offset{s}}; s += ds & cw
      x := offset{xc}
      do {
        rv = each{each{op,...}, rv, each_i{load_exp, x}}
        each{+={.,wid}, x}
      } while (select{x,0} < end)
      # Convert hi from overlapping with lo to separate
      rvc := each{corr, rv}
      # Add existing high part for row groups other than the first
      # Can't interleave this with stores, as the pointers may overlap
      if (ds!=0) {
        def load_add{p,i,{_,hi}} = { hi += load{hi_off{p}, i} }
        each_i{load_add, offset{T, r}, rvc}
      }
      if (end != final_end) {
        # Write both halves, type T (correct high half for lo signedness)
        def st_h{p,i,{lo,hi}} = {
          store{p, i, lo}
          store{hi_off{p}, i, hi - V~~(lo < V**0)}
        }
        each_i{st_h, offset{T, r}, rvc}
      } else {
        # Interleave to final format, type R
        def st_z{p,i,lh} = each{store{p, ...}, 2*i+iota{2}, mzip{...lh}}
        each_i_g{re_el{R,V}, st_z, offset{r}, rvc}
      }
    }}
    if (end == final_end) goto{'done'}
    s = *T~~r; ds = maxvalue{usz}
    xc = end; end += b*wid
  }
  setlabel{'done'}
}
fn insert_add_widen{T, R}(r0:*void, x0:*void, len:usz, wid:usz) : void = {
  assert{len > 1}
  def op = +
  def args = tup{*R~~r0, *T~~x0, len, wid}
  if (not has_simd) {
    insert_generic{op, 0, ...args}
  } else {
    def vl = arch_defvw{} / width{T}
    if (wid < 4*vl) {
      insert_generic{op, 0, ...args}
    } else {
      def I = w_d{T}
      def {r,..._} = args; ri := *I~~r0
      insert_add_wide{op, [vl]T, ri, ...slice{args,1}}
      if (R != I) @for (r, ri over wid) r = cast_i{R, ri}
    }
  }
}

def minvalue{(f64)} = -1/0
def maxvalue{(f64)} =  1/0
export_tab{'si_insert_minmax', flat_table{
  {{op, mv}, T} => insert_assoc{T, op, mv{T}},
  tup{tup{__min,maxvalue}, tup{__max,minvalue}}, tup{i8,i16,i32,f64}
}}
export_tab{'si_insert_add_widen',
  each{insert_add_widen, tup{i8,i16,i32}, tup{i16,i32,f64}}
}
export{'si_insert_add_f64', insert_non_assoc{f64, +}}
