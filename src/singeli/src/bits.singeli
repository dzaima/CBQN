include './base'
include './f64'
include './cbqnDefs'
if (hasarch{'X86_64'}) {
  include './sse3'
  include './avx'
  include './avx2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './bitops'
include './mask'

def bitsel{VL, T, r, bits, e0, e1, len} = {
  def bulk = VL/width{T}
  def VT = [bulk]T
  
  e0v:= VT**e0
  e1v:= VT**e1
  
  maskedLoop{bulk, len, {i, M} => {
    cb:= loadBatchBit{VT, bits, i}
    storeBatch{r, i, homBlend{e0v, e1v, cb}, M}
  }}
}

bitsel_i{VL,T}(r:*void, bits:*u64, e0:u64, e1:u64, len:u64) : void = {
  bitsel{VL, T, *T~~r, bits, trunc{T,e0}, trunc{T,e1}, len}
}

def table{w} = each{{T} => bitsel_i{w, T}, tup{u8, u16, u32, u64}}

def tup_bitsel = table{arch_defvw}; v_bitsel:*type{tupsel{0,tup_bitsel}} = tup_bitsel; 'simd_bitsel'=v_bitsel