include './base'
include './cbqnDefs'
include './f64'
include './bitops'
include './mask'
include './bitalign'

def bitsel{VL, T, r, bits, e0, e1, len} = {
  def bulk = VL/width{T}
  def VT = [bulk]T
  
  e0v:= VT**e0
  e1v:= VT**e1
  @maskedLoop{bulk}(r in tup{'g',r}, b in tup{'b',VT,bits} over i to len) r{homBlend{e0v, e1v, b}}
}

fn bitsel_i{VL,T}(r:*void, bits:*u64, e0:u64, e1:u64, len:u64) : void = {
  bitsel{VL, T, *T~~r, bits, trunc{T,e0}, trunc{T,e1}, len}
}

def table{w} = each{bitsel_i{w, .}, tup{u8, u16, u32, u64}}

exportT{'simd_bitsel', table{arch_defvw}}

(if (has_sel) {
  fn bitwiden_n_8(src:*void, dst:*void, csz:ux, cam:ux) : void = {
    assert{cam>0}
    assert{(csz>1) & (csz<8)}
    def bulk = arch_defvw / 8
    def V = [bulk]u8
    bitalign{tup{2,8,csz}, 8, {s, align} => {
      @maskedLoop{bulk}(dst in tup{V,*u8~~dst} over cam) {
        dst = align{load{*V~~src}}
        ptr_add{u8, src, bulk*s/8}
      }
    }}
  }
  export{'si_bitwiden_n_8', bitwiden_n_8}
})

(if (hasarch{'AARCH64'}) {
  fn bitnarrow_8_n(src:*void, dst:*void, csz:ux, cam:ux) : void = {
    assert{cam>0}
    assert{(csz>1) & (csz<8)}
    def bulk = arch_defvw / 8
    def V = [bulk]u8
    dstC:= *u8~~dst
    dstE:= *u8~~dst + cdiv{csz*cam, 8}
    bitalign{8, tup{2,8,csz}, {s, align} => {
      def get{} = align{load{*V~~src}}
      def next{} = {
        ptr_add{u8, src, bulk}
        ptr_add{u8, dstC, bulk*s/8}
      }
      while (dstC+bulk < dstE) {
        store{*V~~dstC, 0, get{}}
        next{}
      }
      while (dstC < dstE) {
        storeBatch{dstC, 0, get{}, maskAfter{dstE - dstC}}
        next{}
      }
    }}
  }
  export{'si_bitnarrow_8_n', bitnarrow_8_n}
})