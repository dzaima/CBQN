include './base'
include './cbqnDefs'
include './f64'
include './bitops'
include './mask'
include './bitalign'
def arch_defvw{if hasarch{'AVX512BW'}} = 512

def mask_unr{bulk} = __max{2, 8/bulk}

def bitsel{VL, T, r, bits, e0, e1, len} = {
  def bulk = VL/width{T}
  def VT = [bulk]T
  
  e0v:= VT**e0
  e1v:= VT**e1
  @for_mu{bulk,mask_unr{bulk}}(r in tup{'g',r}, b in tup{'b',VT,bits} over i to len) r{eachx{blend_hom, e0v, e1v, b}}
}

fn bitsel_i{VL,T}(r:*void, bits:*u64, e0:u64, e1:u64, len:u64) : void = {
  bitsel{VL, T, *T~~r, bits, trunc{T,e0}, trunc{T,e1}, len}
}

def table{w} = each{bitsel_i{w, .}, tup{u8, u16, u32, u64}}

export_tab{'simd_bitsel', table{arch_defvw{}}}

fn blend_arr_scalar{E}(rp:*void, zero:*void, one0:u64, mask:*void, len:u64) : void = {
  if (same{E,'!'}) {
    fatal{'bad blend'}
  } else if (E==u1) {
    emit{void, 'blendArrScalarBits', rp, zero, cast_i{E, one0}, mask, len}
  } else {
    def bulk = arch_defvw{} / width{E}
    def VT = [bulk]E
    def one = VT**cast_i{E, one0}
    @for_mu{bulk,mask_unr{bulk}}(r in tup{'g',*E~~rp}, zero in tup{VT,*E~~zero}, mask in tup{'b',VT,mask} over i to len) r{eachx{blend_hom, zero, one, mask}}
  }
}

export_tab{'si_blend_arr_scalar', each{blend_arr_scalar, tup{u1, '!', '!', u8, u16, u32, u64}}}

(if (has_sel) {
  fn bitwiden_n_8(src:*void, dst:*void, csz:ux, cam:ux) : void = {
    assert{cam>0}
    assert{(csz>1) & (csz<8)}
    def bulk = arch_defvw{} / 8
    def V = [bulk]u8
    bitalign{tup{2,8,csz}, 8, {s, align} => {
      @for_masked{bulk}(dst in tup{V,*u8~~dst} over cam) {
        dst = align{load{*V~~src}}
        ptr_add{u8, src, bulk*s/8}
      }
    }}
  }
  export{'si_bitwiden_n_8', bitwiden_n_8}
})

(if (hasarch{'AARCH64'}) {
  fn bitnarrow_8_n(src:*void, dst:*void, csz:ux, cam:ux) : void = {
    assert{cam>0}
    assert{(csz>1) & (csz<8)}
    def bulk = arch_defvw{} / 8
    def V = [bulk]u8
    dstC:= *u8~~dst
    dstE:= *u8~~dst + cdiv{csz*cam, 8}
    bitalign{8, tup{2,8,csz}, {s, align} => {
      def get{} = align{load{*V~~src}}
      def next{} = {
        ptr_add{u8, src, bulk}
        ptr_add{u8, dstC, bulk*s/8}
      }
      while (dstC+bulk < dstE) {
        store{*V~~dstC, 0, get{}}
        next{}
      }
      while (dstC < dstE) {
        store_narrow{dstC, 0, get{}, mask_first{dstE - dstC}}
        next{}
      }
    }}
  }
  export{'si_bitnarrow_8_n', bitnarrow_8_n}
})