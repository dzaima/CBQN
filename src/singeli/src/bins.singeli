include './base'
include './cbqnDefs'
include './mask'
def arch_defvw{if hasarch{'AVX512BW'}} = 512

def for_dir{up} = if (up) for else for_backwards

def for_vec_overlap{vl}{vars,begin==0,n,iter} = {
  assert{n >= vl}
  def end = makelabel{}
  j:u64 = 0
  while (1) {
    iter{j, vars}
    j += vl
    if (j > n-vl) { if (j == n) goto{end}; j = n-vl }
  }
  setlabel{end}
}

# Shift as u16, since x86 is missing 8-bit shifts
def shr16{v:V, n} = V~~(re_el{u16, v} >> n)

# Copy half of a vector to the other half
def dup_half{vec,half} = shuf{[4]u64, vec, 2*half+tup{0,1,0,1}}

# Forward or backwards in-place max-scan
# Assumes a whole number of vectors and minimum 0
include './scan_common'
fn max_scan{T, up}(x:*T, len:u64) : void = {
  if (hasarch{'X86_64'}) {
    def op = __max
    def pre = make_scan_idem{T, op, up}
    def vl = (if (hasarch{'AVX2'}) 256 else 128)/width{T}
    def V = [vl]T
    p := V**0
    @for_dir{up} (v in *V~~x over len/vl) {
      v = op{pre{v}, p}
      p = broadcast_last{v, up}
    }
  } else {
    m:T=0; @for_dir{up} (x over len) { if (x > m) m = x; x = m }
  }
}

def getsel{...x} = assert{'shuffling not supported', show{...x}}
if_inline (hasarch{'AVX2'}) {
  def getsel{h:H=[16]T if width{T}==8} = {
    shuf{H, [32]T**h, .}
  }
  def getsel{v:V=[vl==32]T if width{T}==8} = {
    def H = n_h{V}
    vtop := V**(vl/2)
    hs := each{dup_half{v, .}, tup{1,0}}
    {i} => blend_hom{...each{shuf{H,.,i}, hs}, V~~i<vtop}
  }
  def getsel{v:V=[8]T if width{T}==32} = shuf{V, v, .}
}

# Move evens to half 0 and odds to half 1
def uninterleave{x:V=[vl]T if hasarch{'AVX2'}} = {
  def bytes = width{T}/8
  def i = 2*iota{vl/4}
  def i2= flat_table{+, bytes*merge{i,i+1}, iota{bytes}}
  t := shuf{[16]u8, x, i2}
  shuf{[4]u64, t, 0,2,1,3}
}

def rtypes = tup{i8, i16, i32, f64}
# Return index of smallest possible result type given max result value
# (Unused; done in C for now)
def get_rtype{len} = {
  t:u8 = 0
  def c{T, ...ts} = if (len>maxvalue{T}) { ++t; c{...ts} }
  def c{T==f64} = {}
  c{...rtypes}
  t
}
def rtype_arr{gen} = {
  def t = each{gen, rtypes}
  a:*(type{select{t,0}}) = t
}

# Write the last index of v at t+v, for each unique v in w
fn write_indices{I,T}(t:*I, w:*T, n:u64) : void = {
  def break = makelabel{}
  i:u64 = 0; while (1) {
    d:u64 = 16
    id := i+d
    wi := undefined{T}
    if (id >= n) {
      @for (w over j from i to n) store{t, w, cast_i{I, j+1}}
      goto{break}
    } else if ((wi = load{w, i}) == load{w, id}) {
      # Gallop
      md := n - i
      d2 := undefined{u64}
      while ((d2=d+d) < md and wi == load{w, i + d2}) d = d2
      i += d
      l := n - i; if (l > d) l = d
      # Last instance of wi in [i,i+l); shrink l
      while (l > 8) {
        h := l/2
        m := i + h
        if (wi == load{w, m}) i = m
        l -= h
      }
    } else {
      @unroll (j to 8) store{t, load{w, i+j}, cast_i{I, i+j+1}}
      i += 8
    }
  }
  setlabel{break}
}
fn write_indices{I,T if width{I}==8}(t:*I, w:*T, n:u64) : void = {
  @for (w over j to n) store{t, w, cast_i{I, j+1}}
}
def bins_lookup{I, T, up, w:*T, wn:(u64), x:*T, xn:(u64), rp:(*void)} = {
  # Build table
  def tc = 1<<width{T}
  t0:*I = talloc{I, tc}
  @for (t0 over tc) t0 = 0
  t:*I = t0 + tc/2
  write_indices{I,T}(t, *T~~w, wn)
  # Vector bit-table
  def use_vectab = if (hasarch{'AVX2'} and I==i8 and T==i8) 1 else 0
  def done = makelabel{}
  if (use_vectab) bins_vectab_i8{up, w, wn, x, xn, rp, t0, t, done}
  # Main scalar table
  max_scan{I, up}(t0, tc)
  @for (r in *I~~rp, x over xn) r = load{t, x}
  if (use_vectab) setlabel{done}
  tfree{t0}
}

def bins_vectab_i8{up, w, wn, x, xn, rp, t0, t, done if hasarch{'AVX2'}} = {
  assert{wn < 128} # Total must fit in i8
  def vl = 32
  def T = i8
  def V = [vl]T; def H = n_h{V}
  def U = [vl]u8

  # Convert to bit table
  def no_bittab = makelabel{}
  def nb = 256/vl
  nu:u8 = 0; def addu{b} = { nu+=cast_i{u8,popc{b}}; b } # Number of uniques
  vb := U~~make{[nb](ty_u{vl}),
    @collect (t in *V~~t0 over nb) addu{hom_to_int{t > V**0}}
  }
  dup := promote{u64,nu} < wn
  # Unique index to w index conversion
  ui := undefined{V}; ui1 := undefined{V}; ui2 := each{undefined,tup{V,V}}
  if (dup) {
    def maxu = 2*vl
    if (nu > maxu) goto{no_bittab}
    # We'll subtract 1 when indexing so the initial 0 isn't needed
    tui:*i8 = copy{maxu, 0}; i:T = 0
    @for (tui over promote{u64,nu}) { i = load{t, load{w, i}}; tui = i }
    def tv = load{*V~~tui, .}
    ui = tv{0}
    if (nu > 16) ui1 = dup_half{ui, 1}
    ui = dup_half{ui, 0}
    if (nu > vl) ui2 = each{dup_half{tv{1}, .}, tup{0,1}}
  }
  # Popcount on 8-bit values
  def sums{n} = if (n==1) tup{0} else { def s=sums{n/2}; merge{s,s+1} }
  def sum4 = getsel{make{H, sums{vl/2}}}
  bot4 := U**0x0f
  def vpopc{v} = {
    def s{b} = sum4{b&bot4}
    s{shr16{v,4}} + s{v}
  }
  # Bit table
  def swap{v} = shuf{[4]u64, v, 2,3,0,1} # For signedness
  def sel_b = getsel{swap{vb}}
  # Masks for filtering bit table
  def ms = if (up) 256-(1<<(1+iota{8})) else (1<<iota{8})-1
  def sel_m = getsel{make{H, merge{ms - 256*(ms>127), 8**0}}}
  # Exact values for multiples of 8
  store{t0, vpopc{vb}}
  st:i8=0; @for_dir{up} (t0 over 256/8) { st += t0; t0 = st }
  def sel_c = getsel{swap{load{V, t0} - V**dup}}
  # Top 5 bits select bytes from tables; bottom 3 select from mask
  bot3 := U**0x07
  @for_vec_overlap{vl} (j to xn) {
    xv := U~~load{V, x+j}
    xb := xv & bot3
    xt := shr16{xv &~ bot3, 3}
    ind := sel_c{xt} - vpopc{sel_b{xt} & U~~sel_m{xb}}
    if (dup) {
      i0 := V~~ind # Can contain -1
      def isel{u} = shuf{H, u, i0}
      ind = isel{ui}
      if (nu > 16) {
        b := V~~(re_el{u16, i0} << (7 - lb{vl/2}))
        ind = blend_top{ind, isel{ui1}, b}
        if (nu > 32) ind = blend_hom{blend_top{...each{isel,ui2}, b}, ind, i0 < V**vl}
      }
    }
    store{*T~~rp+j, ind}
  }
  goto{done}
  setlabel{no_bittab}
}

# Binary search within vector registers
def bin_search_vec{prim, T, w:*T, wn, x:*T, xn, rp, maxwn if hasarch{'AVX2'}} = {
  def up = prim != '⍒'
  def search = (prim=='∊') | (prim=='⊐')
  assert{wn > 1}; assert{wn < maxwn}
  def wd = width{T}
  def I = if (wd<32) u8 else u32; def wi = width{I}
  def lanes = hasarch{'AVX2'} & (I==u8)
  def isub = wd/wi; def bb = base{1<<wi, .}
  def vl = 256/wd; def svl = vl>>lanes
  def V = [vl]T
  def U = [vl](ty_u{T})
  def lt = if (up) <; else >
  # Number of steps
  log := ceil_log2{wn+1}
  gap := 1<<log - wn
  # Fill with minimum value at the beginning
  def pre = if (search) load{w} else (if (up) minvalue else maxvalue){T}
  wg := *V~~(w-gap)
  wv0:= blend_hom{load{wg}, V**pre, mask_of_first{V,gap}}
  # For multiple lanes, interleave like transpose
  def maxstep = lb{maxwn}
  def lstep = lb{svl}
  def ex = maxstep - lstep
  wv := if (lanes) wv0 else tup{wv0,wv0}
  wv2 := wv # Compiler complains if uninitialized
  if (ex>=1 and wn >= svl) {
    --gap # Allows subtracting < instead of adding <=
    def un = uninterleave
    def tr_half{a, b} = each{vec_select{128,tup{a,b},.}, tup{tup{0,2}, tup{1,3}}}
    def un{{a,b}} = tr_half{un{a},un{b}}
    if (not lanes) select{wv,1} = load{wg, 1}
    wv = un{wv}
    if (ex>=2 and wn >= 2*svl) {
      assert{lanes} # Different transpose pattern needed
      gap -= 2
      tup{wv, wv2} = each{un, tr_half{wv, un{load{wg, 1}}}}
    }
  }
  def ms{v}{h} = getsel{re_el{I, if (lanes) half{v,h} else select{v,h}}}
  def selw = ms{wv}{0}; def selw1 = if (ex>=1) ms{wv}{1} else 'undef'
  def selw2 = if (ex>=2) each{ms{wv2}, iota{2}} else 'undef'
  # Offset at end
  off := U~~V**cast_i{i8, gap-(1-search)}
  # Midpoint bits for each step
  def lowbits = bb{copy{isub,isub}}
  bits := each{{j} => U**(lowbits << j), iota{lstep}}
  # Unroll sizes up to a full lane, handling extra lanes conditionally
  # in the largest one
  @unroll (klog from 2 to __min{maxstep,lstep}+1) {
    def last = klog==lstep
    def this = if (not last) log==klog else log>=klog
    if (this) @for_vec_overlap{vl} (j to xn) {
      xv:= load{V, x+j}
      s := U**bb{iota{isub}}  # Select sequential bytes within each U
      def cmpx{cmp}{se,ind} = cmp{xv, V~~se{re_el{I,ind}}}
      def ltx = cmpx{lt}; def eqx = cmpx{==}
      @unroll (j to klog) {
        m := s | select{bits,klog-1-j}
        s = blend_hom{m, s, ltx{selw, m}}
      }
      r := if (isub==1) s else s>>(lb{isub}+wd-wi)
      # b records if xv was found; c is added to the index
      def r_out = prim!='∊'
      def get_up{var,cmpx,use}{set,...a} = if (use) set{var, cmpx{...a}}
      b := undefined{U}; def up_b = get_up{b, eqx, search}
      c := undefined{U}; def up_c = get_up{c, ltx, r_out}
      up_b{=, selw, s}
      # Extra selection lanes
      if (last and ex>=1 and log>=klog+1) {
        r += r
        def up_bc{set,se} = { up_b{|=,se,s}; up_c{set,se,s} }
        up_bc{=,selw1}
        if (ex>=2 and log>=klog+2) {
          r += r
          each{up_bc{+=, .}, selw2}
        }
        if (r_out) r += c
      }
      if (r_out) {
        r -= off
        if (prim=='⊐') r = blend_hom{U**cast_i{u8,wn}, r, b}
        store_narrow_relaxed{*u8~~rp+j, r}
      } else {
        def B = ty_u{vl}; out := cast_i{B, hom_to_int{b}}
        store{*B~~rp, cdiv{j,vl}, out>>((-j)%vl)}
      }
    }
  }
}

(if (hasarch{'AVX2'}) {
  fn avx2_search_bin{prim, T, maxwn}(rp:*(if (prim=='∊') u64 else i8), w:*void, wn:u64, x:*void, xn:u64) : void = {
    bin_search_vec{prim, T, *T~~w, wn, *T~~x, xn, rp, maxwn}
  }
  export_tab{
    'avx2_member_sort',
    each{avx2_search_bin{'∊',.,.}, tup{i16,i32}, tup{32,16}}
  }
  export_tab{
    'avx2_indexOf_sort',
    each{avx2_search_bin{'⊐',.,.}, tup{i8,i16,i32}, tup{64,16,16}}
  }
})

def unroll_sizes = tup{4,1}
fn write{T,k}(r:*void, i:u64, ...vs:k**u64) : void = {
  each{{j,v} => store{*T~~r, i+j, cast_i{T,v}}, iota{k}, vs}
}
def wr_arrs = each{{k} => rtype_arr{{T} => write{T,k}}, unroll_sizes}

def bin_search_branchless{up, w, wn, x, n, res, rtype} = {
  def lt = if (up) <; else >
  ws := w - 1
  l0 := wn + 1
  # Take a list of indices in x/res to allow unrolling
  def search{inds} = {
    xs:= each{load{x,.}, inds} # Values
    ss:= each{{_}=>ws, inds}      # Initial lower bound
    l := l0; h := undefined{u64}  # Interval size l, same for all values
    while ((h=l/2) > 0) {
      # Branchless update
      def bin1{s, x, m} = { if (not lt{x, load{m}}) s = m }
      each{bin1, ss, xs, each{bind{+,h}, ss}}
      l -= h
    }
    each{{s} => u64~~(s - ws), ss}
  }
  # Unroll by 4 then 1
  def search{i, k} = search{each{bind{+,i}, iota{k}}}
  j:u64 = 0
  def searches{k, wr_arr} = {
    wr := load{wr_arr, rtype}
    while (j+k <= n) { wr(res, j, ...search{j, k}); j+=k }
  }
  each{searches, unroll_sizes, wr_arrs}
}

fn bins{T, up}(w:*void, wn:u64, x:*void, xn:u64, rp:*void, rty:u8) : void = {
  def param = tup{up, *T~~w, wn, *T~~x, xn, rp}
  def lookup{k} = {
    if (rty == k) bins_lookup{select{rtypes,k}, T, ...param}
    else if (k+1 < length{rtypes}) lookup{k+1}
  }
  # For >=8 i8 values, vector bit-table is as good as binary search
  def wn_vec = if (T==i8) 8 else 2*256/width{T}
  if (hasarch{'AVX2'} and T<=i32 and wn < wn_vec and xn >= 256/width{T}) {
    bin_search_vec{if (up) '⍋' else '⍒', T, ...slice{param,1}, wn_vec}
  # Lookup table threshold has to account for cost of
  # populating the table (proportional to wn until it's large), and
  # initializing the table (constant, much higher for i16)
  } else if (T==i8 and xn>=32 and (xn>=512 or xn >= wn>>6 + 32)) {
    lookup{0}
  } else if (T==i16 and xn>=512 and (xn>=1<<14 or xn >= wn>>6 + (u64~~3<<(12+rty))/(ceil_log2{wn}+2))) {
    lookup{0}
  } else {
    bin_search_branchless{...param, rty}
  }
}

export_tab{
  'si_bins',
  join{table{bins, tup{i8,i16,i32,f64}, tup{1,0}}}
}

# Utility for narrowing binary search right argument
include './f64'
require{'math.h'}
fn saturate{F,T,...up}(dst:*void, src:*void, n:u64) : void = {
  # Auto-vectorizes, although not that well for f64
  def a = minvalue{T}
  def b = maxvalue{T}
  @for (d in *T~~dst, xf in *F~~src over n) {
    x := if (F==f64) (if (select{up,0}) __floor else __ceil){xf} else xf
    d = cast_i{T, x}
    if (not x<b) d = b
    if (x<a) d = a
  }
}

export_tab{
  'si_saturate',
  each{{a}=>saturate{...a}, merge{
    tup{tup{i16,i8}, tup{i32,i8}, tup{i32,i16}},
    join{table{tup{f64, ...}, tup{i8,i16,i32}, tup{1,0}}}
  }}
}
