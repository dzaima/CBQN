include './base'
include './f64'
include './cbqnDefs'
if (hasarch{'X86_64'}) {
  include './sse3'
  include './avx'
  include './avx2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './bitops'
include './mask'
include 'util/tup'


def rootty{T & isprim{T}} = T
def rootty{T & isvec{T}} = eltype{T}

def is_s{X} = issigned{rootty{X}}
def is_u{X} = isunsigned{rootty{X}}

def ty_sc{O, R} = R # keep floats as-is
def ty_sc{O, R & is_s{O} & is_u{R}} = ty_s{R}
def ty_sc{O, R & is_u{O} & is_s{R}} = ty_u{R}

def bqn_or{a, b} = (a+b)-(a*b)


# + & -
def arithChk1{F==__add, M, w:T, x:T, r:T} = tup{'topAny', M{(w^r) & (x^r)}}
def arithChk1{F==__sub, M, w:T, x:T, r:T} = tup{'topAny', M{(w^x) & (w^r)}}
def arithChk1{F==__add, M, w:T, x:T, r:T & isvec{T} & tern{hasarch{'X86_64'}, elwidth{T}<=16, 1}} = tup{'anyne', __adds{w,x}, r}
def arithChk1{F==__sub, M, w:T, x:T, r:T & isvec{T} & tern{hasarch{'X86_64'}, elwidth{T}<=16, 1}} = tup{'anyne', __subs{w,x}, r}



def arithChk2{F, M, w:T, x:T & is_s{T} & (match{F,__add} | match{F,__sub})} = {
  r:= F{w,x}
  tup{r, arithChk1{F, M, w, x, r}}
}

# ×
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i8==eltype{T} & hasarch{'X86_64'}} = {
  def wp = unpackQ{w, T ~~ (T**0 > w)}
  def xp = unpackQ{x, T ~~ (T**0 > x)}
  def rp = each{__mul, wp, xp}
  def bad = each{{v} => [16]i16 ~~ ((v<<8)>>8 != v), rp}
  if (M{0}) { # masked check
    tup{packQ{rp}, tup{'homAny', M{packQ{bad}}}}
  } else { # unmasked check; can do check in a simpler way
    tup{packQ{rp}, tup{'homAny', tupsel{0,bad}|tupsel{1,bad}}}
  }
}
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i16==eltype{T} & hasarch{'X86_64'}} = {
  rl:= __mul  {w,x}
  rh:= __mulhi{w,x}
  tup{rl, tup{'anyne', rh, rl>>15}}
}
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i32==eltype{T} & hasarch{'X86_64'}} = {
  max:= [8]f32 ~~ [8]u32**0x4efffffe
  def cf32{x} = emit{[8]f32, '_mm256_cvtepi32_ps', x}
  f32mul:= cf32{w} * cf32{x}
  tup{w*x, tup{'homAny', M{abs{f32mul} >= max}}}
  # TODO fallback to the below if the above fails
  # TODO don't do this, but instead shuffle one half, do math, unshuffle that half
  # def wp = unpackQ{w, T**0}
  # def xp = unpackQ{x, T**0}
  # def rp = each{__mul32, wp, xp}
  # def T2 = to_el{i64, T}
  # def bad = each{{v} => {
  #   ((T2~~v + T2**0x80000000) ^ T2**(cast{i64,1}<<63)) > T2**cast_i{i64, (cast{u64,1}<<63) | 0xFFFFFFFF}
  # }, rp}
  # tup{packQQ{each{{v} => v & T2**0xFFFFFFFF, rp}}, tup{'homAny', tupsel{0,bad}|tupsel{1,bad}}} this doesn't use M
}

def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & hasarch{'AARCH64'}} = {
  def r12 = mul12{w, x}
  rl:= packLo{r12}
  rh:= packHi{r12}
  tup{rl, tup{'homAny', M{rh != (rl >> (elwidth{T}-1))}}}
}



def runner{u, R, F} = {
  def c = ~u
  
  def run{F, M, w, x} = { show{'todo', c, R, F, w, x}; emit{void,'__builtin_abort'}; w }
  
  def run{F, M, w:T, x:T & c & R!=u32} = {
    arithChk2{F, M, w, x}
  }
  
  def run{F, M, w, x & u} = tup{F{w, x}, tup{'none'}} # trivial base implementation
  
  def toggleTop{x:X} = x ^ X**(1<<(elwidth{X}-1))
  def run{F==__sub, M, w:VU, x:VU & is_u{VU}} = { # 'b'-'a'
    def VS = ty_s{VU}
    run{F, M, VS~~toggleTop{w}, VS~~toggleTop{x}}
  }
  def run{F, M, w:VU, x:VS & is_u{VU} & is_s{VS}} = { # 'a'+3, 'a'-3
    def r = run{F, M, VS~~toggleTop{w}, x}
    tup{toggleTop{VU~~tupsel{0,r}}, tupsel{1,r}}
  }
  def run{F==__add, M, w:VS, x:VU & is_s{VS} & is_u{VU}} = run{F, M, x, w} # 3+'a' → 'a'+3
  
  def run{F, M, w:VW, x:VX & c & R==u32 & (match{F,__add} | match{F,__sub})} = { # 'a'+1, 'a'-1
    r:= F{ty_u{w}, ty_u{x}}
    tup{to_el{R, VW}~~r, tup{'homAny', M{r > type{r}**1114111}}}
  }
  run
}

def runChecks_any{F, vals} = { F{tree_fold{|, each{{c}=>tupsel{1,c}, vals}}} }
def runChecks{type=='homAny', vals, M} = runChecks_any{homAny, vals}
def runChecks{type=='topAny', vals, M} = runChecks_any{topAny, vals}
def runChecks{type=='none',   vals, M} = 0
def runChecks{type=='anyne',  vals, M} = {
  def cols = flip{vals}
  def xs = tupsel{1, cols}
  def ys = tupsel{2, cols}
  if (tuplen{vals}==1) {
    anyne{...xs, ...ys, M}
  } else {
    assert{M{0} == 0}
    ~homAll{tree_fold{&, each{==, xs, ys}}}
  }
}

def arithProcess{F, run, overflow, M, is, cw, cx, TY} = {
  def r0 = flip{each{{w1, x1} => run{F, M, w1, x1}, cw, cx}}
  def values = tupsel{0, r0}
  def checks = tupsel{1, r0}
  def ctype = tupsel{0,tupsel{0,checks}}
  assert{tree_fold{&, each{{c}=>match{ctype, tupsel{0,c}}, checks}}}
  if (rare{runChecks{ctype, checks, M}}) overflow{tupsel{0, is}}
  each{{c} => TY~~c, values}
}

def arithAAimpl{vw, mode, F, W, X, R, w, x, r, len} = {
  # show{F, mode, W, X, R}
  if (R==u1) {
    def bulk = vw/64
    def TY = [bulk]u64
    maskedLoop{bulk, cdiv{len, 64}, {i, M} => {
      cw:= loadBatch{*u64~~w, i, TY}
      cx:= loadBatch{*u64~~x, i, TY}
      storeBatch{*u64~~r, i, F{cw, cx}, M}
    }}
  } else if (match{F,__mul} and W!=u1 and X==u1 and W==R) { # 0‿1‿1‿1‿1‿0‿1‿1×3‿1‿4‿1‿5‿9‿2‿6
    def bulk = vw / width{W}
    def TU = ty_u{R}
    def TV = [bulk]TU
    
    muLoop{bulk, 2, len, {is, M} => {
      def cw = loadBatch{*TU~~w, is, TV}
      def cx = loadBatchBit{TV, *u64~~x, is}
      storeBatch{*TU~~r, is, each{&, cw, cx}, M}
    }}
  } else {
    def bulk = vw / max{max{width{W}, width{X}}, width{R}}
    def overflow = tern{mode==1, {i}=>return{i}, tern{mode==2, {i}=>return{1}, 0}}
    def TY = [bulk]R
    
    def run = runner{match{overflow, 0}, R, F}
    
    def unr = tern{mode==0, 2, 1} # 2x unroll non-overflowing cases; suppresses clang's default unrolling, which unrolls a lot more; 2x appears to be plenty
    muLoop{bulk, unr, len, {is, M} => {
      def cw = loadBatch{*W~~w, is, ty_sc{W, TY}}
      def cx = loadBatch{*X~~x, is, ty_sc{X, TY}}
      storeBatch{*R~~r, is, arithProcess{F, run, overflow, M, is, cw, cx, TY}, M}
    }}
  }
}

fn arithAAc{vw, mode, F, W, X, R}(r:*void, w:*void, x:*void, len:u64) : u64 = {
  arithAAimpl{vw, mode, F, W, X, R, w, x, r, len}
  if (mode==1) len
  else 0
}
fn arithAAu{vw, mode, F, W, X, R}(r:*void, w:*void, x:*void, len:u64) : void = {
  arithAAimpl{vw, mode, F, W, X, R, w, x, r, len}
}

def arithAA{mode, F, W, X, R} = {
  def vw = arch_defvw
  if (mode==1 or mode==2) arithAAc{vw, mode, F, W, X, R}
  else arithAAu{vw, mode, F, W, X, R}
}

# mode: 0:overflow-checked, needed; 1:overflow-erroring; 2: overflow-checked, not needed
fn arithSAf{vw, mode, F, swap, W, X, R}(r:*void, w:u64, x:*void, len:u64) : u64 = {
  # show{F, swap, mode, W, X, R}
  assert{len>0}
  def bulk = vw / max{width{W}, width{R}}
  def TY = [bulk]R
  def overflow = tern{mode==1, {i}=>return{1}, {i}=>return{i}}
  
  def run = runner{(R==f64) | (mode==2), R, F}
  def getW{v} = trunc{W, v}
  def getW{v & W==f64} = interp_f64{v}
  cw:= ty_sc{W, TY}**getW{w}
  
  def unr = tern{mode==2, 2, 1} # same as in arithAAimpl
  muLoop{bulk, unr, len, {is, M} => {
    def cx = loadBatch{*X~~x, is, ty_sc{X, TY}}
    def cws = tuplen{is}**cw
    storeBatch{*R~~r, is, arithProcess{F, run, overflow, M, is, tern{swap,cx,cws}, tern{swap,cws,cx}, TY}, M}
  }}
  
  if (mode==1) 0
  else len
}

def arithSA{mode, F, swap, W, X, R} = arithSAf{arch_defvw, mode, F, swap, W, X, R}

fn andBytes{vw}(r: *u8, x: *u8, maskU64:u64, len:u64) : void = {
  assert{vw>=64}
  def bulk = vw / width{u8}
  def T8 = [bulk]u8
  def T64 = [bulk/8]u64
  maskFull:= T8~~T64**maskU64
  maskedLoop{bulk, len, {i, M} => {
    storeBatch{r, i, loadBatch{x, i, T8} & maskFull, M}
  }}
}

export{'simd_andBytes', andBytes{arch_defvw}}
export{'orSAc_f64_f64_f64', arithSA{2,bqn_or,0,f64,f64,f64}}
include 'gen/arDefs'
