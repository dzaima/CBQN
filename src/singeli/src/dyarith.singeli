include './base'
include './f64'
include './cbqnDefs'
include './sse3'
include './avx'
include './avx2'
include './bitops'
include './mask'


def rootty{T & isprim{T}} = T
def rootty{T & isvec{T}} = eltype{T}

def is_s{X} = issigned{rootty{X}}
def is_u{X} = isunsigned{rootty{X}}

def ty_sc{O, R} = R # keep floats as-is
def ty_sc{O, R & is_s{O} & is_u{R}} = ty_s{R}
def ty_sc{O, R & is_u{O} & is_s{R}} = ty_u{R}

def bqn_or{a, b} = (a+b)-(a*b)


# + & -
def arithChk1{F==__add, M, w:T, x:T, r:T} = topAny{M{(w^r) & (x^r)}}
def arithChk1{F==__sub, M, w:T, x:T, r:T} = topAny{M{(w^x) & (w^r)}}
def arithChk1{F==__add, M, w:T, x:T, r:T & isvec{T} & elwidth{T}<=16} = anyne{__adds{w,x}, r, M}
def arithChk1{F==__sub, M, w:T, x:T, r:T & isvec{T} & elwidth{T}<=16} = anyne{__subs{w,x}, r, M}



def arithChk2{F, M, w:T, x:T & is_s{T} & (match{F,__add} | match{F,__sub})} = {
  r:= F{w,x}
  tup{r, arithChk1{F, M, w, x, r}}
}

# ×
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i8==eltype{T}} = {
  def wp = unpackQ{w, T ~~ (T**0 > w)}
  def xp = unpackQ{x, T ~~ (T**0 > x)}
  def rp = each{__mul, wp, xp}
  def bad = each{{v} => [16]i16 ~~ ((v<<8)>>8 != v), rp}
  if (M{0}) { # masked check
    tup{packQ{rp}, homAny{M{packQ{bad}}}}
  } else { # unmasked check; can do check in a simpler way
    tup{packQ{rp}, homAny{tupsel{0,bad}|tupsel{1,bad}}}
  }
}
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i16==eltype{T}} = {
  rl:= __mul  {w,x}
  rh:= __mulhi{w,x}
  tup{rl, anyne{rh, rl>>15, M}}
}
def arithChk2{F, M, w:T, x:T & match{F,__mul} & isvec{T} & i32==eltype{T}} = {
  max:= [8]f32 ~~ [8]u32**0x4efffffe
  def cf32{x} = emit{[8]f32, '_mm256_cvtepi32_ps', x}
  f32mul:= cf32{w} * cf32{x}
  tup{w*x, homAny{M{abs{f32mul} >= max}}}
  # TODO fallback to the below if the above fails
  # TODO don't do this, but instead shuffle one half, do math, unshuffle that half
  # def wp = unpackQ{w, T**0}
  # def xp = unpackQ{x, T**0}
  # def rp = each{__mul32, wp, xp}
  # def T2 = to_el{i64, T}
  # def bad = each{{v} => {
  #   ((T2~~v + T2**0x80000000) ^ T2**(cast{i64,1}<<63)) > T2**cast_i{i64, (cast{u64,1}<<63) | 0xFFFFFFFF}
  # }, rp}
  # tup{packQQ{each{{v} => v & T2**0xFFFFFFFF, rp}}, homAny{tupsel{0,bad}|tupsel{1,bad}}} this doesn't use M
}



def runner{u, R, F} = {
  def c = ~u
  
  def run{F, OO, M, w, x} = { show{'todo', c, R, F, w, x}; emit{void,'__builtin_abort'}; w }
  
  def run{F, OO, M, w:T, x:T & c & R!=u32} = {
    def r2 = arithChk2{F, M, w, x}
    if (rare{tupsel{1,r2}}) OO{}
    tupsel{0,r2}
  }
  
  def run{F, OO, M, w, x & u} = F{w, x} # trivial base implementation
  
  def toggleTop{x:X} = x ^ X**(1<<(elwidth{X}-1))
  def run{F==__sub, OO, M, w:VU, x:VU & is_u{VU}} = { # 'b'-'a'
    def VS = ty_s{VU}
    run{F, OO, M, VS~~toggleTop{w}, VS~~toggleTop{x}}
  }
  def run{F, OO, M, w:VU, x:VS & is_u{VU} & is_s{VS}} = { # 'a'+3, 'a'-3
    toggleTop{VU~~run{F, OO, M, VS~~toggleTop{w}, x}}
  }
  def run{F==__add, OO, M, w:VS, x:VU & is_s{VS} & is_u{VU}} = run{F, OO, M, x, w} # 3+'a' → 'a'+3
  
  def run{F, OO, M, w:VW, x:VX & c & R==u32 & (match{F,__add} | match{F,__sub})} = { # 'a'+1, 'a'-1
    r:= F{ty_u{w}, ty_u{x}}
    if (homAny{M{r > type{r}**1114111}}) OO{}
    to_el{R, VW}~~r
  }
  run
}


def arithAAimpl{vw, mode, F, W, X, R, w, x, r, len} = {
  # show{F, mode, W, X, R}
  if (R==u1) {
    def bulk = vw/64
    def TY = [bulk]u64
    maskedLoop{bulk, cdiv{len, 64}, {i, M} => {
      cw:= loadBatch{*u64~~w, i, TY}
      cx:= loadBatch{*u64~~x, i, TY}
      storeBatch{*u64~~r, i, F{cw, cx}, M}
    }}
  } else if (match{F,__mul} and W!=u1 and X==u1 and W==R) { # 0‿1‿1‿1‿1‿0‿1‿1×3‿1‿4‿1‿5‿9‿2‿6
    def bulk = vw / width{W}
    def TU = ty_u{R}
    def TV = [bulk]TU
    
    maskedLoop{bulk, len, {i, M} => {
      cw:= loadBatch{*TU~~w, i, TV}
      cx:= loadBatchBit{TV, *u64~~x, i}
      storeBatch{*TU~~r, i, cw&cx, M}
    }}
  } else {
    def bulk = vw / max{max{width{W}, width{X}}, width{R}}
    def overflow = tern{mode==1, {i}=>return{i}, tern{mode==2, {i}=>return{1}, 0}}
    def TY = [bulk]R
    
    def run = runner{match{overflow, 0}, R, F}
    
    maskedLoop{bulk, len, {i, M} => {
      cw:= loadBatch{*W~~w, i, ty_sc{W, TY}}
      cx:= loadBatch{*X~~x, i, ty_sc{X, TY}}
      storeBatch{*R~~r, i, TY~~run{F, {} => overflow{i}, M, cw, cx}, M}
    }}
  }
}

arithAAc{vw, mode, F, W, X, R}(r:*void, w:*void, x:*void, len:u64) : u64 = {
  arithAAimpl{vw, mode, F, W, X, R, w, x, r, len}
  if (mode==1) len
  else 0
}
arithAAu{vw, mode, F, W, X, R}(r:*void, w:*void, x:*void, len:u64) : void = {
  arithAAimpl{vw, mode, F, W, X, R, w, x, r, len}
}

def arithAA{mode, F, W, X, R} = {
  def vw = 256
  if (mode==1 or mode==2) arithAAc{vw, mode, F, W, X, R}
  else arithAAu{vw, mode, F, W, X, R}
}

# mode: 0:overflow-checked, needed; 1:overflow-erroring; 2: overflow-checked, not needed
arithSAf{vw, mode, F, swap, W, X, R}(r:*void, w:u64, x:*void, len:u64) : u64 = {
  # show{F, swap, mode, W, X, R}
  assert{len>0}
  def bulk = vw / max{width{W}, width{R}}
  def TY = [bulk]R
  def overflow = tern{mode==1, {i}=>return{1}, {i}=>return{i}}
  
  def run = runner{(R==f64) | (mode==2), R, F}
  def getW{v} = trunc{W, v}
  def getW{v & W==f64} = interp_f64{v}
  cw:= ty_sc{W, TY}**getW{w}
  
  maskedLoop{bulk, len, {i, M} => {
    cx:= loadBatch{*X~~x, i, ty_sc{X, TY}}
    storeBatch{*R~~r, i, TY~~run{F, {} => overflow{i}, M, tern{swap,cx,cw}, tern{swap,cw,cx}}, M}
  }}
  
  if (mode==1) 0
  else len
}

def arithSA{mode, F, swap, W, X, R} = arithSAf{256, mode, F, swap, W, X, R}

andBytes{vw}(r: *u8, x: *u8, maskU64:u64, len:u64) : void = {
  assert{vw>=64}
  def bulk = vw / width{u8}
  def T8 = [bulk]u8
  def T64 = [bulk/8]u64
  maskFull:= T8~~T64**maskU64
  maskedLoop{bulk, len, {i, M} => {
    storeBatch{r, i, loadBatch{x, i, T8} & maskFull, M}
  }}
}

'avx2_andBytes'=andBytes{256}
'orSAc_f64_f64_f64'=arithSA{2,bqn_or,0,f64,f64,f64}
include 'gen/arDefs'
