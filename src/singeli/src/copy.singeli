include './base'
include './sse3'
include './avx'
include './avx2'
include './mask'
include './cbqnDefs'
include './bitops'

def copyFromBits{T, xp: *u64, rp: *eltype{T}, l:u64} = {
  def bulk = vcount{T}
  def TU = ty_u{T}
  
  maskedLoop{bulk, l, {i, M} => {
    x:= loadBatchBit{TU, xp, i}
    r:= x & TU ~~ T**1
    storeBatch{rp, i, T~~r, M}
  }}
}


copy{vw, X, R}(x: *void, r: *void, l:u64, xRaw: *void) : void = {
  assert{l!=0}
  
  def bulk = vw/max{width{X}, width{R}}
  xp:= *tern{X==u1, u64, X} ~~ x
  rp:= *tern{R==u1, u64, R} ~~ r
  def XV = [bulk]X
  def RV = [bulk]R
  
  if (R==u64) {
    # show{'R==u64', X, R}
    assert{((X==u8) | (X==u16)) | (X==u32)}
    maskedLoop{bulk, l, {i, M} => { # TODO could maybe read 256 bits and use unpack to write >256
      v:= loadBatch{xp, i, RV}
      v|= RV ** (cbqn_c32Tag{} << 48)
      storeBatch{rp, i, v, M}
    }}
  } else if (X==u1 and R==u1) {
    # show{'u1u1', X, R}
    def V64 = [vw/64]u64
    maskedLoop{vcount{V64}, cdiv{l, 64}, {i, M} => {
      v:= loadBatch{xp, i, V64}
      storeBatch{rp, i, v, M}
    }}
  } else if (X==u1) {
    # show{'X==u1', X, R}
    copyFromBits{[bulk]R, *u64~~x, *R~~r, l}
  } else if (R==u1) {
    # show{'R==u1', X, R}
    def XU = ty_u{XV}
    @forNZ (i to cdiv{l,vcount{XV}}) {
      v:= loadBatch{xp, i, XV}
      r:= homMask{(XU~~v) == XU~~XV**1}
      b_setBatch{vcount{XV}, rp, i, r} # TODO something more special for f64
    }
  } else if (width{X}<=width{R}) {
    # show{'w{X}<=w{R}', X, R}
    maskedLoop{bulk, l, {i, M} => {
      v:= loadBatch{xp, i, RV}
      storeBatch{rp, i, v, M}
    }}
  } else {
    # show{'w{X}>w{R}', X, R}
    maskedLoop{bulk, l, {i, M} => {
      v:= loadBatch{xp, i, XV}
      storeBatch{rp, i, v, M}
    }}
  }
}

# avx2_copy_src_dst
# x→int & equal-width copies
'avx2_copy_1_1'     = copy{256, u1,  u1}
'avx2_copy_1_i8'    = copy{256, u1,  i8}
'avx2_copy_1_i16'   = copy{256, u1,  i16}
'avx2_copy_1_i32'   = copy{256, u1,  i32}

'avx2_copy_i8_1'    = copy{256, i8,  u1}
'avx2_copy_i8_i8', 'avx2_copy_c8_c8' = copy{256, i8,  i8}
'avx2_copy_i8_i16'  = copy{256, i8,  i16}
'avx2_copy_i8_i32'  = copy{256, i8,  i32}

'avx2_copy_i16_1'   = copy{256, i16, u1}
'avx2_copy_i16_i8'  = copy{256, i16, i8}
'avx2_copy_i16_i16', 'avx2_copy_c16_c16' = copy{256, i16, i16}
'avx2_copy_i16_i32' = copy{256, i16, i32}

'avx2_copy_i32_1'   = copy{256, i32, u1}
'avx2_copy_i32_i8'  = copy{256, i32, i8}
'avx2_copy_i32_i16' = copy{256, i32, i16}
'avx2_copy_i32_i32', 'avx2_copy_c32_c32'= copy{256, i32, i32}

'avx2_copy_f64_1'   = copy{256, f64, u1}
'avx2_copy_f64_i8'  = copy{256, f64, i8}
'avx2_copy_f64_i16' = copy{256, f64, i16}
'avx2_copy_f64_i32' = copy{256, f64, i32}

# x→f64, x→B (no avx2_copy_B_B because that may possibly need refcounting)
'avx2_copy_1_f64',   'avx2_copy_1_B'   = copy{256, u1,  f64}
'avx2_copy_i8_f64',  'avx2_copy_i8_B'  = copy{256, i8,  f64}
'avx2_copy_i16_f64', 'avx2_copy_i16_B' = copy{256, i16, f64}
'avx2_copy_i32_f64', 'avx2_copy_i32_B' = copy{256, i32, f64}
'avx2_copy_f64_f64', 'avx2_copy_f64_B' = copy{256, f64, f64}

# chr→x
'avx2_copy_c8_c16'  = copy{256, u8, u16}
'avx2_copy_c8_c32'  = copy{256, u8, u32}
'avx2_copy_c8_B'    = copy{256, u8, u64}

'avx2_copy_c16_c8'  = copy{256, u16, u8}
'avx2_copy_c16_c32' = copy{256, u16, u32}
'avx2_copy_c16_B'   = copy{256, u16, u64}

'avx2_copy_c32_c8'  = copy{256, u32, u8}
'avx2_copy_c32_c16' = copy{256, u32, u16}
'avx2_copy_c32_B'   = copy{256, u32, u64}

# B→chr
'avx2_copy_B_c8'  = copy{256, u64, u8}
'avx2_copy_B_c16' = copy{256, u64, u16}
'avx2_copy_B_c32' = copy{256, u64, u32}
