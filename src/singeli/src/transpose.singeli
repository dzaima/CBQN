include './base'
include './cbqnDefs'
include './f64'
include './mask'
include './bitops'

# Group l (power of 2) elements into paired groups of length o
# e.g. pairs{2, iota{8}} = {{0,1,4,5}, {2,3,6,7}}
def pairs{o, x} = {
  def i = iota{length{x}/2}
  def g = 2*i - i%o
  select{x, tup{g, g+o}}
}
def unpack_pass{o, x} = merge{...each{unpackQ, ...pairs{o, x}}}
def permute_pass{o, x} = {
  def p = pairs{o, x}
  def h{s} = each{{a,b}=>emit{[8]i32, '_mm256_permute2f128_si256', a,b,s}, ...p}
  merge{h{16b20}, h{16b31}}
}
def unpack_to{f, l, x} = {
  def pass = if (f) permute_pass else unpack_pass
  pass{l, if (l==1) x else unpack_to{0, l/2, x}}
}
# Last pass for square kernel packed in halves
def shuf_pass{x} = each{{v} => shuf{[4]i64, v, 4b3120}, x}

# Square kernel where width is a full vector
def transpose_square{VT, l, x if hasarch{'AVX2'}} = unpack_to{1, l/2, x}

def load2{a:*T, b:*T} = pair{load{a}, load{b}}
def store2{a:*T, b:*T, v:T2 if w128i{T} and w256{T2}} = {
  each{{p, i} => store{p, 0, T~~half{v,i}}, tup{a,b}, iota{2}}
}
def load_k {VT, src,    l, w if w256{VT}} = each{{i}  =>load {*VT~~(src+i*w), 0       }, iota{l}}
def store_k{VT, dst, x, l, h if w256{VT}} = each{{i,v}=>store{*VT~~(dst+i*h), 0, VT~~v}, iota{l}, x}
def load_k {VT, src,    l, w if w128{VT}} = each{{i}  =>{p:=src+  i*w; load2 {*VT~~p, *VT~~(p+l*w)   }}, iota{l}}
def store_k{VT, dst, x, l, h if w128{VT}} = each{{i,v}=>{p:=dst+2*i*h; store2{*VT~~p, *VT~~(p+  h), v}}, iota{l}, x}

# Transpose kernel of size kw,kh in size w,h array
def kernel{src:*T, dst:*T, kw, kh, w, h} = {
  def n = (kw*kh*width{T}) / 256             # Number of vectors
  def xvs = load_k{[kw]T, src, n, w}
  def xt  = unpack_to{n==kh, n/2, xvs}       # Transpose n by n
  def rvs = if (n==kw) xt else shuf_pass{xt} # To kh by kh for packed square
  store_k{[kh]T, dst, rvs, n, h}
}


def for_mult{k}{vars,begin,end,iter} = {
  assert{begin == 0}
  @for (i to end/k) iter{k*i, vars}
}
def for_mult_max{k, m}{vars,begin,end,iter} = {
  @for_mult{k} (i0 to end) {
    i:=i0; if (i>m) i = m
    iter{i, vars}
  }
}

def transpose_with_kernel{T, k, kh, call_base, rp:*T, xp:*T, w, h, ws, hs} = {
  def at{x,y} = tup{xp + y*ws + x, rp + x*hs + y}
  
  # Cache line info
  def line_bytes = 64
  def line_elts = line_bytes / (width{T}/8)

  def use_overlap{o} = o >= 2  # For overlapped SIMD instead of scalar
  wo := w%k
  # Effective width: number of columns read, counting overlap twice
  # Just use base transpose for short overhang; otherwise round up
  we := w; if (use_overlap{wo}) we += k - wo
  wm := w - k
  
  if (line_elts > 2*k or h&(line_elts-1) != 0 or h != hs) {
    ho := h%k
    # Effective height, like we for w
    he := h; if (use_overlap{ho}) he += k - ho
    def has_half = 2*k == kh
    if (has_half and he==kh and h<he) he = k  # Skip main loop; caught with he<h tests later
    # Main transpose
    @for_mult_max{kh, h-kh} (y to he) {
      @for_mult_max{k, wm} (x to we) {
        kernel{...at{x,y}, k, kh, ws, hs}
      }
    }
    # Half-row(s) for non-square i16 case
    if (has_half and (he & k) != 0) {
      n := 1 + cast_i{u64, he < h}  # 2 for overlapped halves
      e := h%kh; if (he<h or e<k) e = k
      @for (yi to n) {
        y:u64 = 0; if (yi == n-1) y = h - e
        @for_mult_max{k, wm} (x to we) {
          kernel{...at{x,y}, k, k, ws, hs}
        }
      }
    }
    # Base transpose used if overlap wasn't
    if (ho!=0 and he==h) { hd := h-ho; call_base{rp+hd, xp+ws*hd, w, ho} }
  } else {
    # Result rows are aligned with each other so it's possible to
    # write a full cache line at a time
    # This case is here to mitigate cache associativity problems at
    # at multiples of 256 or so, but it's faster whenever it applies
    assert{k == kh}
    def VT = [k]T
    def line_vecs = line_bytes / (width{VT}/8)
    def store_line{p:*T, vs} = each{{i,v} => store{p, i, T~~v}, iota{line_vecs}, vs}
    def get_lines{loadx} = {
      def vt{i} = transpose_square{VT, k, each{loadx, k*i + iota{k}}}
      each{tup, ...each{vt, iota{line_vecs}}}
    }
    ro := tail{6, -u64~~rp} / (width{T}/8)  # Offset to align within cache line; assume elt-aligned
    wh := ws*h
    yn := h
    if (ro != 0) {
      ra := line_elts - ro
      y := h - ra
      rpe := rp + y + (w-1)*hs  # Cache aligned
      # Part of first and last result row aren't covered by the split loop
      def trtail{dst, src, len} = @for (i to len) store{dst, i, load{src, ws*i}}
      trtail{rp, xp, ro}
      trtail{rpe, xp + y*ws + w-1, ra}
      # Transpose first few rows and last few rows together
      @for_mult_max{k, wm} (x to we) {
        {xpo,rpo} := at{x, y}
        o := ws*y + x
        def loadx{_} = {
          l:=load{*VT~~(xp+o)}
          o+=ws; if (o>wh-k) o -= wh-1  # Jump from last source row to first, shifting right 1
          l
        }
        def rls = get_lines{loadx}  # 4 rows of 2 vectors each
        each{{i,v} => {p:=rpo+i*hs; if (i<3 or p<rpe) store_line{*VT~~p, v}}, iota{k}, rls}
      }
      --yn  # One strip handled
    }
    @for_mult{line_elts} (y0 to yn) { y := y0 + ro
      @for_mult_max{k, wm} (x to we) {
        {xpo,rpo} := at{x, y}
        def rls = get_lines{{i} => load{*VT~~(xpo+i*ws), 0}}
        each{{i,v} => store_line{*VT~~(rpo+i*hs), v}, iota{k}, rls}
      }
    }
  }
  
  if (we==w) @for(wd from w-wo to w) {
    xpo:=xp+wd; rpo:=rp+hs*wd
    @for (i to h) store{rpo, i, load{xpo, ws*i}}
  }
}

# Interleave n values of type T from x0 and x1 into r
fn interleave{T}(r0:*void, x0:*void, x1:*void, n:u64) : void = {
  rp := *T~~r0
  @for (x0 in *T~~x0, x1 in *T~~x1 over i to n) {
    store{rp, i*2, x0}; store{rp, i*2+1, x1}
  }
}

fn transpose{T, k, kh}(r0:*void, x0:*void, w:u64, h:u64, ws:u64, hs:u64) : void = {
  # Scalar transpose defined in C
  def ts = if (T==i8) 'i8' else if (T==i16) 'i16' else if (T==i32) 'i32' else 'i64'
  def call_base{...a} = emit{void, merge{'transpose_',ts}, ...a, ws, hs}
  
  rp:*T = *T~~r0
  xp:*T = *T~~x0
  if (hasarch{'AVX2'} and w>=k and h>=k) {
    transpose_with_kernel{T, k, kh, call_base, rp, xp, w, h, ws, hs}
  } else {
    if      (h==2 and h==hs) interleave{T}(r0, x0, *void~~(xp+ws), w)
    else if (w==2 and w==ws) @for (r0 in rp, r1 in rp+hs over i to h) { r0 = load{xp, i*2}; r1 = load{xp, i*2+1} }
    else call_base{rp, xp, w, h}
  }
}

def transpose{T, k} = transpose{T, k, k}

exportT{'simd_transpose', tup{
  transpose{i8 , 16},
  transpose{i16, 8, 16},
  transpose{i32, 8},
  transpose{i64, 4}
}}

exportT{'si_interleave', each{interleave, tup{i8, i16, i32, i64}}}
