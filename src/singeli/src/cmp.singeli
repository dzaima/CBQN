include './base'
include './cbqnDefs'
include './f64'
include './avx'
include './avx2'
include './bitops'

def fmt{T} = 'unknown'
def fmt{T&match{T,__eq}}='='; def fmt{T&match{T,__ne}}='≠'
def fmt{T&match{T,__gt}}='>'; def fmt{T&match{T,__ge}}='≥'
def fmt{T&match{T,__lt}}='<'; def fmt{T&match{T,__le}}='≤'


def cif{v, G} = { show{'cif argument not known at compile time!'}; G{} }
def cif{v, G & match{v,0}} = 0
def cif{v, G & match{v,1}} = G{0}

def fillbits{dst:*u64, len:(Size), v   } = { emit{void, 'fillBits',    dst, len, v   }; return{}; }
def fillbits{dst:*u64, len:(Size), v, x} = { emit{void, 'fillBitsDec', dst, len, v, x}; return{}; }
def cmp_err{x} = { emit{void, 'cmp_err'}; return{}; }

cmpIX(dst:*u64, len:Size, x:u64, v:u1) : void = {
  nan:u1 = q_f64{x}
  if (~(nan | q_chr{x})) cmp_err{x}
  fillbits{dst, len, v & ~nan, x}
}

def eqne{op} = match{op,__eq}|match{op,__ne}

def pathAS{dst, len, T, op, x & issigned{T}} = {
  def R{f & eqne{op}} = {
    if (rare{floor{f}!=f}) fillbits{dst, len, op{0,1}, x} # also includes check for NaN/sNaN
    ftrunc{i64,f}
  }
  def R{f & match{op,__lt}|match{op,__ge}} = ftrunc{i64,ceil{f}}
  def R{f & match{op,__gt}|match{op,__le}} = ftrunc{i64,floor{f}}
  
  xf:f64 = interp_f64{x}
  xi64:i64 = R{xf}
  xT:T = trunc{T, xi64}
  if (rare{ext{i64, xT}!=xi64}) {
    cif{~eqne{op}, {_}=>{ # NaN was already checked for ≠/=
      if (isNaN{xf}) { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }}
    fillbits{dst, len, op{0,xf}}
  }
  xT
}

def pathAS{dst, len, T, op, x & T==f64} = {
  if (rare{~q_f64{x}}) {
    cif{~eqne{op}, {_}=>{ if(~q_chr{x}) cmp_err{x}; }}
    fillbits{dst, len, op{0,1}, x}
  }
  from_B{T,x}
}

def pathAS{dst, len, T, op, x & isunsigned{T}} = {
  if (rare{~q_chr{x}}) {
    cif{~eqne{op}, {_}=>{ if(~q_f64{x}) cmp_err{x}; }}
    fillbits{dst, len, op{1,0}, x}
  }
  xc32:u32 = from_B{u32,x}
  if (xc32 > maxvalue{T}) fillbits{dst, len, op{0,1}}
  trunc{T, xc32}
}



def any2bit{VT, unr, op, wS, wV, xS, xV, dst:*u64, len:(Size)} = {
  xi:Size = 0
  ri:Size = 0
  def bam = vcount{VT}*unr
  am:Size = cdiv{len,bam}
  assert{am>0}
  while (ri < am) {
    r:u64 = 0
    @unroll (j from 0 to unr) r = r | (ext{u64, getmask{op{wV{xi+j}, xV{xi+j}}}} << (j*vcount{VT}))
    b_set{bam, dst, ri, r}
    xi = xi+unr
    ri = ri+1
  }
}
aa2bit{VT, unr, op}(dst:*u64, wr:*u8, xr:*u8, len:Size) : void = {
  wv:= cast_vp{VT, wr}; ws:= cast_p{eltype{VT}, wr}
  xv:= cast_vp{VT, xr}; xs:= cast_p{eltype{VT}, xr}
  any2bit{VT, unr, op, {i}=>load{ws,i}, {i}=>load{wv,i}, {i}=>load{xs,i}, {i}=>load{xv,i}, dst, len}
}

as2bit{VT, unr, op}(dst:*u64, wr:*u8, x:u64, len:Size) : void = { # show{VT,unr,fmt{op}}
  wv:= cast_vp{VT, wr}; ws:= cast_p{eltype{VT}, wr}
  xv:= broadcast{VT, pathAS{dst, len, eltype{VT}, op, x}}
  any2bit{VT, unr, op, {i}=>load{ws,i}, {i}=>load{wv,i}, {i}=>x, {i}=>xv, dst, len}
}

bitAA{bitop}(dst:*u64, wr:*u8, xr:*u8, len:Size) : void = {
  ws:= cast_p{u64, wr}
  xs:= cast_p{u64, xr}
  @for (dst,ws,xs over _ from 0 to cdiv{len,64}) dst = bitop{ws,xs}
}

not(dst:*u64, x:*u64, len:Size) : void = { am:=cdiv{len,64}; assert{am>0}; @for (dst,x over _ from 0 to am) dst = ~x }
cpy(dst:*u64, x:*u64, len:Size) : void = { am:=cdiv{len,64}; assert{am>0}; @for (dst,x over _ from 0 to am) dst =  x }

bitAS{op}(dst:*u64, wr:*u8, x:u64, len:Size) : void = { # show{'bitAS'}
  xf:f64 = interp_f64{x}
  r0:u1 = op{0,xf}
  r1:u1 = op{1,xf}
  if (r0==r1) {
    if (rare{isNaN{xf}}) {
      if (eqne{op}) fillbits{dst, len, r0, x}
      else { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }
    fillbits{dst, len, r0}
    return{}
  }
  if (r0) call{not, dst, cast_p{u64,wr}, len}
  else call{cpy, dst, cast_p{u64,wr}, len}
}


# arr-arr =≠
'avx2_eqAA_i8','avx2_eqAA_u8'=aa2bit{[32]i8,1,__eq}; 'avx2_eqAA_i16','avx2_eqAA_u16'=aa2bit{[16]i16,1,__eq}; 'avx2_eqAA_i32','avx2_eqAA_u32'=aa2bit{[8]i32,1,__eq}
'avx2_neAA_i8','avx2_neAA_u8'=aa2bit{[32]i8,1,__ne}; 'avx2_neAA_i16','avx2_neAA_u16'=aa2bit{[16]i16,1,__ne}; 'avx2_neAA_i32','avx2_neAA_u32'=aa2bit{[8]i32,1,__ne}
'avx2_eqAA_u1'=bitAA{{a,b}=>a ^ ~b}; 'avx2_eqAA_f64'=aa2bit{[4]f64,2,__eq}
'avx2_neAA_u1'=bitAA{{a,b}=>a ^  b}; 'avx2_neAA_f64'=aa2bit{[4]f64,2,__ne}
# arr-arr >≥
'avx2_gtAA_i8'=aa2bit{[32]i8,1,__gt}; 'avx2_gtAA_i16'=aa2bit{[16]i16,1,__gt}; 'avx2_gtAA_i32','avx2_gtAA_u32'=aa2bit{[8]i32,1,__gt}; 'avx2_gtAA_f64'=aa2bit{[4]f64,2,__gt}
'avx2_geAA_i8'=aa2bit{[32]i8,1,__ge}; 'avx2_geAA_i16'=aa2bit{[16]i16,1,__ge}; 'avx2_geAA_i32','avx2_geAA_u32'=aa2bit{[8]i32,1,__ge}; 'avx2_geAA_f64'=aa2bit{[4]f64,2,__ge}
'avx2_gtAA_u8'=aa2bit{[32]u8,1,__gt}; 'avx2_gtAA_u16'=aa2bit{[16]u16,1,__gt}; 'avx2_gtAA_u1'=bitAA{{a,b}=>a & ~b}
'avx2_geAA_u8'=aa2bit{[32]u8,1,__ge}; 'avx2_geAA_u16'=aa2bit{[16]u16,1,__ge}; 'avx2_geAA_u1'=bitAA{{a,b}=>a | ~b}

# arr-scalar numeric
'avx2_eqAS_i8'=as2bit{[32]i8,1,__eq}; 'avx2_eqAS_i16'=as2bit{[16]i16,1,__eq}; 'avx2_eqAS_i32'=as2bit{[8]i32,1,__eq}; 'avx2_eqAS_f64'=as2bit{[4]f64,2,__eq}
'avx2_neAS_i8'=as2bit{[32]i8,1,__ne}; 'avx2_neAS_i16'=as2bit{[16]i16,1,__ne}; 'avx2_neAS_i32'=as2bit{[8]i32,1,__ne}; 'avx2_neAS_f64'=as2bit{[4]f64,2,__ne}
'avx2_gtAS_i8'=as2bit{[32]i8,1,__gt}; 'avx2_gtAS_i16'=as2bit{[16]i16,1,__gt}; 'avx2_gtAS_i32'=as2bit{[8]i32,1,__gt}; 'avx2_gtAS_f64'=as2bit{[4]f64,2,__gt}
'avx2_geAS_i8'=as2bit{[32]i8,1,__ge}; 'avx2_geAS_i16'=as2bit{[16]i16,1,__ge}; 'avx2_geAS_i32'=as2bit{[8]i32,1,__ge}; 'avx2_geAS_f64'=as2bit{[4]f64,2,__ge}
'avx2_ltAS_i8'=as2bit{[32]i8,1,__lt}; 'avx2_ltAS_i16'=as2bit{[16]i16,1,__lt}; 'avx2_ltAS_i32'=as2bit{[8]i32,1,__lt}; 'avx2_ltAS_f64'=as2bit{[4]f64,2,__lt}
'avx2_leAS_i8'=as2bit{[32]i8,1,__le}; 'avx2_leAS_i16'=as2bit{[16]i16,1,__le}; 'avx2_leAS_i32'=as2bit{[8]i32,1,__le}; 'avx2_leAS_f64'=as2bit{[4]f64,2,__le}
# arr-scalar character & bit
'avx2_eqAS_u8'=as2bit{[32]u8,1,__eq}; 'avx2_eqAS_u16'=as2bit{[16]u16,1,__eq}; 'avx2_eqAS_u32'=as2bit{[8]u32,1,__eq}; 'avx2_eqAS_u1'=bitAS{__eq}
'avx2_neAS_u8'=as2bit{[32]u8,1,__ne}; 'avx2_neAS_u16'=as2bit{[16]u16,1,__ne}; 'avx2_neAS_u32'=as2bit{[8]u32,1,__ne}; 'avx2_neAS_u1'=bitAS{__ne}
'avx2_gtAS_u8'=as2bit{[32]u8,1,__gt}; 'avx2_gtAS_u16'=as2bit{[16]u16,1,__gt}; 'avx2_gtAS_u32'=as2bit{[8]u32,1,__gt}; 'avx2_gtAS_u1'=bitAS{__gt}
'avx2_geAS_u8'=as2bit{[32]u8,1,__ge}; 'avx2_geAS_u16'=as2bit{[16]u16,1,__ge}; 'avx2_geAS_u32'=as2bit{[8]u32,1,__ge}; 'avx2_geAS_u1'=bitAS{__ge}
'avx2_ltAS_u8'=as2bit{[32]u8,1,__lt}; 'avx2_ltAS_u16'=as2bit{[16]u16,1,__lt}; 'avx2_ltAS_u32'=as2bit{[8]u32,1,__lt}; 'avx2_ltAS_u1'=bitAS{__lt}
'avx2_leAS_u8'=as2bit{[32]u8,1,__le}; 'avx2_leAS_u16'=as2bit{[16]u16,1,__le}; 'avx2_leAS_u32'=as2bit{[8]u32,1,__le}; 'avx2_leAS_u1'=bitAS{__le}
