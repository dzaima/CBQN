include './base'
include './cbqnDefs'
include './f64'
if (hasarch{'X86_64'}) {
  include './sse3'
  include './avx'
  include './avx2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './bitops'


def fillbits{dst:*u64, len:(Size), v   } = { emit{void, 'fillBits',    dst, len, v   }; return{}; }
def fillbits{dst:*u64, len:(Size), v, x} = { emit{void, 'fillBitsDec', dst, len, v, x}; return{}; }
def cmp_err{x} = { emit{void, 'cmp_err'}; return{}; }

cmpIX(dst:*u64, len:Size, x:u64, v:u1) : void = {
  nan:u1 = q_f64{x}
  if (~(nan | q_chr{x})) cmp_err{x}
  fillbits{dst, len, v & ~nan, x}
}

def eqne{op} = match{op,__eq}|match{op,__ne}

def pathAS{dst, len, T, op, x & issigned{T}} = {
  def R{f & eqne{op}} = {
    if (rare{floor{f}!=f}) fillbits{dst, len, op{0,1}, x} # also includes check for NaN/sNaN
    ftrunc{i64,f}
  }
  def R{f & match{op,__lt}|match{op,__ge}} = ftrunc{i64,ceil{f}}
  def R{f & match{op,__gt}|match{op,__le}} = ftrunc{i64,floor{f}}
  
  xf:f64 = interp_f64{x}
  xi64:i64 = R{xf}
  xT:T = trunc{T, xi64}
  def nanchk{} = {
    if(~eqne{op}) { # NaN was already checked for â‰ /=
      if (isNaN{xf}) { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }
  }
  if (~hasarch{'X86_64'}) nanchk{}
  if (rare{promote{i64, xT}!=xi64}) {
    if (hasarch{'X86_64'}) nanchk{}
    fillbits{dst, len, op{0,xf}}
  }
  xT
}

def pathAS{dst, len, T, op, x & T==f64} = {
  if (rare{~q_f64{x}}) {
    if (~eqne{op}) if (~q_chr{x}) cmp_err{x}
    fillbits{dst, len, op{0,1}, x}
  }
  from_B{T,x}
}

def pathAS{dst, len, T, op, x & isunsigned{T}} = {
  if (rare{~q_chr{x}}) {
    if (~eqne{op}) if (~q_f64{x}) cmp_err{x}
    fillbits{dst, len, op{1,0}, x}
  }
  xc32:u32 = from_B{u32,x}
  if (xc32 > maxvalue{T}) fillbits{dst, len, op{0,1}}
  trunc{T, xc32}
}



def any2bit{VT, unr, op, wS, wV, xS, xV, dst:*u64, len:(Size)} = {
  xi:Size = 0
  ri:Size = 0
  def bulk = vcount{VT}*unr
  am:Size = cdiv{len,bulk}
  assert{am>0}
  while (ri < am) {
    r:u64 = 0
    if (hasarch{'AARCH64'}) {
      def step{j, n & n==1} = op{wV{xi+j}, xV{xi+j}}
      def step{j, n & n>=2} = pack{step{j, n/2}, step{j+n/2, n/2}}
      r = promote{u64, homMask{step{0, unr}}}
    } else {
      @unroll (j to unr) r|= promote{u64, homMask{op{wV{xi+j}, xV{xi+j}}}} << (j*vcount{VT})
    }
    b_setBatch{bulk, dst, ri, r}
    xi+= unr
    ri+= 1
  }
}
aa2bit{VT, unr, op}(dst:*u64, wr:*void, xr:*void, len:Size) : void = {
  wv:= *VT~~wr; ws:= *eltype{VT}~~wr
  xv:= *VT~~xr; xs:= *eltype{VT}~~xr
  any2bit{VT, unr, op, {i}=>load{ws,i}, {i}=>load{wv,i}, {i}=>load{xs,i}, {i}=>load{xv,i}, dst, len}
}

as2bit{VT, unr, op}(dst:*u64, wr:*void, x:u64, len:Size) : void = {
  wv:= *VT~~wr; ws:= *eltype{VT}~~wr
  xv:= VT**pathAS{dst, len, eltype{VT}, op, x}
  any2bit{VT, unr, op, {i}=>load{ws,i}, {i}=>load{wv,i}, {i}=>x, {i}=>xv, dst, len}
}

bitAA{bitop}(dst:*u64, wr:*void, xr:*void, len:Size) : void = {
  @forNZ (dst, w in *u64~~wr, x in *u64~~xr over _ to cdiv{len,64}) dst = bitop{w,x}
}

not(dst:*u64, x:*u64, len:Size) : void = { am:=cdiv{len,64}; emit{void, 'bit_negatePtr', dst, x, am} }
cpy(dst:*u64, x:*u64, len:Size) : void = { am:=cdiv{len,64}; emit{void, 'memcpy', dst, x, am*8} }

bitAS{op}(dst:*u64, wr:*void, x:u64, len:Size) : void = { # show{'bitAS'}
  xf:f64 = interp_f64{x}
  r0:u1 = op{0,xf}
  r1:u1 = op{1,xf}
  if (r0==r1) {
    if (rare{isNaN{xf}}) {
      if (eqne{op}) fillbits{dst, len, r0, x}
      else { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }
    fillbits{dst, len, r0}
    return{}
  }
  if (r0) call{not, dst, *u64~~wr, len}
  else    call{cpy, dst, *u64~~wr, len}
}

def table{aa, F, w, G_bit, G_vec} = {
  tup{
    G_bit{},
    ...each{{E}=>{
      def E2 = (if (aa and isunsigned{E} and ((match{F,__eq} | match{F,__ne})  |  (E==u32))) {ty_s{E}} else E)
      def bulk = w/width{E2}
      G_vec{[bulk]E2, max{1, 8/bulk}}
    }, tup{i8, i16, i32, f64, u8, u16, u32}}
  }
}
def tableAA{w, F, F_bit} = table{1, F, w, {} => bitAA{F_bit}, {V, u} => aa2bit{V, u, F}}
def tableAS{w, F       } = table{0, F, w, {} => bitAS{F},     {V, u} => as2bit{V, u, F}}

def tup_eqAA = tableAA{arch_defvw, __eq, {a,b}=>a ^ ~b}; v_eqAA:*type{tupsel{0,tup_eqAA}} = tup_eqAA; 'simd_eqAA'=v_eqAA
def tup_neAA = tableAA{arch_defvw, __ne, {a,b}=>a ^  b}; v_neAA:*type{tupsel{0,tup_neAA}} = tup_neAA; 'simd_neAA'=v_neAA
def tup_gtAA = tableAA{arch_defvw, __gt, {a,b}=>a & ~b}; v_gtAA:*type{tupsel{0,tup_gtAA}} = tup_gtAA; 'simd_gtAA'=v_gtAA
def tup_geAA = tableAA{arch_defvw, __ge, {a,b}=>a | ~b}; v_geAA:*type{tupsel{0,tup_geAA}} = tup_geAA; 'simd_geAA'=v_geAA

def tup_eqAS = tableAS{arch_defvw, __eq}; v_eqAS:*type{tupsel{0,tup_eqAS}} = tup_eqAS; 'simd_eqAS'=v_eqAS
def tup_neAS = tableAS{arch_defvw, __ne}; v_neAS:*type{tupsel{0,tup_neAS}} = tup_neAS; 'simd_neAS'=v_neAS
def tup_gtAS = tableAS{arch_defvw, __gt}; v_gtAS:*type{tupsel{0,tup_gtAS}} = tup_gtAS; 'simd_gtAS'=v_gtAS
def tup_geAS = tableAS{arch_defvw, __ge}; v_geAS:*type{tupsel{0,tup_geAS}} = tup_geAS; 'simd_geAS'=v_geAS
def tup_ltAS = tableAS{arch_defvw, __lt}; v_ltAS:*type{tupsel{0,tup_ltAS}} = tup_ltAS; 'simd_ltAS'=v_ltAS
def tup_leAS = tableAS{arch_defvw, __le}; v_leAS:*type{tupsel{0,tup_leAS}} = tup_leAS; 'simd_leAS'=v_leAS
