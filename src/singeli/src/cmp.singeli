include './base'
include './cbqnDefs'
if (hasarch{'X86_64'}) {
  include './x86_vec'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './f64'
include './bitops'


def fillbits{dst:*u64, len:(ux), v   } = { emit{void, 'fillBits',    dst, len, v   }; return{}; }
def fillbits{dst:*u64, len:(ux), v, x} = { emit{void, 'fillBitsDec', dst, len, v, x}; return{}; }
def cmp_err{x} = { emit{void, 'cmp_err'}; return{}; }

fn cmpIX(dst:*u64, len:ux, x:u64, v:u1) : void = {
  nan:u1 = q_f64{x}
  if (~(nan | q_chr{x})) cmp_err{x}
  fillbits{dst, len, v&~nan, x}
}

def eqne{op} = same{op,__eq}|same{op,__ne}

def pathAS{dst, len, T, op, x & issigned{T}} = {
  def R{f & eqne{op}} = {
    if (rare{floor{f}!=f}) fillbits{dst, len, op{0,1}, x} # also includes check for NaN/sNaN
    ftrunc{i64,f}
  }
  def R{f & same{op,__lt}|same{op,__ge}} = ftrunc{i64,ceil{f}}
  def R{f & same{op,__gt}|same{op,__le}} = ftrunc{i64,floor{f}}
  
  xf:f64 = interp_f64{x}
  xi64:i64 = R{xf}
  xT:T = trunc{T, xi64}
  def nanchk{} = {
    if(~eqne{op}) { # NaN was already checked for â‰ /=
      if (isNaN{xf}) { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }
  }
  if (~hasarch{'X86_64'}) nanchk{}
  if (rare{promote{i64, xT}!=xi64}) {
    if (hasarch{'X86_64'}) nanchk{}
    fillbits{dst, len, op{0,xf}}
  }
  xT
}

def pathAS{dst, len, T, op, x & T==f64} = {
  if (rare{~q_f64{x}}) {
    if (~eqne{op}) if (~q_chr{x}) cmp_err{x}
    fillbits{dst, len, op{0,1}, x}
  }
  from_B{T,x}
}

def pathAS{dst, len, T, op, x & isunsigned{T}} = {
  if (rare{~q_chr{x}}) {
    if (~eqne{op}) if (~q_f64{x}) cmp_err{x}
    fillbits{dst, len, op{1,0}, x}
  }
  xc32:u32 = from_B{u32,x}
  if (xc32 > maxvalue{T}) fillbits{dst, len, op{0,1}}
  trunc{T, xc32}
}



def any2bit{VT, unr, op, wS, wV, xS, xV, dst:*u64, len:(ux)} = {
  xi:ux = 0
  ri:ux = 0
  def bulk = vcount{VT}*unr
  am:ux = cdiv{len,bulk}
  assert{am>0}
  while (ri < am) {
    r:u64 = 0
    if (hasarch{'AARCH64'}) {
      r = promote{u64, homMask{...each{{j}=>op{wV{xi+j}, xV{xi+j}}, iota{unr}}}}
    } else {
      @unroll (j to unr) r|= promote{u64, homMask{op{wV{xi+j}, xV{xi+j}}}} << (j*vcount{VT})
    }
    b_setBatch{bulk, dst, ri, r}
    xi+= unr
    ri+= 1
  }
}
fn aa2bit{VT, unr, op}(dst:*u64, wr:*void, xr:*void, len:ux) : void = {
  wv:= *VT~~wr; ws:= *eltype{VT}~~wr
  xv:= *VT~~xr; xs:= *eltype{VT}~~xr
  any2bit{VT, unr, op, load{ws,.}, load{wv,.}, load{xs,.}, load{xv,.}, dst, len}
}

fn as2bit{VT, unr, op}(dst:*u64, wr:*void, x:u64, len:ux) : void = {
  wv:= *VT~~wr; ws:= *eltype{VT}~~wr
  xv:= VT**pathAS{dst, len, eltype{VT}, op, x}
  any2bit{VT, unr, op, load{ws,.}, load{wv,.}, {i}=>x, {i}=>xv, dst, len}
}

fn bitAA{bitop}(dst:*u64, wr:*void, xr:*void, len:ux) : void = {
  @forNZ (dst, w in *u64~~wr, x in *u64~~xr over cdiv{len,64}) dst = bitop{w,x}
}

fn not(dst:*u64, x:*u64, len:ux) : void = { am:=cdiv{len,64}; emit{void, 'bit_negatePtr', dst, x, am} }
fn cpy(dst:*u64, x:*u64, len:ux) : void = { am:=cdiv{len,64}; emit{void, 'memcpy', dst, x, am*8} }

fn bitAS{op}(dst:*u64, wr:*void, x:u64, len:ux) : void = { # show{'bitAS'}
  xf:f64 = interp_f64{x}
  r0:u1 = op{0,xf}
  r1:u1 = op{1,xf}
  if (r0==r1) {
    if (rare{isNaN{xf}}) {
      if (eqne{op}) fillbits{dst, len, r0, x}
      else { call{cmpIX, dst, len, x, op{0,1}}; return{}; }
    }
    fillbits{dst, len, r0}
    return{}
  }
  if (r0) call{not, dst, *u64~~wr, len}
  else    call{cpy, dst, *u64~~wr, len}
}

def table{aa, F, w, G_bit, G_vec} = {
  tup{
    G_bit{},
    ...each{{E}=>{
      def E2 = (if (aa and isunsigned{E} and ((same{F,__eq} | same{F,__ne})  |  (E==u32))) {ty_s{E}} else E)
      def bulk = w/width{E2}
      G_vec{[bulk]E2, max{tern{hasarch{'AARCH64'},2,1}, 8/bulk}}
    }, tup{i8, i16, i32, f64, u8, u16, u32}}
  }
}
def tableAA{w, F, F_bit} = table{1, F, w, {} => bitAA{F_bit}, {V, u} => aa2bit{V, u, F}}
def tableAS{w, F       } = table{0, F, w, {} => bitAS{F},     {V, u} => as2bit{V, u, F}}


exportT{'simd_eqAS', tableAS{arch_defvw, __eq}}; exportT{'simd_eqAA', tableAA{arch_defvw, __eq, {a,b}=>a ^ ~b}}
exportT{'simd_neAS', tableAS{arch_defvw, __ne}}; exportT{'simd_neAA', tableAA{arch_defvw, __ne, {a,b}=>a ^  b}}
exportT{'simd_gtAS', tableAS{arch_defvw, __gt}}; exportT{'simd_gtAA', tableAA{arch_defvw, __gt, {a,b}=>a & ~b}}
exportT{'simd_geAS', tableAS{arch_defvw, __ge}}; exportT{'simd_geAA', tableAA{arch_defvw, __ge, {a,b}=>a | ~b}}
exportT{'simd_ltAS', tableAS{arch_defvw, __lt}}
exportT{'simd_leAS', tableAS{arch_defvw, __le}}
