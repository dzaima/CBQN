include './base'
include './cbqnDefs'
include './mask'
include './bitops'
include './lut'

def arch_minvw = if (hasarch{'AARCH64'}) 64 else 128
def arch_minv{T=[_]E if width{T}< arch_minvw} = [arch_minvw / width{E}]E
def arch_minv{T      if width{T}>=arch_minvw} = T

def gather
if_inline (hasarch{'AVX2'}) {
  # def:T - masked original content
  # b:B - pointer to data to index; if width{B}<elwidth{T}, padding bytes are garbage read after wanted position
  # idx - actual (unscaled) index list
  def gather{d:T, b:B, idx:([8]i32), M if w256{T,32}} = {
    if (M{0}) T ~~ emit{[8]i32, '_mm256_mask_i32gather_epi32', d, *void~~b, idx, M{T,'to sign bits'}, elwidth{B}/8}
    else      T ~~ emit{[8]i32, '_mm256_i32gather_epi32',         *void~~b, idx,                      elwidth{B}/8}
  }
  def gather{d:T, b:B, idx:([4]i32), M if w256{T,64}} = {
    if (M{0}) T ~~ emit{[4]i64, '_mm256_mask_i32gather_epi64', d, *void~~b, idx, M{T,'to sign bits'}, elwidth{B}/8}
    else      T ~~ emit{[4]i64, '_mm256_i32gather_epi64',         *void~~b, idx,                      elwidth{B}/8}
  }
}

def wrapChk{cw0:VI, xlf, M} = {
  cw:= cw0 + (xlf & lt_hom{VI, cw0, VI**0})
  if (any_hom{M, ty_u{cw} >= ty_u{xlf}}) return{0}
  cw
}

def masked_multistore{r0, vs, M, end} = { # returns bumped-forwards r
  r:= r0
  def left = if (M{0}) { left:ux = M{'count'} } else 0
  def lastMaskedStore = make_opt_branch{M{0}, tup{one_type{vs}}, {c} => {
    store_narrow{r, 0, c, mask_first{left}}
    end{}
  }}
  
  each{{i, c: [k]_} => {
    if (M{0}) {
      if (i+1 == length{vs} or left<k) lastMaskedStore{c}
      left-= k
    }
    store_narrow{r, 0, c, mask_none}
    r+= k
  }, inds{vs}, vs}
  r
}



def vptr{VT=[_]E, ptr} = tup{VT, *E~~ptr}

fn wrap_inds{TI if issigned{TI}}(src:*void, dst:*void, n:u64, cyc0:u64) : void = {
  def cyc = cast_i{TI,cyc0}
  if (has_simd) {
    def bulk = arch_defvw{} / width{TI}
    def VT = [bulk]TI
    @for_masked{bulk}(src in vptr{VT, src}, dst in vptr{VT, dst} over n) {
      dst = blend_hom{src, src + VT**cyc, src < VT**0}
    }
  } else {
    @for (src in *TI~~src, dst in *TI~~dst over n) dst = tern{src<0, src+cyc, src}
  }
}
export_tab{'si_wrap_inds', each{wrap_inds, tup{i8}}}



def inds_buf_max = 64
export{'INDS_BUF_MAX_COPY', ux~~inds_buf_max}
(if (has_sel) {
  fn select_rows_fn{TD, nt, ni, G}(inds:*u8, x0:*void, xbump:u64, r0:*void, rbump:u64, r1:*void) : ux = G{inds, *TD~~x0, xbump, *TD~~r0, rbump, r1} # TG,nt,ni args just for prettier names for debugging
  
  def select_rows{TD} = {
    def try_nt{nt} = match(lut_gen{'c', TD, nt, 2}) {
      {{nt, ni, G}} => {
        def r = tup{nt, ni, select_rows_fn{TD, nt, ni, {inds, x0, xbump, r0, rbump, r1} => {
          # lprintf{'inds=', [ni]u8, ' tab=', [nt]TD, ' xbump=', xbump, ' rbump=', rbump, ' allowed=', (*u8~~r1) - *u8~~r0}
          def iv = load{*[ni]u8~~inds, 0}
          x:*TD = x0
          r:*TD = r0
          iters:ux = 0
          while (*void~~(r + ni) <= r1) {
            def rs = G{x}{iv}
            masked_multistore{r, rs, mask_none, '!'}
            x+= xbump
            r+= rbump
            ++iters
          }
          iters
        }}}
        merge{tup{r}, try_nt{nt+1}}
      }
      {..._} => tup{}
    }
    
    try_nt{2}
  }
  def select_rows_parts = each{select_rows, tup{u8, u16, u32, u64}}
  def max_nt = fold{__max, each{select{.,0}, join{select_rows_parts}}}
  def max_ni = fold{__max, each{select{.,1}, join{select_rows_parts}}}
  def select_rows_tab_h = lb{max_nt}+1
  
  # >{t ← (¯1+⌊16÷𝕩)⌾(¯1⊸⊑) ⌊𝕩÷˜↕16 ⋄ ⟨𝕩, ⌊16÷𝕩, 1+¯1⊑t, t⟩}¨ 2↓↕8
  # >{t←(¯1+𝕩-𝕩|16)⌾(¯1⊸⊑) 𝕩|↕16 ⋄ ⟨𝕩, 𝕩 - 1+¯1⊑t, t⟩}¨ 2↓↕8
  repeat_tab:*u8 = join{each{{k} => merge{
    merge{range{15}%k, k - 16%k - 1}, # TODO top not used, can clean up
    merge{(range{15}/k)>>0, ((16/k)>>0) - 1}
  }, 1+range{8}}}
  fn simd_repeat_inds(src:*u8, dst:*u8, start:u8, csz:u8) : ux = { # src and dst may be the same
    assert{(start>=1) & (start<=16)}
    assert{start < max_ni}
    def V = [16]u8
    def VU16 = re_el{u16, V}
    def px{x} = promote{ux, x}
    
    shufr:u8 = 1 # number of repeats produced by shuffle path
    v:= load{*V~~src, 0}
    if (start <= 8) {
      tab:= *V~~(repeat_tab + 32 * (start-1))
      def l0 = load{tab, 0}
      def l1 = load{tab, 1}
      v = shuf{V, v, l0}
      v+= V~~(VU16~~l1 * broadcast{VU16, csz})
      shufr = extract{l1, 15}+1
    }
    
    def shufe = px{shufr} * px{start} # number of elements produced by shuffle path
    
    cdst:= dst
    r:ux = 0
    do {
      r+= px{shufr}
      store{*V~~cdst, 0, v}
      v+= broadcast{V, csz * shufr}
      cdst+= shufe
    } while (max_ni>16 and cdst+start <= dst + max_ni)
    # lprintf{shufr, r, ux~~(cdst - dst) / px{start}}
    # lprintf{load{*[max_ni]u8~~dst}, cdst - dst}
    r
  }
  export{'simd_repeat_inds', simd_repeat_inds}
  
  def null_fn = select_rows_fn{void, 'BAD', 0, {..._} => { emit{void,'fatal','"bad select_rows"'}; 0 }}
  export{'null_fn', null_fn}
  export{'select_rows_tab_h', ux~~select_rows_tab_h}
  export_tab{'select_rows_tab', join{flip{each{{row} => {
    def a = each{select{.,0}, row} + 0 # +0 to work around findmatches bug
    def b = 1<<range{select_rows_tab_h}
    each{match { {{i}}=>select{select{row,i},2}; {{}}=>null_fn }, findmatches{a, b}}
  }, select_rows_parts}}}}
  
  def exportP{T, n, vs} = { a:*T = vs; export{n, a} }
  exportP{u8, 'select_rows_max_indn',   each{{row} => if (length{row}==0) 0 else one_val{       each{select{.,1}, row}}, select_rows_parts}}
  exportP{u8, 'select_rows_min_logcsz', each{{row} => if (length{row}==0) 0 else lb{fold{__min, each{select{.,0}, row}}}, select_rows_parts}}
  
  def select_rows_better = scan{{p,{v,i}} => if (length{v}==0) p else i, 0, each{tup, select_rows_parts, range{4}}}
  exportP{u8, 'select_rows_better', select_rows_better+1}
  
  fn select_rows_widen{sh}(src:*void, dst:*void, max:ux, n:ux) : ux = {
    if (sh != 0) {
      def bulk = (arch_defvw{}/8) >> sh
      def WV = [bulk<<sh]u8
      def V = [bulk]u8
      if ((n<<sh) > inds_buf_max) return{0}
      if (max >= (inds_buf_max>>sh)) return{0}
      
      @for_backwards (i to inds_buf_max/(width{WV}/8)) {
        def s = load{arch_minv{V}, *u8~~src + i*bulk, bulk}
        def v = raw_widen_inds{bulk, sh, s}
        store{*WV~~dst, i, v}
      }
    }
    sh
  }
  export_tab{'select_rows_widen', each{{t0, t1} => {
    def {S, D} = each{select{tup{u8,u16,u32,u64},.}, tup{t0, t1}}
    select_rows_widen{t1-t0}
  }, select_rows_better, range{4}}}
})



fn select_fn{rw, TI, TD}(w0:*void, x0:*void, r0:*void, wl:u64, xl:u64) : u1 = {
  w:= *TI ~~ w0
  x:= *TD ~~ x0
  r:= *TD ~~ r0
  def wd = width{TD}
  def wi = width{TI}
  if (TI==i8) { # TODO some minimum bound on wl?
    def trytab{nt} = match(lut_gen{'i', TD, nt, 2}) {
      {{nt, ni, G}} => {
        if (xl <= nt) {
          def VI = [ni]TI
          def xlf = VI**cast_i{TI, xl}
          # show{TD, nt, ni, G}
          # lprintf{'LUT of ', VI, ' ⊏ ', [nt]TD, ' with ', wl, ' ≡  ≠𝕨, ', xl, ' ≡ ≠𝕩'}
          def lut = G{x}
          @for_masked{ni}(w0 in tup{VI,w}, M in 'm' over wl) {
            def w = wrapChk{w0, xlf, M}
            def rs = lut{ty_u{w}}
            r = masked_multistore{r, rs, M, {} => return{1}}
          }
          return{1}
        }
        trytab{nt+1}
      }
      {x} => {}
    }
    trytab{2}
  }
  
  if (hasarch{'AVX2'}) {
    def TIE = i32
    def TDE = tern{wd<32, u32, TD}
    def bulk = rw / width{TDE}
    def VI = [bulk]TIE
    def VD = [bulk]TDE
    def xlf = VI**cast_i{TIE, xl}
    
    @for_masked{bulk}(cw0 in tup{VI,w}, sr in tup{'g',r}, M in 'm' over wl) {
      cw:= wrapChk{cw0, xlf, M}
      got:= gather{VD**0, x, cw, M}
      if (TDE!=TD) got&= VD**tail{wd}
      sr{got}
    }
  } else {
    def ix = ty_s{ux}
    def wrap{w0:T if quality{T}=='i'} = {
      def w1 = promote{ix, w0}
      tern{w1<0, xl + ux~~w1, ux~~w1}
    }
    if (wi>=32) {
      @for (r, w0 in w over wl) {
        def w2 = wrap{w0}
        if (rare{w2>=xl}) return{0}
        r = load{x, w2}
      }
    } else {
      def block_size = (1<<14) / (wi/8)
      @for_blocks{block_size}(bl to wl) {
        def {s,e} = bl
        def {ok, min, max} = get_range{w, s, e}
        if (not ok) return{0}
        if (rare{max >= i64~~xl}) return{0}
        if (min < 0) {
          if (rare{min < -i64~~xl}) return{0}
          # TODO use wrap_inds
          @for (w, r over _ from s to e) r = load{x, wrap{w}}
        } else {
          @for (w, r over _ from s to e) r = load{x, promote{ux,ty_u{w}}}
        }
      }
    }
  }
  1
}
def select_fn{TI, TD} = select_fn{arch_defvw{}, TI, TD}

export_tab{'si_select_tab', join{table{select_fn,
  tup{i8, i16, i32},       # indices
  tup{u8, u16, u32, u64}}} # values
}



(if(has_sel) {
  fn simd_select_bool128(w0:*void, x0:*void, r0:*void, wl:u64, xl:u64) : u1 = {
    def TI = i8
    def VI = [arch_defvw{}/8]TI
    def VU = ty_u{VI}
    w:= *VI ~~ w0
    xlf:= VI**cast_i{TI, xl}
    
    if (hasarch{'AARCH64'}) {
      def xrev = rbit{load{*VU ~~ x0}}
      @for_masked{16}(cw0 in w, r in *u16~~r0, M in 'm' over i to wl) {
        def cw = ty_u{wrapChk{cw0, xlf, M}}
        def byte = shuf{[16]u8, xrev, cw>>3}
        r = hom_to_int{ty_s{byte << (cw & VU**7)} < VI**0}
      }
    } else {
      if (wl>32 and xl<=16) {
        xb:= shuf{[4]u64, expand_bits{[32]u8, load{*u32~~x0}}, 0,1,0,1}
        @for_masked{32}(cw0 in w, sr in *u32~~r0, M in 'm' over wl) {
          cw:= wrapChk{cw0, xlf, M}
          sr = hom_to_int{shuf{[16]i8, xb, cw}}
        }
      } else {
        x:= shuf{[4]u64, load{*VI ~~ x0}, 0,1,0,1}
        low:= VI**7
        b  := VI~~make{[32]u8, 1 << (iota{32} & 7)}
        @for_masked{32}(cw0 in w, sr in *u32~~r0, M in 'm' over wl) {
          cw:= wrapChk{cw0, xlf, M}
          byte:= shuf{[16]i8, x, VI~~(([8]u32~~(cw&~low))>>3)}
          mask:= shuf{[16]i8, b, cw & low}
          sr = hom_to_int{(mask & byte) == mask}
        }
      }
    }
    1
  }
  export{'simd_select_bool128', simd_select_bool128}
  
  fn si_select_rows_8bit(inds:*u8, indn:ux, src:*void, dst:*void, rows:ux) : void = { # leaves zeroes in result cells above indn
    def bulk = arch_defvw{} / 8
    def V = [bulk]u8
    def lut = shuf_u8bits{inds, indn}
    @for_masked{bulk}(src in tup{V,*u8~~src}, dst in tup{V,*u8~~dst} over rows) {
      dst = lut{src}
    }
  }
  export{'si_select_rows_8bit', si_select_rows_8bit}
})



