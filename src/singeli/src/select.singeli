include './base'
include './cbqnDefs'
include './mask'
include './bitops'
include './lut'
include 'util/tup'

def has_sel = hasarch{'AVX2'} or hasarch{'AARCH64'}

def gather
if_inline (hasarch{'AVX2'}) {
  # def:T - masked original content
  # b:B - pointer to data to index; if width{B}<elwidth{T}, padding bytes are garbage read after wanted position
  # idx - actual (unscaled) index list
  def gather{d:T, b:B, idx:([8]i32), M if w256{T,32}} = {
    if (M{0}) T ~~ emit{[8]i32, '_mm256_mask_i32gather_epi32', d, *void~~b, idx, M{T,'to sign bits'}, elwidth{B}/8}
    else      T ~~ emit{[8]i32, '_mm256_i32gather_epi32',         *void~~b, idx,                      elwidth{B}/8}
  }
  def gather{d:T, b:B, idx:([4]i32), M if w256{T,64}} = {
    if (M{0}) T ~~ emit{[4]i64, '_mm256_mask_i32gather_epi64', d, *void~~b, idx, M{T,'to sign bits'}, elwidth{B}/8}
    else      T ~~ emit{[4]i64, '_mm256_i32gather_epi64',         *void~~b, idx,                      elwidth{B}/8}
  }
}

def wrapChk{cw0:VI, xlf, M} = {
  cw:= cw0 + (xlf & VI~~(cw0<VI**0))
  if (homAny{M{ty_u{cw} >= ty_u{xlf}}}) return{0}
  cw
}

def masked_multistore{r0, vs, M, end} = { # returns bumped-forwards r
  r:= r0
  def left = if (M{0}) { left:ux = M{'count'} } else 0
  def lastMaskedStore = makeOptBranch{M{0}, tup{oneType{vs}}, {c} => {
    storeBatch{r, 0, c, maskAfter{left}}
    end{}
  }}
  
  each{{i, c: [k]_} => {
    if (M{0}) {
      if (i+1 == length{vs} or left<k) lastMaskedStore{c}
      left-= k
    }
    storeBatch{r, 0, c, maskNone}
    r+= k
  }, inds{vs}, vs}
  r
}



fn select_fn{rw, TI, TD}(w0:*void, x0:*void, r0:*void, wl:u64, xl:u64) : u1 = { # TODO don't require SIMD?
  w:= *TI ~~ w0
  x:= *TD ~~ x0
  r:= *TD ~~ r0
  def wd = width{TD}
  def wi = width{TI}
  if (TI==i8) { # TODO some minimum bound on wl?
    def trytab{nt} = match(lut_gen{'i', TD, nt, 2}) {
      {{nt, ni, G}} => {
        if (xl <= nt) {
          def VI = [ni]TI
          def xlf = VI**cast_i{TI, xl}
          # show{TD, nt, ni, G}
          # lprintf{'LUT of ', VI, ' âŠ ', [nt]TD, ' with ', wl, ' â‰¡  â‰ ð•¨, ', xl, ' â‰¡ â‰ ð•©'}
          def lut = G{x}
          @maskedLoop{ni}(w0 in tup{VI,w}, M in 'm' over wl) {
            def w = wrapChk{w0, xlf, M}
            def rs = lut{ty_u{w}}
            r = masked_multistore{r, rs, M, {} => return{1}}
          }
          return{1}
        }
        trytab{nt+1}
      }
      {x} => {}
    }
    trytab{2}
  }
  
  if (hasarch{'AVX2'}) {
    def TIE = i32
    def TDE = tern{wd<32, u32, TD}
    def bulk = rw / width{TDE}
    def VI = [bulk]TIE
    def VD = [bulk]TDE
    def xlf = VI**cast_i{TIE, xl}
    
    @maskedLoop{bulk}(cw0 in tup{VI,w}, sr in tup{'g',r}, M in 'm' over wl) {
      cw:= wrapChk{cw0, xlf, M}
      got:= gather{VD**0, x, cw, M}
      if (TDE!=TD) got&= VD**((1<<wd)-1)
      sr{got}
    }
  } else {
    def ix = ty_s{ux}
    def wrap{w0:T if quality{T}=='i'} = {
      def w1 = promote{ix, w0}
      tern{w1<0, xl + ux~~w1, ux~~w1}
    }
    if (wi>=32) {
      @for (r, w0 in w over wl) {
        def w2 = wrap{w0}
        if (rare{w2>=xl}) return{0}
        r = load{x, w2}
      }
    } else {
      def block_size = (1<<14) / (wi/8)
      @for_blocks{block_size}(bl to wl) {
        def {s,e} = bl
        def {ok, min, max} = get_range{w, s, e}
        if (not ok) return{0}
        if (rare{max >= i64~~xl}) return{0}
        if (min < 0) {
          if (rare{min < -i64~~xl}) return{0}
          # TODO use wrap_inds
          @for (w, r over _ from s to e) r = load{x, wrap{w}}
        } else {
          @for (w, r over _ from s to e) r = load{x, promote{ux,ty_u{w}}}
        }
      }
    }
  }
  1
}
def select_fn{TI, TD} = select_fn{arch_defvw, TI, TD}

exportT{'si_select_tab', join{table{select_fn,
  tup{i8, i16, i32},       # indices
  tup{u8, u16, u32, u64}}} # values
}



(if(has_sel) {
  fn simd_select_bool128(w0:*void, x0:*void, r0:*void, wl:u64, xl:u64) : u1 = {
    def TI = i8
    def VI = [arch_defvw/8]TI
    def VU = ty_u{VI}
    w:= *VI ~~ w0
    xlf:= VI**cast_i{TI, xl}
    
    if (hasarch{'AARCH64'}) {
      def xrev = rbit{load{*VU ~~ x0}}
      @maskedLoop{16}(cw0 in w, r in *u16~~r0, M in 'm' over i to wl) {
        def cw = ty_u{wrapChk{cw0, xlf, M}}
        def byte = sel{[16]u8, xrev, cw>>3}
        r = homMask{ty_s{byte << (cw & VU**7)} < VI**0}
      }
    } else {
      if (wl>32 and xl<=16) {
        xb:= shuf{[4]u64, spreadBits{[32]u8, load{*u32~~x0}}, 4b1010}
        @maskedLoop{32}(cw0 in w, sr in *u32~~r0, M in 'm' over wl) {
          cw:= wrapChk{cw0, xlf, M}
          sr = homMask{sel{[16]i8, xb, cw}}
        }
      } else {
        x:= shuf{[4]u64, load{*VI ~~ x0}, 4b1010}
        low:= VI**7
        b  := VI~~make{[32]u8, 1 << (iota{32} & 7)}
        @maskedLoop{32}(cw0 in w, sr in *u32~~r0, M in 'm' over wl) {
          cw:= wrapChk{cw0, xlf, M}
          byte:= sel{[16]i8, x, VI~~(([8]u32~~(cw&~low))>>3)}
          mask:= sel{[16]i8, b, cw & low}
          sr = homMask{(mask & byte) == mask}
        }
      }
    }
    1
  }
  export{'simd_select_bool128', simd_select_bool128}
})



